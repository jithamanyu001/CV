{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwpE6x5s9NA91EnXhmTzzr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jithamanyu001/CV/blob/main/WaveNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn,optim\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt # for making figures"
      ],
      "metadata": {
        "id": "sO0_BW01BTao"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/makemore/master/names.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7TdvOHuSEH-",
        "outputId": "f5896f7e-5b00-41a0-fc2c-700314fefab9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-09-04 08:25:01--  https://raw.githubusercontent.com/karpathy/makemore/master/names.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228145 (223K) [text/plain]\n",
            "Saving to: ‘names.txt’\n",
            "\n",
            "\rnames.txt             0%[                    ]       0  --.-KB/s               \rnames.txt           100%[===================>] 222.80K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-09-04 08:25:01 (7.76 MB/s) - ‘names.txt’ saved [228145/228145]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_names=open(\"/content/names.txt\",'r').read().splitlines()\n"
      ],
      "metadata": {
        "id": "lm3Xk5st-se7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(''.join(all_names))))\n",
        "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s,i in stoi.items()}\n",
        "vocab_size = len(itos)\n",
        "print(itos)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ju5TGtWsANaV",
        "outputId": "08b47aae-6770-445a-b517-c51134bdb2c4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
            "27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# shuffle up the words\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(all_names)"
      ],
      "metadata": {
        "id": "iXZoAEOyBZo2"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build the dataset\n",
        "block_size = 16 # context length: how many characters do we take to predict the next one?\n",
        "\n",
        "def build_dataset(words):\n",
        "  X, Y = [], []\n",
        "\n",
        "  for w in words:\n",
        "    context = [0] * block_size\n",
        "    for ch in w + '.':\n",
        "      ix = stoi[ch]\n",
        "      X.append(context)\n",
        "      Y.append(ix)\n",
        "      context = context[1:] + [ix] # crop and append\n",
        "\n",
        "  X = torch.tensor(X)\n",
        "  Y = torch.tensor(Y)\n",
        "  print(X.shape, Y.shape)\n",
        "  return X, Y\n",
        "n=int(0.8*len(all_names))\n",
        "X_train,Y_train=build_dataset(all_names[:n])\n",
        "X_test,Y_test=build_dataset(all_names[n:])"
      ],
      "metadata": {
        "id": "3EwKbWrdAcK9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b391fdb4-f6c3-4b16-f43e-3e7714f3daf6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([182625, 16]) torch.Size([182625])\n",
            "torch.Size([45521, 16]) torch.Size([45521])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def makeBatch(X,Y,batch_size=32):\n",
        "  n=X.shape[0]\n",
        "  idx=random.sample(list(range(n)),k=batch_size)\n",
        "  return X[idx],Y[idx]"
      ],
      "metadata": {
        "id": "VCACSYouBWM2"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class WaveNet(nn.Module):\n",
        "  def __init__(self,vocab_size,embedding_size,hidden_size,combine,block_size):\n",
        "    super(WaveNet,self).__init__()\n",
        "    self.embedding=nn.Embedding(vocab_size,embedding_size)\n",
        "    self.combine=combine\n",
        "    self.lin1=nn.Linear(embedding_size*combine,hidden_size)\n",
        "    self.bn1=nn.BatchNorm1d(block_size//combine)\n",
        "    self.lin2=nn.Linear(hidden_size*combine,hidden_size)\n",
        "    self.bn2=nn.BatchNorm1d(block_size//combine**2)\n",
        "    self.lin3=nn.Linear(hidden_size*combine,hidden_size)\n",
        "    self.bn3=nn.BatchNorm1d(block_size//combine**3)\n",
        "    self.lin4=nn.Linear(hidden_size*combine,hidden_size)\n",
        "    self.bn4=nn.BatchNorm1d(block_size//combine**4)\n",
        "    self.output=nn.Linear(hidden_size,vocab_size)\n",
        "  def forward(self,x,y=None):\n",
        "    embedding=self.embedding(x)\n",
        "    B,L,emd=embedding.shape\n",
        "    embedding=embedding.view([B,L//self.combine,emd*self.combine])\n",
        "    embedding=F.tanh(self.lin1(embedding))\n",
        "    embedding=self.bn1(embedding)\n",
        "    B,L,emd=embedding.shape\n",
        "    embedding=embedding.view([B,L//self.combine,emd*self.combine])\n",
        "    embedding=F.tanh(self.lin2(embedding))\n",
        "    embedding=self.bn2(embedding)\n",
        "    B,L,emd=embedding.shape\n",
        "    embedding=embedding.view([B,L//self.combine,emd*self.combine])\n",
        "    embedding=F.tanh(self.lin3(embedding))\n",
        "    embedding=self.bn3(embedding)\n",
        "    B,L,emd=embedding.shape\n",
        "    embedding=embedding.view([B,L//self.combine,emd*self.combine])\n",
        "    embedding=F.tanh(self.lin4(embedding))\n",
        "    embedding=self.bn4(embedding)\n",
        "    embedding=embedding.view(B,-1)\n",
        "    logits=self.output(embedding)\n",
        "    if y==None:\n",
        "      return logits,None\n",
        "    else:\n",
        "      loss=F.cross_entropy(logits,y)\n",
        "      return logits,loss\n",
        ""
      ],
      "metadata": {
        "id": "vaDy9s2ZUK1k"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_size=50\n",
        "hidden_size=100\n",
        "combine=2"
      ],
      "metadata": {
        "id": "D_ItC0hPV-Sk"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=WaveNet(vocab_size,embedding_size,hidden_size,combine,block_size)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6RIRhb5lWBFN",
        "outputId": "3dab9f3c-5a02-46e8-a85b-ae06f0a40e4f"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "WaveNet(\n",
              "  (embedding): Embedding(27, 50)\n",
              "  (lin1): Linear(in_features=100, out_features=100, bias=True)\n",
              "  (bn1): BatchNorm1d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (lin2): Linear(in_features=200, out_features=100, bias=True)\n",
              "  (bn2): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (lin3): Linear(in_features=200, out_features=100, bias=True)\n",
              "  (bn3): BatchNorm1d(2, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (lin4): Linear(in_features=200, out_features=100, bias=True)\n",
              "  (bn4): BatchNorm1d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (output): Linear(in_features=100, out_features=27, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer=optim.AdamW(model.parameters(),0.001)\n",
        "epochs=10000\n",
        "batch_size=128"
      ],
      "metadata": {
        "id": "RNq0t2t3clIa"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lossTrain=[]\n",
        "lossTest=[]\n",
        "for i in range(epochs):\n",
        "  optimizer.zero_grad()\n",
        "  x,y=makeBatch(X_train,Y_train,batch_size)\n",
        "  logits,loss=model(x,y)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  print(f\"Train loss for epoch{i}: {loss.item()}\")\n",
        "  lossTrain.append(loss.item())\n",
        "  x,y=makeBatch(X_test,Y_test,batch_size)\n",
        "  logits,loss=model(x,y)\n",
        "  print(f\"Test loss for epoch{i}: {loss.item()}\")\n",
        "  lossTest.append(loss.item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6_w1K84d_px",
        "outputId": "50bbc65b-d440-4f00-a1e7-b35755091ba4"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Train loss for epoch7500: 2.1466970443725586\n",
            "Test loss for epoch7500: 2.2026259899139404\n",
            "Train loss for epoch7501: 1.738340973854065\n",
            "Test loss for epoch7501: 2.039677619934082\n",
            "Train loss for epoch7502: 1.80586576461792\n",
            "Test loss for epoch7502: 2.089885711669922\n",
            "Train loss for epoch7503: 2.093270778656006\n",
            "Test loss for epoch7503: 2.0659708976745605\n",
            "Train loss for epoch7504: 1.9384955167770386\n",
            "Test loss for epoch7504: 2.071631908416748\n",
            "Train loss for epoch7505: 1.867493987083435\n",
            "Test loss for epoch7505: 2.1748344898223877\n",
            "Train loss for epoch7506: 2.021557569503784\n",
            "Test loss for epoch7506: 2.037782907485962\n",
            "Train loss for epoch7507: 2.024129867553711\n",
            "Test loss for epoch7507: 1.9927377700805664\n",
            "Train loss for epoch7508: 1.7914018630981445\n",
            "Test loss for epoch7508: 2.1429529190063477\n",
            "Train loss for epoch7509: 1.749616026878357\n",
            "Test loss for epoch7509: 2.259732961654663\n",
            "Train loss for epoch7510: 1.8885424137115479\n",
            "Test loss for epoch7510: 2.2162482738494873\n",
            "Train loss for epoch7511: 2.00355863571167\n",
            "Test loss for epoch7511: 2.2073583602905273\n",
            "Train loss for epoch7512: 2.084305763244629\n",
            "Test loss for epoch7512: 1.997962236404419\n",
            "Train loss for epoch7513: 1.981904149055481\n",
            "Test loss for epoch7513: 1.824076771736145\n",
            "Train loss for epoch7514: 1.8301796913146973\n",
            "Test loss for epoch7514: 2.117788791656494\n",
            "Train loss for epoch7515: 2.0811445713043213\n",
            "Test loss for epoch7515: 2.1486146450042725\n",
            "Train loss for epoch7516: 1.817440390586853\n",
            "Test loss for epoch7516: 2.1813440322875977\n",
            "Train loss for epoch7517: 2.029866933822632\n",
            "Test loss for epoch7517: 1.897444248199463\n",
            "Train loss for epoch7518: 1.8803622722625732\n",
            "Test loss for epoch7518: 2.110778331756592\n",
            "Train loss for epoch7519: 1.827440619468689\n",
            "Test loss for epoch7519: 1.903901219367981\n",
            "Train loss for epoch7520: 2.0933735370635986\n",
            "Test loss for epoch7520: 2.078571319580078\n",
            "Train loss for epoch7521: 1.9005849361419678\n",
            "Test loss for epoch7521: 1.9547337293624878\n",
            "Train loss for epoch7522: 1.8146603107452393\n",
            "Test loss for epoch7522: 2.0644257068634033\n",
            "Train loss for epoch7523: 2.1644201278686523\n",
            "Test loss for epoch7523: 2.1637611389160156\n",
            "Train loss for epoch7524: 1.8850181102752686\n",
            "Test loss for epoch7524: 2.1064934730529785\n",
            "Train loss for epoch7525: 1.9481569528579712\n",
            "Test loss for epoch7525: 2.0238113403320312\n",
            "Train loss for epoch7526: 2.0048470497131348\n",
            "Test loss for epoch7526: 2.2741005420684814\n",
            "Train loss for epoch7527: 1.9944652318954468\n",
            "Test loss for epoch7527: 2.055724859237671\n",
            "Train loss for epoch7528: 1.9344033002853394\n",
            "Test loss for epoch7528: 1.9887800216674805\n",
            "Train loss for epoch7529: 2.0192370414733887\n",
            "Test loss for epoch7529: 2.085763692855835\n",
            "Train loss for epoch7530: 2.0727033615112305\n",
            "Test loss for epoch7530: 1.976334810256958\n",
            "Train loss for epoch7531: 2.042722463607788\n",
            "Test loss for epoch7531: 2.0045793056488037\n",
            "Train loss for epoch7532: 1.8819149732589722\n",
            "Test loss for epoch7532: 2.043781042098999\n",
            "Train loss for epoch7533: 1.7362060546875\n",
            "Test loss for epoch7533: 2.1381988525390625\n",
            "Train loss for epoch7534: 1.8417223691940308\n",
            "Test loss for epoch7534: 2.0788493156433105\n",
            "Train loss for epoch7535: 1.8608384132385254\n",
            "Test loss for epoch7535: 1.9117546081542969\n",
            "Train loss for epoch7536: 1.792515516281128\n",
            "Test loss for epoch7536: 2.062077760696411\n",
            "Train loss for epoch7537: 2.0032899379730225\n",
            "Test loss for epoch7537: 1.8976606130599976\n",
            "Train loss for epoch7538: 2.026862859725952\n",
            "Test loss for epoch7538: 2.0778980255126953\n",
            "Train loss for epoch7539: 2.1229898929595947\n",
            "Test loss for epoch7539: 2.063934087753296\n",
            "Train loss for epoch7540: 2.0630269050598145\n",
            "Test loss for epoch7540: 2.107863426208496\n",
            "Train loss for epoch7541: 2.163200616836548\n",
            "Test loss for epoch7541: 2.1319804191589355\n",
            "Train loss for epoch7542: 2.2636523246765137\n",
            "Test loss for epoch7542: 2.024939775466919\n",
            "Train loss for epoch7543: 2.0207717418670654\n",
            "Test loss for epoch7543: 2.1317713260650635\n",
            "Train loss for epoch7544: 1.7632495164871216\n",
            "Test loss for epoch7544: 2.0719876289367676\n",
            "Train loss for epoch7545: 2.103959798812866\n",
            "Test loss for epoch7545: 2.220597267150879\n",
            "Train loss for epoch7546: 1.9481078386306763\n",
            "Test loss for epoch7546: 2.0349502563476562\n",
            "Train loss for epoch7547: 2.059602975845337\n",
            "Test loss for epoch7547: 1.916729211807251\n",
            "Train loss for epoch7548: 2.0311951637268066\n",
            "Test loss for epoch7548: 2.217395305633545\n",
            "Train loss for epoch7549: 1.8751674890518188\n",
            "Test loss for epoch7549: 2.016108512878418\n",
            "Train loss for epoch7550: 2.0307984352111816\n",
            "Test loss for epoch7550: 1.8793483972549438\n",
            "Train loss for epoch7551: 1.7883894443511963\n",
            "Test loss for epoch7551: 2.1215507984161377\n",
            "Train loss for epoch7552: 1.8601956367492676\n",
            "Test loss for epoch7552: 2.0601441860198975\n",
            "Train loss for epoch7553: 2.0802276134490967\n",
            "Test loss for epoch7553: 2.0963125228881836\n",
            "Train loss for epoch7554: 2.0223000049591064\n",
            "Test loss for epoch7554: 1.968561053276062\n",
            "Train loss for epoch7555: 2.0012171268463135\n",
            "Test loss for epoch7555: 1.8892666101455688\n",
            "Train loss for epoch7556: 1.9593584537506104\n",
            "Test loss for epoch7556: 1.8607770204544067\n",
            "Train loss for epoch7557: 1.9761043787002563\n",
            "Test loss for epoch7557: 1.9273021221160889\n",
            "Train loss for epoch7558: 1.8814858198165894\n",
            "Test loss for epoch7558: 1.966378927230835\n",
            "Train loss for epoch7559: 1.8212018013000488\n",
            "Test loss for epoch7559: 2.0655360221862793\n",
            "Train loss for epoch7560: 2.2412705421447754\n",
            "Test loss for epoch7560: 2.2764577865600586\n",
            "Train loss for epoch7561: 2.009002685546875\n",
            "Test loss for epoch7561: 2.0035204887390137\n",
            "Train loss for epoch7562: 2.1023027896881104\n",
            "Test loss for epoch7562: 2.0011188983917236\n",
            "Train loss for epoch7563: 2.075530767440796\n",
            "Test loss for epoch7563: 2.0946197509765625\n",
            "Train loss for epoch7564: 1.9693080186843872\n",
            "Test loss for epoch7564: 1.9531272649765015\n",
            "Train loss for epoch7565: 1.7982369661331177\n",
            "Test loss for epoch7565: 2.195815324783325\n",
            "Train loss for epoch7566: 2.05075740814209\n",
            "Test loss for epoch7566: 2.1116178035736084\n",
            "Train loss for epoch7567: 2.0761892795562744\n",
            "Test loss for epoch7567: 1.9801194667816162\n",
            "Train loss for epoch7568: 1.8203201293945312\n",
            "Test loss for epoch7568: 2.036928176879883\n",
            "Train loss for epoch7569: 2.001457691192627\n",
            "Test loss for epoch7569: 2.094850778579712\n",
            "Train loss for epoch7570: 1.9583361148834229\n",
            "Test loss for epoch7570: 2.146860122680664\n",
            "Train loss for epoch7571: 1.9767792224884033\n",
            "Test loss for epoch7571: 2.0504822731018066\n",
            "Train loss for epoch7572: 2.1706702709198\n",
            "Test loss for epoch7572: 2.0800561904907227\n",
            "Train loss for epoch7573: 2.04355525970459\n",
            "Test loss for epoch7573: 1.9795032739639282\n",
            "Train loss for epoch7574: 1.8108359575271606\n",
            "Test loss for epoch7574: 2.0240771770477295\n",
            "Train loss for epoch7575: 1.942328929901123\n",
            "Test loss for epoch7575: 1.9402393102645874\n",
            "Train loss for epoch7576: 1.8820639848709106\n",
            "Test loss for epoch7576: 2.177091121673584\n",
            "Train loss for epoch7577: 1.7663933038711548\n",
            "Test loss for epoch7577: 2.038954257965088\n",
            "Train loss for epoch7578: 1.744671106338501\n",
            "Test loss for epoch7578: 2.0738918781280518\n",
            "Train loss for epoch7579: 1.8375332355499268\n",
            "Test loss for epoch7579: 1.8446296453475952\n",
            "Train loss for epoch7580: 1.9225637912750244\n",
            "Test loss for epoch7580: 2.270232677459717\n",
            "Train loss for epoch7581: 1.9805086851119995\n",
            "Test loss for epoch7581: 2.0618736743927\n",
            "Train loss for epoch7582: 1.7818350791931152\n",
            "Test loss for epoch7582: 2.0343873500823975\n",
            "Train loss for epoch7583: 1.9471174478530884\n",
            "Test loss for epoch7583: 2.439615249633789\n",
            "Train loss for epoch7584: 2.1514334678649902\n",
            "Test loss for epoch7584: 2.066704511642456\n",
            "Train loss for epoch7585: 1.9741706848144531\n",
            "Test loss for epoch7585: 2.0512421131134033\n",
            "Train loss for epoch7586: 2.043250322341919\n",
            "Test loss for epoch7586: 1.9963417053222656\n",
            "Train loss for epoch7587: 1.7558292150497437\n",
            "Test loss for epoch7587: 1.9397176504135132\n",
            "Train loss for epoch7588: 1.9126582145690918\n",
            "Test loss for epoch7588: 2.145026445388794\n",
            "Train loss for epoch7589: 2.0367887020111084\n",
            "Test loss for epoch7589: 2.0010874271392822\n",
            "Train loss for epoch7590: 2.137801170349121\n",
            "Test loss for epoch7590: 2.1550347805023193\n",
            "Train loss for epoch7591: 1.9522267580032349\n",
            "Test loss for epoch7591: 1.8747450113296509\n",
            "Train loss for epoch7592: 2.080983877182007\n",
            "Test loss for epoch7592: 1.9998914003372192\n",
            "Train loss for epoch7593: 1.8893064260482788\n",
            "Test loss for epoch7593: 1.985504150390625\n",
            "Train loss for epoch7594: 1.9339172840118408\n",
            "Test loss for epoch7594: 2.0962417125701904\n",
            "Train loss for epoch7595: 2.0206356048583984\n",
            "Test loss for epoch7595: 2.0351953506469727\n",
            "Train loss for epoch7596: 1.7724473476409912\n",
            "Test loss for epoch7596: 1.8566176891326904\n",
            "Train loss for epoch7597: 2.1393954753875732\n",
            "Test loss for epoch7597: 1.8979557752609253\n",
            "Train loss for epoch7598: 2.051626443862915\n",
            "Test loss for epoch7598: 2.215491771697998\n",
            "Train loss for epoch7599: 2.0450403690338135\n",
            "Test loss for epoch7599: 1.9682199954986572\n",
            "Train loss for epoch7600: 2.1993749141693115\n",
            "Test loss for epoch7600: 1.8663283586502075\n",
            "Train loss for epoch7601: 1.8396961688995361\n",
            "Test loss for epoch7601: 2.206695795059204\n",
            "Train loss for epoch7602: 1.7950235605239868\n",
            "Test loss for epoch7602: 1.9160916805267334\n",
            "Train loss for epoch7603: 1.8885698318481445\n",
            "Test loss for epoch7603: 2.2312142848968506\n",
            "Train loss for epoch7604: 1.8073979616165161\n",
            "Test loss for epoch7604: 1.999513030052185\n",
            "Train loss for epoch7605: 1.652472734451294\n",
            "Test loss for epoch7605: 2.167952299118042\n",
            "Train loss for epoch7606: 2.0967164039611816\n",
            "Test loss for epoch7606: 1.9486807584762573\n",
            "Train loss for epoch7607: 2.188230514526367\n",
            "Test loss for epoch7607: 2.019714593887329\n",
            "Train loss for epoch7608: 1.9676129817962646\n",
            "Test loss for epoch7608: 2.0266835689544678\n",
            "Train loss for epoch7609: 2.1211788654327393\n",
            "Test loss for epoch7609: 2.213071346282959\n",
            "Train loss for epoch7610: 1.6236803531646729\n",
            "Test loss for epoch7610: 2.0156362056732178\n",
            "Train loss for epoch7611: 1.9532142877578735\n",
            "Test loss for epoch7611: 2.024306058883667\n",
            "Train loss for epoch7612: 1.8646519184112549\n",
            "Test loss for epoch7612: 2.1864736080169678\n",
            "Train loss for epoch7613: 2.0147058963775635\n",
            "Test loss for epoch7613: 2.0518884658813477\n",
            "Train loss for epoch7614: 1.9801210165023804\n",
            "Test loss for epoch7614: 2.118825912475586\n",
            "Train loss for epoch7615: 2.0276851654052734\n",
            "Test loss for epoch7615: 2.1867151260375977\n",
            "Train loss for epoch7616: 1.9034202098846436\n",
            "Test loss for epoch7616: 1.9712636470794678\n",
            "Train loss for epoch7617: 2.065512180328369\n",
            "Test loss for epoch7617: 2.1010231971740723\n",
            "Train loss for epoch7618: 1.8799734115600586\n",
            "Test loss for epoch7618: 2.2004432678222656\n",
            "Train loss for epoch7619: 1.813521385192871\n",
            "Test loss for epoch7619: 2.104567050933838\n",
            "Train loss for epoch7620: 1.8346517086029053\n",
            "Test loss for epoch7620: 1.8795185089111328\n",
            "Train loss for epoch7621: 1.907016634941101\n",
            "Test loss for epoch7621: 2.132615089416504\n",
            "Train loss for epoch7622: 1.9809163808822632\n",
            "Test loss for epoch7622: 2.0360963344573975\n",
            "Train loss for epoch7623: 1.96738600730896\n",
            "Test loss for epoch7623: 1.9878339767456055\n",
            "Train loss for epoch7624: 2.0097742080688477\n",
            "Test loss for epoch7624: 2.151190996170044\n",
            "Train loss for epoch7625: 1.9726754426956177\n",
            "Test loss for epoch7625: 2.0856592655181885\n",
            "Train loss for epoch7626: 1.7839009761810303\n",
            "Test loss for epoch7626: 2.064462423324585\n",
            "Train loss for epoch7627: 2.1090962886810303\n",
            "Test loss for epoch7627: 1.9530129432678223\n",
            "Train loss for epoch7628: 1.9915926456451416\n",
            "Test loss for epoch7628: 1.8231861591339111\n",
            "Train loss for epoch7629: 1.9189168214797974\n",
            "Test loss for epoch7629: 2.1456212997436523\n",
            "Train loss for epoch7630: 1.8650751113891602\n",
            "Test loss for epoch7630: 2.0101981163024902\n",
            "Train loss for epoch7631: 2.139936923980713\n",
            "Test loss for epoch7631: 2.2189955711364746\n",
            "Train loss for epoch7632: 1.9028704166412354\n",
            "Test loss for epoch7632: 2.1859309673309326\n",
            "Train loss for epoch7633: 2.0000672340393066\n",
            "Test loss for epoch7633: 2.224905014038086\n",
            "Train loss for epoch7634: 1.9396872520446777\n",
            "Test loss for epoch7634: 2.1573033332824707\n",
            "Train loss for epoch7635: 2.0080671310424805\n",
            "Test loss for epoch7635: 2.0381855964660645\n",
            "Train loss for epoch7636: 2.139333724975586\n",
            "Test loss for epoch7636: 1.970391035079956\n",
            "Train loss for epoch7637: 1.934251070022583\n",
            "Test loss for epoch7637: 2.1310911178588867\n",
            "Train loss for epoch7638: 1.990490198135376\n",
            "Test loss for epoch7638: 2.004284381866455\n",
            "Train loss for epoch7639: 2.065364360809326\n",
            "Test loss for epoch7639: 2.0350098609924316\n",
            "Train loss for epoch7640: 1.8916579484939575\n",
            "Test loss for epoch7640: 2.051786184310913\n",
            "Train loss for epoch7641: 2.008789300918579\n",
            "Test loss for epoch7641: 2.0024819374084473\n",
            "Train loss for epoch7642: 2.021916627883911\n",
            "Test loss for epoch7642: 2.089600086212158\n",
            "Train loss for epoch7643: 1.9531861543655396\n",
            "Test loss for epoch7643: 1.991927981376648\n",
            "Train loss for epoch7644: 1.8071470260620117\n",
            "Test loss for epoch7644: 1.9535901546478271\n",
            "Train loss for epoch7645: 1.8319075107574463\n",
            "Test loss for epoch7645: 2.0776925086975098\n",
            "Train loss for epoch7646: 2.10150146484375\n",
            "Test loss for epoch7646: 2.223048686981201\n",
            "Train loss for epoch7647: 1.8947023153305054\n",
            "Test loss for epoch7647: 2.1566317081451416\n",
            "Train loss for epoch7648: 2.022495746612549\n",
            "Test loss for epoch7648: 1.9023069143295288\n",
            "Train loss for epoch7649: 2.163489580154419\n",
            "Test loss for epoch7649: 1.9361931085586548\n",
            "Train loss for epoch7650: 1.9903291463851929\n",
            "Test loss for epoch7650: 1.9672696590423584\n",
            "Train loss for epoch7651: 1.9240188598632812\n",
            "Test loss for epoch7651: 2.1154935359954834\n",
            "Train loss for epoch7652: 2.0278525352478027\n",
            "Test loss for epoch7652: 2.0222010612487793\n",
            "Train loss for epoch7653: 1.8235784769058228\n",
            "Test loss for epoch7653: 1.9285300970077515\n",
            "Train loss for epoch7654: 2.1656036376953125\n",
            "Test loss for epoch7654: 2.1114234924316406\n",
            "Train loss for epoch7655: 1.9594571590423584\n",
            "Test loss for epoch7655: 2.0080714225769043\n",
            "Train loss for epoch7656: 2.044912576675415\n",
            "Test loss for epoch7656: 1.8231955766677856\n",
            "Train loss for epoch7657: 1.9331103563308716\n",
            "Test loss for epoch7657: 1.820702075958252\n",
            "Train loss for epoch7658: 1.8832367658615112\n",
            "Test loss for epoch7658: 2.1834917068481445\n",
            "Train loss for epoch7659: 1.9114108085632324\n",
            "Test loss for epoch7659: 2.100400447845459\n",
            "Train loss for epoch7660: 1.8892688751220703\n",
            "Test loss for epoch7660: 2.072380304336548\n",
            "Train loss for epoch7661: 2.0718071460723877\n",
            "Test loss for epoch7661: 1.9793164730072021\n",
            "Train loss for epoch7662: 1.5636557340621948\n",
            "Test loss for epoch7662: 2.314222574234009\n",
            "Train loss for epoch7663: 1.8106945753097534\n",
            "Test loss for epoch7663: 1.9743257761001587\n",
            "Train loss for epoch7664: 2.155993700027466\n",
            "Test loss for epoch7664: 2.0782644748687744\n",
            "Train loss for epoch7665: 2.0093820095062256\n",
            "Test loss for epoch7665: 1.996530294418335\n",
            "Train loss for epoch7666: 2.086601495742798\n",
            "Test loss for epoch7666: 1.9371376037597656\n",
            "Train loss for epoch7667: 1.8657426834106445\n",
            "Test loss for epoch7667: 1.9151654243469238\n",
            "Train loss for epoch7668: 1.9131654500961304\n",
            "Test loss for epoch7668: 1.8785375356674194\n",
            "Train loss for epoch7669: 1.7132078409194946\n",
            "Test loss for epoch7669: 1.9909995794296265\n",
            "Train loss for epoch7670: 2.0906944274902344\n",
            "Test loss for epoch7670: 1.8905174732208252\n",
            "Train loss for epoch7671: 2.0406906604766846\n",
            "Test loss for epoch7671: 1.9277613162994385\n",
            "Train loss for epoch7672: 1.8880131244659424\n",
            "Test loss for epoch7672: 2.2551941871643066\n",
            "Train loss for epoch7673: 2.104900598526001\n",
            "Test loss for epoch7673: 2.1725289821624756\n",
            "Train loss for epoch7674: 1.924451470375061\n",
            "Test loss for epoch7674: 1.9531959295272827\n",
            "Train loss for epoch7675: 1.9579746723175049\n",
            "Test loss for epoch7675: 2.1314711570739746\n",
            "Train loss for epoch7676: 2.145521402359009\n",
            "Test loss for epoch7676: 1.948434829711914\n",
            "Train loss for epoch7677: 2.0322964191436768\n",
            "Test loss for epoch7677: 1.8257020711898804\n",
            "Train loss for epoch7678: 1.9172698259353638\n",
            "Test loss for epoch7678: 2.0953288078308105\n",
            "Train loss for epoch7679: 1.6729111671447754\n",
            "Test loss for epoch7679: 2.132070541381836\n",
            "Train loss for epoch7680: 2.003711700439453\n",
            "Test loss for epoch7680: 2.094866991043091\n",
            "Train loss for epoch7681: 1.9070543050765991\n",
            "Test loss for epoch7681: 2.115413188934326\n",
            "Train loss for epoch7682: 1.9069815874099731\n",
            "Test loss for epoch7682: 2.13391375541687\n",
            "Train loss for epoch7683: 1.8324055671691895\n",
            "Test loss for epoch7683: 2.171990156173706\n",
            "Train loss for epoch7684: 2.0057036876678467\n",
            "Test loss for epoch7684: 2.189797878265381\n",
            "Train loss for epoch7685: 1.9171866178512573\n",
            "Test loss for epoch7685: 1.8163321018218994\n",
            "Train loss for epoch7686: 2.1406517028808594\n",
            "Test loss for epoch7686: 1.926803708076477\n",
            "Train loss for epoch7687: 1.9299222230911255\n",
            "Test loss for epoch7687: 1.929166555404663\n",
            "Train loss for epoch7688: 1.8658946752548218\n",
            "Test loss for epoch7688: 1.8056035041809082\n",
            "Train loss for epoch7689: 1.868032693862915\n",
            "Test loss for epoch7689: 1.9559276103973389\n",
            "Train loss for epoch7690: 2.1051135063171387\n",
            "Test loss for epoch7690: 2.20139741897583\n",
            "Train loss for epoch7691: 2.2687363624572754\n",
            "Test loss for epoch7691: 2.0610740184783936\n",
            "Train loss for epoch7692: 1.9799562692642212\n",
            "Test loss for epoch7692: 1.9765082597732544\n",
            "Train loss for epoch7693: 2.020293951034546\n",
            "Test loss for epoch7693: 1.9375697374343872\n",
            "Train loss for epoch7694: 1.929230809211731\n",
            "Test loss for epoch7694: 2.254533529281616\n",
            "Train loss for epoch7695: 1.983230710029602\n",
            "Test loss for epoch7695: 2.043067455291748\n",
            "Train loss for epoch7696: 1.9598819017410278\n",
            "Test loss for epoch7696: 2.1638267040252686\n",
            "Train loss for epoch7697: 1.9972703456878662\n",
            "Test loss for epoch7697: 2.041635513305664\n",
            "Train loss for epoch7698: 1.8472729921340942\n",
            "Test loss for epoch7698: 2.287858724594116\n",
            "Train loss for epoch7699: 2.1063716411590576\n",
            "Test loss for epoch7699: 1.8930902481079102\n",
            "Train loss for epoch7700: 1.9778374433517456\n",
            "Test loss for epoch7700: 2.135584831237793\n",
            "Train loss for epoch7701: 1.9740519523620605\n",
            "Test loss for epoch7701: 1.942732810974121\n",
            "Train loss for epoch7702: 1.8832755088806152\n",
            "Test loss for epoch7702: 2.0746612548828125\n",
            "Train loss for epoch7703: 1.9921270608901978\n",
            "Test loss for epoch7703: 2.0563650131225586\n",
            "Train loss for epoch7704: 1.9439153671264648\n",
            "Test loss for epoch7704: 2.138681173324585\n",
            "Train loss for epoch7705: 2.1361138820648193\n",
            "Test loss for epoch7705: 2.1665570735931396\n",
            "Train loss for epoch7706: 2.05460524559021\n",
            "Test loss for epoch7706: 2.2065794467926025\n",
            "Train loss for epoch7707: 1.8226096630096436\n",
            "Test loss for epoch7707: 2.193838119506836\n",
            "Train loss for epoch7708: 2.0308713912963867\n",
            "Test loss for epoch7708: 2.2552919387817383\n",
            "Train loss for epoch7709: 2.227128028869629\n",
            "Test loss for epoch7709: 2.1486546993255615\n",
            "Train loss for epoch7710: 2.04546856880188\n",
            "Test loss for epoch7710: 1.8535045385360718\n",
            "Train loss for epoch7711: 2.187964916229248\n",
            "Test loss for epoch7711: 2.1206016540527344\n",
            "Train loss for epoch7712: 2.177216053009033\n",
            "Test loss for epoch7712: 1.9880480766296387\n",
            "Train loss for epoch7713: 1.883510708808899\n",
            "Test loss for epoch7713: 1.975983738899231\n",
            "Train loss for epoch7714: 1.7631402015686035\n",
            "Test loss for epoch7714: 1.8309688568115234\n",
            "Train loss for epoch7715: 2.136164665222168\n",
            "Test loss for epoch7715: 2.0848281383514404\n",
            "Train loss for epoch7716: 1.8409698009490967\n",
            "Test loss for epoch7716: 2.1309988498687744\n",
            "Train loss for epoch7717: 1.7037811279296875\n",
            "Test loss for epoch7717: 2.206857919692993\n",
            "Train loss for epoch7718: 1.8373619318008423\n",
            "Test loss for epoch7718: 2.2130887508392334\n",
            "Train loss for epoch7719: 1.7317421436309814\n",
            "Test loss for epoch7719: 1.9773354530334473\n",
            "Train loss for epoch7720: 2.0035836696624756\n",
            "Test loss for epoch7720: 2.0879690647125244\n",
            "Train loss for epoch7721: 2.101724863052368\n",
            "Test loss for epoch7721: 1.8664448261260986\n",
            "Train loss for epoch7722: 2.066054105758667\n",
            "Test loss for epoch7722: 2.0674619674682617\n",
            "Train loss for epoch7723: 2.0035290718078613\n",
            "Test loss for epoch7723: 1.9712156057357788\n",
            "Train loss for epoch7724: 2.2262094020843506\n",
            "Test loss for epoch7724: 1.905600905418396\n",
            "Train loss for epoch7725: 1.846110463142395\n",
            "Test loss for epoch7725: 1.987701654434204\n",
            "Train loss for epoch7726: 1.85883367061615\n",
            "Test loss for epoch7726: 2.043684959411621\n",
            "Train loss for epoch7727: 1.905795931816101\n",
            "Test loss for epoch7727: 1.9764950275421143\n",
            "Train loss for epoch7728: 1.7860262393951416\n",
            "Test loss for epoch7728: 1.9302904605865479\n",
            "Train loss for epoch7729: 2.1989619731903076\n",
            "Test loss for epoch7729: 2.1743156909942627\n",
            "Train loss for epoch7730: 1.7590980529785156\n",
            "Test loss for epoch7730: 1.9720001220703125\n",
            "Train loss for epoch7731: 1.982835292816162\n",
            "Test loss for epoch7731: 1.9514477252960205\n",
            "Train loss for epoch7732: 2.0797927379608154\n",
            "Test loss for epoch7732: 2.133861780166626\n",
            "Train loss for epoch7733: 2.0849852561950684\n",
            "Test loss for epoch7733: 1.9821723699569702\n",
            "Train loss for epoch7734: 2.0462467670440674\n",
            "Test loss for epoch7734: 2.153610944747925\n",
            "Train loss for epoch7735: 1.677091360092163\n",
            "Test loss for epoch7735: 2.0136871337890625\n",
            "Train loss for epoch7736: 1.8682949542999268\n",
            "Test loss for epoch7736: 2.1305761337280273\n",
            "Train loss for epoch7737: 2.0442428588867188\n",
            "Test loss for epoch7737: 1.9702471494674683\n",
            "Train loss for epoch7738: 2.001840829849243\n",
            "Test loss for epoch7738: 2.011928081512451\n",
            "Train loss for epoch7739: 1.9559186697006226\n",
            "Test loss for epoch7739: 2.0393545627593994\n",
            "Train loss for epoch7740: 1.8937565088272095\n",
            "Test loss for epoch7740: 1.9149596691131592\n",
            "Train loss for epoch7741: 1.9266387224197388\n",
            "Test loss for epoch7741: 2.163349151611328\n",
            "Train loss for epoch7742: 1.9577457904815674\n",
            "Test loss for epoch7742: 2.135071039199829\n",
            "Train loss for epoch7743: 1.9811826944351196\n",
            "Test loss for epoch7743: 1.9664653539657593\n",
            "Train loss for epoch7744: 1.9755445718765259\n",
            "Test loss for epoch7744: 2.142631769180298\n",
            "Train loss for epoch7745: 1.7725410461425781\n",
            "Test loss for epoch7745: 2.2138173580169678\n",
            "Train loss for epoch7746: 1.8443949222564697\n",
            "Test loss for epoch7746: 1.9612547159194946\n",
            "Train loss for epoch7747: 2.077086925506592\n",
            "Test loss for epoch7747: 2.1387135982513428\n",
            "Train loss for epoch7748: 2.0555970668792725\n",
            "Test loss for epoch7748: 1.9932689666748047\n",
            "Train loss for epoch7749: 1.815370798110962\n",
            "Test loss for epoch7749: 2.068281412124634\n",
            "Train loss for epoch7750: 2.019758462905884\n",
            "Test loss for epoch7750: 2.179267168045044\n",
            "Train loss for epoch7751: 2.203360080718994\n",
            "Test loss for epoch7751: 2.180117130279541\n",
            "Train loss for epoch7752: 1.8578007221221924\n",
            "Test loss for epoch7752: 2.0237934589385986\n",
            "Train loss for epoch7753: 2.048510789871216\n",
            "Test loss for epoch7753: 2.0252130031585693\n",
            "Train loss for epoch7754: 1.903723120689392\n",
            "Test loss for epoch7754: 1.9182552099227905\n",
            "Train loss for epoch7755: 1.8797303438186646\n",
            "Test loss for epoch7755: 2.0375723838806152\n",
            "Train loss for epoch7756: 2.055088520050049\n",
            "Test loss for epoch7756: 2.0774576663970947\n",
            "Train loss for epoch7757: 1.9492638111114502\n",
            "Test loss for epoch7757: 1.9672492742538452\n",
            "Train loss for epoch7758: 1.859435796737671\n",
            "Test loss for epoch7758: 2.0878686904907227\n",
            "Train loss for epoch7759: 2.0214052200317383\n",
            "Test loss for epoch7759: 2.0044219493865967\n",
            "Train loss for epoch7760: 1.9842426776885986\n",
            "Test loss for epoch7760: 2.184356212615967\n",
            "Train loss for epoch7761: 1.9202896356582642\n",
            "Test loss for epoch7761: 2.2370667457580566\n",
            "Train loss for epoch7762: 2.0606627464294434\n",
            "Test loss for epoch7762: 2.1279826164245605\n",
            "Train loss for epoch7763: 2.128955125808716\n",
            "Test loss for epoch7763: 1.992248773574829\n",
            "Train loss for epoch7764: 1.9616104364395142\n",
            "Test loss for epoch7764: 2.273697853088379\n",
            "Train loss for epoch7765: 1.960859775543213\n",
            "Test loss for epoch7765: 2.0884315967559814\n",
            "Train loss for epoch7766: 1.8463951349258423\n",
            "Test loss for epoch7766: 1.9340871572494507\n",
            "Train loss for epoch7767: 1.9424406290054321\n",
            "Test loss for epoch7767: 1.8478409051895142\n",
            "Train loss for epoch7768: 1.956673502922058\n",
            "Test loss for epoch7768: 1.8207885026931763\n",
            "Train loss for epoch7769: 1.9619596004486084\n",
            "Test loss for epoch7769: 2.089843511581421\n",
            "Train loss for epoch7770: 2.0777997970581055\n",
            "Test loss for epoch7770: 1.996766209602356\n",
            "Train loss for epoch7771: 1.8435838222503662\n",
            "Test loss for epoch7771: 2.0710229873657227\n",
            "Train loss for epoch7772: 1.9177944660186768\n",
            "Test loss for epoch7772: 2.0481417179107666\n",
            "Train loss for epoch7773: 2.0210697650909424\n",
            "Test loss for epoch7773: 2.1958706378936768\n",
            "Train loss for epoch7774: 2.1144955158233643\n",
            "Test loss for epoch7774: 2.2593793869018555\n",
            "Train loss for epoch7775: 1.891410231590271\n",
            "Test loss for epoch7775: 2.0882766246795654\n",
            "Train loss for epoch7776: 2.143514633178711\n",
            "Test loss for epoch7776: 2.0463550090789795\n",
            "Train loss for epoch7777: 2.1257665157318115\n",
            "Test loss for epoch7777: 1.9296282529830933\n",
            "Train loss for epoch7778: 1.812872290611267\n",
            "Test loss for epoch7778: 1.914639949798584\n",
            "Train loss for epoch7779: 2.0942652225494385\n",
            "Test loss for epoch7779: 2.0626466274261475\n",
            "Train loss for epoch7780: 1.8021788597106934\n",
            "Test loss for epoch7780: 2.1820690631866455\n",
            "Train loss for epoch7781: 1.8985965251922607\n",
            "Test loss for epoch7781: 1.9201940298080444\n",
            "Train loss for epoch7782: 1.9600509405136108\n",
            "Test loss for epoch7782: 2.0013155937194824\n",
            "Train loss for epoch7783: 1.9704337120056152\n",
            "Test loss for epoch7783: 2.241628646850586\n",
            "Train loss for epoch7784: 1.9902267456054688\n",
            "Test loss for epoch7784: 2.077517509460449\n",
            "Train loss for epoch7785: 1.9794862270355225\n",
            "Test loss for epoch7785: 2.169762134552002\n",
            "Train loss for epoch7786: 1.8811774253845215\n",
            "Test loss for epoch7786: 2.0336453914642334\n",
            "Train loss for epoch7787: 1.971930980682373\n",
            "Test loss for epoch7787: 1.9037585258483887\n",
            "Train loss for epoch7788: 1.9439623355865479\n",
            "Test loss for epoch7788: 2.3989715576171875\n",
            "Train loss for epoch7789: 2.1006147861480713\n",
            "Test loss for epoch7789: 2.0213191509246826\n",
            "Train loss for epoch7790: 2.3083341121673584\n",
            "Test loss for epoch7790: 2.0398871898651123\n",
            "Train loss for epoch7791: 2.025512218475342\n",
            "Test loss for epoch7791: 2.1568171977996826\n",
            "Train loss for epoch7792: 1.8887345790863037\n",
            "Test loss for epoch7792: 2.1105470657348633\n",
            "Train loss for epoch7793: 1.8144001960754395\n",
            "Test loss for epoch7793: 2.278428077697754\n",
            "Train loss for epoch7794: 1.8866907358169556\n",
            "Test loss for epoch7794: 1.9880343675613403\n",
            "Train loss for epoch7795: 1.9587512016296387\n",
            "Test loss for epoch7795: 1.9922544956207275\n",
            "Train loss for epoch7796: 1.8745577335357666\n",
            "Test loss for epoch7796: 1.7594749927520752\n",
            "Train loss for epoch7797: 1.9395437240600586\n",
            "Test loss for epoch7797: 1.9546440839767456\n",
            "Train loss for epoch7798: 1.9855690002441406\n",
            "Test loss for epoch7798: 2.0754125118255615\n",
            "Train loss for epoch7799: 1.8089567422866821\n",
            "Test loss for epoch7799: 2.013697624206543\n",
            "Train loss for epoch7800: 1.8586300611495972\n",
            "Test loss for epoch7800: 2.0044898986816406\n",
            "Train loss for epoch7801: 2.0687780380249023\n",
            "Test loss for epoch7801: 2.0253632068634033\n",
            "Train loss for epoch7802: 2.033979892730713\n",
            "Test loss for epoch7802: 1.9129486083984375\n",
            "Train loss for epoch7803: 1.9735236167907715\n",
            "Test loss for epoch7803: 2.008335828781128\n",
            "Train loss for epoch7804: 2.1166810989379883\n",
            "Test loss for epoch7804: 2.0602777004241943\n",
            "Train loss for epoch7805: 1.8517155647277832\n",
            "Test loss for epoch7805: 2.1450788974761963\n",
            "Train loss for epoch7806: 2.066269874572754\n",
            "Test loss for epoch7806: 2.114450216293335\n",
            "Train loss for epoch7807: 2.0516045093536377\n",
            "Test loss for epoch7807: 2.1152396202087402\n",
            "Train loss for epoch7808: 1.8783398866653442\n",
            "Test loss for epoch7808: 2.0372865200042725\n",
            "Train loss for epoch7809: 1.7151345014572144\n",
            "Test loss for epoch7809: 2.1655683517456055\n",
            "Train loss for epoch7810: 1.9579341411590576\n",
            "Test loss for epoch7810: 2.024498224258423\n",
            "Train loss for epoch7811: 1.8292100429534912\n",
            "Test loss for epoch7811: 2.0412633419036865\n",
            "Train loss for epoch7812: 2.0287277698516846\n",
            "Test loss for epoch7812: 2.176546573638916\n",
            "Train loss for epoch7813: 1.9503471851348877\n",
            "Test loss for epoch7813: 2.022045373916626\n",
            "Train loss for epoch7814: 1.9556282758712769\n",
            "Test loss for epoch7814: 1.9927984476089478\n",
            "Train loss for epoch7815: 1.8839584589004517\n",
            "Test loss for epoch7815: 1.8664079904556274\n",
            "Train loss for epoch7816: 1.7825673818588257\n",
            "Test loss for epoch7816: 1.9675809144973755\n",
            "Train loss for epoch7817: 1.8647435903549194\n",
            "Test loss for epoch7817: 2.0881361961364746\n",
            "Train loss for epoch7818: 1.9979736804962158\n",
            "Test loss for epoch7818: 2.0957911014556885\n",
            "Train loss for epoch7819: 1.8761450052261353\n",
            "Test loss for epoch7819: 1.984885334968567\n",
            "Train loss for epoch7820: 1.896618127822876\n",
            "Test loss for epoch7820: 1.9771223068237305\n",
            "Train loss for epoch7821: 2.0445375442504883\n",
            "Test loss for epoch7821: 1.8786165714263916\n",
            "Train loss for epoch7822: 2.002241849899292\n",
            "Test loss for epoch7822: 2.037456750869751\n",
            "Train loss for epoch7823: 2.0963690280914307\n",
            "Test loss for epoch7823: 2.166403293609619\n",
            "Train loss for epoch7824: 2.177608013153076\n",
            "Test loss for epoch7824: 2.13793683052063\n",
            "Train loss for epoch7825: 1.9086358547210693\n",
            "Test loss for epoch7825: 1.916902780532837\n",
            "Train loss for epoch7826: 1.8949472904205322\n",
            "Test loss for epoch7826: 2.227799415588379\n",
            "Train loss for epoch7827: 1.8017308712005615\n",
            "Test loss for epoch7827: 2.026116132736206\n",
            "Train loss for epoch7828: 2.091139316558838\n",
            "Test loss for epoch7828: 1.9290467500686646\n",
            "Train loss for epoch7829: 2.0543789863586426\n",
            "Test loss for epoch7829: 2.0968921184539795\n",
            "Train loss for epoch7830: 1.8546196222305298\n",
            "Test loss for epoch7830: 1.8982646465301514\n",
            "Train loss for epoch7831: 2.0206522941589355\n",
            "Test loss for epoch7831: 2.185342311859131\n",
            "Train loss for epoch7832: 1.9917834997177124\n",
            "Test loss for epoch7832: 1.9142048358917236\n",
            "Train loss for epoch7833: 1.972000002861023\n",
            "Test loss for epoch7833: 2.184802293777466\n",
            "Train loss for epoch7834: 1.9356567859649658\n",
            "Test loss for epoch7834: 1.9875521659851074\n",
            "Train loss for epoch7835: 2.2378761768341064\n",
            "Test loss for epoch7835: 2.1328768730163574\n",
            "Train loss for epoch7836: 1.9795598983764648\n",
            "Test loss for epoch7836: 2.143198013305664\n",
            "Train loss for epoch7837: 2.1107702255249023\n",
            "Test loss for epoch7837: 1.8344284296035767\n",
            "Train loss for epoch7838: 2.145887851715088\n",
            "Test loss for epoch7838: 2.229809522628784\n",
            "Train loss for epoch7839: 1.8194187879562378\n",
            "Test loss for epoch7839: 2.118483781814575\n",
            "Train loss for epoch7840: 2.0829973220825195\n",
            "Test loss for epoch7840: 1.92617666721344\n",
            "Train loss for epoch7841: 1.988612413406372\n",
            "Test loss for epoch7841: 2.030133008956909\n",
            "Train loss for epoch7842: 2.1635923385620117\n",
            "Test loss for epoch7842: 2.045520305633545\n",
            "Train loss for epoch7843: 2.059497833251953\n",
            "Test loss for epoch7843: 2.226313591003418\n",
            "Train loss for epoch7844: 1.96928071975708\n",
            "Test loss for epoch7844: 1.6979529857635498\n",
            "Train loss for epoch7845: 2.0824925899505615\n",
            "Test loss for epoch7845: 2.046356201171875\n",
            "Train loss for epoch7846: 1.839514136314392\n",
            "Test loss for epoch7846: 2.149747848510742\n",
            "Train loss for epoch7847: 2.019970655441284\n",
            "Test loss for epoch7847: 2.042161464691162\n",
            "Train loss for epoch7848: 1.821691870689392\n",
            "Test loss for epoch7848: 2.0797500610351562\n",
            "Train loss for epoch7849: 2.0027716159820557\n",
            "Test loss for epoch7849: 2.0186848640441895\n",
            "Train loss for epoch7850: 2.0345511436462402\n",
            "Test loss for epoch7850: 1.9675626754760742\n",
            "Train loss for epoch7851: 1.944219946861267\n",
            "Test loss for epoch7851: 1.9572789669036865\n",
            "Train loss for epoch7852: 1.88543701171875\n",
            "Test loss for epoch7852: 2.034050941467285\n",
            "Train loss for epoch7853: 1.949580192565918\n",
            "Test loss for epoch7853: 2.2294507026672363\n",
            "Train loss for epoch7854: 2.043792486190796\n",
            "Test loss for epoch7854: 2.112668514251709\n",
            "Train loss for epoch7855: 1.956789493560791\n",
            "Test loss for epoch7855: 2.0995266437530518\n",
            "Train loss for epoch7856: 1.965599536895752\n",
            "Test loss for epoch7856: 1.9624980688095093\n",
            "Train loss for epoch7857: 1.7447421550750732\n",
            "Test loss for epoch7857: 1.8160521984100342\n",
            "Train loss for epoch7858: 1.914595127105713\n",
            "Test loss for epoch7858: 1.9807946681976318\n",
            "Train loss for epoch7859: 1.8533233404159546\n",
            "Test loss for epoch7859: 2.143745183944702\n",
            "Train loss for epoch7860: 1.8126124143600464\n",
            "Test loss for epoch7860: 2.248378038406372\n",
            "Train loss for epoch7861: 1.7711248397827148\n",
            "Test loss for epoch7861: 2.042069435119629\n",
            "Train loss for epoch7862: 1.9730443954467773\n",
            "Test loss for epoch7862: 2.06455659866333\n",
            "Train loss for epoch7863: 2.1836941242218018\n",
            "Test loss for epoch7863: 2.103766918182373\n",
            "Train loss for epoch7864: 2.054243326187134\n",
            "Test loss for epoch7864: 1.8651654720306396\n",
            "Train loss for epoch7865: 2.016197443008423\n",
            "Test loss for epoch7865: 1.9415003061294556\n",
            "Train loss for epoch7866: 1.9757288694381714\n",
            "Test loss for epoch7866: 1.959471344947815\n",
            "Train loss for epoch7867: 1.9188036918640137\n",
            "Test loss for epoch7867: 2.1324896812438965\n",
            "Train loss for epoch7868: 2.0568974018096924\n",
            "Test loss for epoch7868: 1.8313740491867065\n",
            "Train loss for epoch7869: 2.059802293777466\n",
            "Test loss for epoch7869: 2.107717275619507\n",
            "Train loss for epoch7870: 1.9693975448608398\n",
            "Test loss for epoch7870: 2.0023610591888428\n",
            "Train loss for epoch7871: 2.0964531898498535\n",
            "Test loss for epoch7871: 1.9931457042694092\n",
            "Train loss for epoch7872: 2.0082504749298096\n",
            "Test loss for epoch7872: 2.0807888507843018\n",
            "Train loss for epoch7873: 1.9465383291244507\n",
            "Test loss for epoch7873: 2.101423978805542\n",
            "Train loss for epoch7874: 2.217134952545166\n",
            "Test loss for epoch7874: 2.020618200302124\n",
            "Train loss for epoch7875: 1.8623394966125488\n",
            "Test loss for epoch7875: 2.141188144683838\n",
            "Train loss for epoch7876: 2.167006731033325\n",
            "Test loss for epoch7876: 1.7756023406982422\n",
            "Train loss for epoch7877: 1.7590574026107788\n",
            "Test loss for epoch7877: 2.096902847290039\n",
            "Train loss for epoch7878: 2.0175299644470215\n",
            "Test loss for epoch7878: 1.9935895204544067\n",
            "Train loss for epoch7879: 1.9097185134887695\n",
            "Test loss for epoch7879: 1.9737671613693237\n",
            "Train loss for epoch7880: 1.9352251291275024\n",
            "Test loss for epoch7880: 2.032426357269287\n",
            "Train loss for epoch7881: 2.0685019493103027\n",
            "Test loss for epoch7881: 2.0984063148498535\n",
            "Train loss for epoch7882: 1.841632604598999\n",
            "Test loss for epoch7882: 1.8284318447113037\n",
            "Train loss for epoch7883: 1.8635773658752441\n",
            "Test loss for epoch7883: 1.939592719078064\n",
            "Train loss for epoch7884: 1.9721765518188477\n",
            "Test loss for epoch7884: 1.9352819919586182\n",
            "Train loss for epoch7885: 2.0276224613189697\n",
            "Test loss for epoch7885: 1.9877707958221436\n",
            "Train loss for epoch7886: 1.7964979410171509\n",
            "Test loss for epoch7886: 1.92961847782135\n",
            "Train loss for epoch7887: 2.068950891494751\n",
            "Test loss for epoch7887: 1.9124915599822998\n",
            "Train loss for epoch7888: 1.8981239795684814\n",
            "Test loss for epoch7888: 2.1268582344055176\n",
            "Train loss for epoch7889: 1.954354166984558\n",
            "Test loss for epoch7889: 2.07161021232605\n",
            "Train loss for epoch7890: 2.039747714996338\n",
            "Test loss for epoch7890: 2.1721224784851074\n",
            "Train loss for epoch7891: 1.8542683124542236\n",
            "Test loss for epoch7891: 2.0960495471954346\n",
            "Train loss for epoch7892: 1.8978033065795898\n",
            "Test loss for epoch7892: 1.9237565994262695\n",
            "Train loss for epoch7893: 1.8794854879379272\n",
            "Test loss for epoch7893: 1.9812889099121094\n",
            "Train loss for epoch7894: 1.850041389465332\n",
            "Test loss for epoch7894: 2.015554904937744\n",
            "Train loss for epoch7895: 1.8677994012832642\n",
            "Test loss for epoch7895: 2.083042860031128\n",
            "Train loss for epoch7896: 2.0591933727264404\n",
            "Test loss for epoch7896: 2.135317325592041\n",
            "Train loss for epoch7897: 2.0560824871063232\n",
            "Test loss for epoch7897: 2.0808494091033936\n",
            "Train loss for epoch7898: 1.8993278741836548\n",
            "Test loss for epoch7898: 2.045142412185669\n",
            "Train loss for epoch7899: 1.8400777578353882\n",
            "Test loss for epoch7899: 2.1017706394195557\n",
            "Train loss for epoch7900: 1.8820980787277222\n",
            "Test loss for epoch7900: 1.9161468744277954\n",
            "Train loss for epoch7901: 1.9056824445724487\n",
            "Test loss for epoch7901: 2.155506134033203\n",
            "Train loss for epoch7902: 2.201685905456543\n",
            "Test loss for epoch7902: 2.0802347660064697\n",
            "Train loss for epoch7903: 1.9130253791809082\n",
            "Test loss for epoch7903: 1.998059868812561\n",
            "Train loss for epoch7904: 2.0266919136047363\n",
            "Test loss for epoch7904: 1.9638993740081787\n",
            "Train loss for epoch7905: 1.977886438369751\n",
            "Test loss for epoch7905: 2.126579999923706\n",
            "Train loss for epoch7906: 1.906502366065979\n",
            "Test loss for epoch7906: 1.923187017440796\n",
            "Train loss for epoch7907: 1.8309104442596436\n",
            "Test loss for epoch7907: 1.9947291612625122\n",
            "Train loss for epoch7908: 2.0275254249572754\n",
            "Test loss for epoch7908: 2.047112464904785\n",
            "Train loss for epoch7909: 1.8951808214187622\n",
            "Test loss for epoch7909: 1.9378098249435425\n",
            "Train loss for epoch7910: 1.8304613828659058\n",
            "Test loss for epoch7910: 2.1316990852355957\n",
            "Train loss for epoch7911: 2.083857297897339\n",
            "Test loss for epoch7911: 2.087833881378174\n",
            "Train loss for epoch7912: 2.177229404449463\n",
            "Test loss for epoch7912: 2.121549129486084\n",
            "Train loss for epoch7913: 1.9509637355804443\n",
            "Test loss for epoch7913: 2.183088779449463\n",
            "Train loss for epoch7914: 2.0762839317321777\n",
            "Test loss for epoch7914: 2.032641649246216\n",
            "Train loss for epoch7915: 1.9736864566802979\n",
            "Test loss for epoch7915: 1.9904550313949585\n",
            "Train loss for epoch7916: 2.0924441814422607\n",
            "Test loss for epoch7916: 2.018558979034424\n",
            "Train loss for epoch7917: 1.9911227226257324\n",
            "Test loss for epoch7917: 1.9776787757873535\n",
            "Train loss for epoch7918: 1.9045977592468262\n",
            "Test loss for epoch7918: 2.0294060707092285\n",
            "Train loss for epoch7919: 1.7264736890792847\n",
            "Test loss for epoch7919: 1.9341182708740234\n",
            "Train loss for epoch7920: 2.0773346424102783\n",
            "Test loss for epoch7920: 2.0766749382019043\n",
            "Train loss for epoch7921: 1.9970253705978394\n",
            "Test loss for epoch7921: 2.006514549255371\n",
            "Train loss for epoch7922: 2.037234306335449\n",
            "Test loss for epoch7922: 1.9933570623397827\n",
            "Train loss for epoch7923: 2.080512762069702\n",
            "Test loss for epoch7923: 2.050609827041626\n",
            "Train loss for epoch7924: 2.1682674884796143\n",
            "Test loss for epoch7924: 1.7873711585998535\n",
            "Train loss for epoch7925: 2.2793350219726562\n",
            "Test loss for epoch7925: 2.1776211261749268\n",
            "Train loss for epoch7926: 2.0640218257904053\n",
            "Test loss for epoch7926: 2.130452871322632\n",
            "Train loss for epoch7927: 1.8909990787506104\n",
            "Test loss for epoch7927: 2.0495660305023193\n",
            "Train loss for epoch7928: 2.1383049488067627\n",
            "Test loss for epoch7928: 2.113905191421509\n",
            "Train loss for epoch7929: 1.7945690155029297\n",
            "Test loss for epoch7929: 2.058344602584839\n",
            "Train loss for epoch7930: 2.0948870182037354\n",
            "Test loss for epoch7930: 2.0790679454803467\n",
            "Train loss for epoch7931: 1.9888801574707031\n",
            "Test loss for epoch7931: 2.1435725688934326\n",
            "Train loss for epoch7932: 2.0182974338531494\n",
            "Test loss for epoch7932: 1.918501377105713\n",
            "Train loss for epoch7933: 1.9659206867218018\n",
            "Test loss for epoch7933: 2.1871681213378906\n",
            "Train loss for epoch7934: 1.9861247539520264\n",
            "Test loss for epoch7934: 2.212301731109619\n",
            "Train loss for epoch7935: 2.2211358547210693\n",
            "Test loss for epoch7935: 1.994394063949585\n",
            "Train loss for epoch7936: 1.8294676542282104\n",
            "Test loss for epoch7936: 1.7911540269851685\n",
            "Train loss for epoch7937: 1.9190795421600342\n",
            "Test loss for epoch7937: 2.0307984352111816\n",
            "Train loss for epoch7938: 1.9870643615722656\n",
            "Test loss for epoch7938: 1.9814826250076294\n",
            "Train loss for epoch7939: 1.953410267829895\n",
            "Test loss for epoch7939: 2.1297836303710938\n",
            "Train loss for epoch7940: 1.8624898195266724\n",
            "Test loss for epoch7940: 2.070963144302368\n",
            "Train loss for epoch7941: 1.8087048530578613\n",
            "Test loss for epoch7941: 1.9268510341644287\n",
            "Train loss for epoch7942: 2.1256065368652344\n",
            "Test loss for epoch7942: 2.0491585731506348\n",
            "Train loss for epoch7943: 2.040822982788086\n",
            "Test loss for epoch7943: 1.9812639951705933\n",
            "Train loss for epoch7944: 2.2204837799072266\n",
            "Test loss for epoch7944: 2.0639288425445557\n",
            "Train loss for epoch7945: 1.924592137336731\n",
            "Test loss for epoch7945: 2.1287436485290527\n",
            "Train loss for epoch7946: 1.8709696531295776\n",
            "Test loss for epoch7946: 2.059591293334961\n",
            "Train loss for epoch7947: 2.167325735092163\n",
            "Test loss for epoch7947: 2.0175693035125732\n",
            "Train loss for epoch7948: 2.2057878971099854\n",
            "Test loss for epoch7948: 2.095057964324951\n",
            "Train loss for epoch7949: 1.9422805309295654\n",
            "Test loss for epoch7949: 1.9083318710327148\n",
            "Train loss for epoch7950: 2.2176852226257324\n",
            "Test loss for epoch7950: 2.0721185207366943\n",
            "Train loss for epoch7951: 2.1314454078674316\n",
            "Test loss for epoch7951: 2.011815071105957\n",
            "Train loss for epoch7952: 1.8494045734405518\n",
            "Test loss for epoch7952: 2.181671380996704\n",
            "Train loss for epoch7953: 1.9640504121780396\n",
            "Test loss for epoch7953: 2.1751303672790527\n",
            "Train loss for epoch7954: 2.1095781326293945\n",
            "Test loss for epoch7954: 1.9990098476409912\n",
            "Train loss for epoch7955: 2.11014723777771\n",
            "Test loss for epoch7955: 1.954908847808838\n",
            "Train loss for epoch7956: 1.7460813522338867\n",
            "Test loss for epoch7956: 2.0329713821411133\n",
            "Train loss for epoch7957: 1.853909969329834\n",
            "Test loss for epoch7957: 2.1193149089813232\n",
            "Train loss for epoch7958: 2.093498468399048\n",
            "Test loss for epoch7958: 1.8419947624206543\n",
            "Train loss for epoch7959: 2.175119400024414\n",
            "Test loss for epoch7959: 2.0623116493225098\n",
            "Train loss for epoch7960: 1.976389765739441\n",
            "Test loss for epoch7960: 2.102315902709961\n",
            "Train loss for epoch7961: 1.9055808782577515\n",
            "Test loss for epoch7961: 2.0495221614837646\n",
            "Train loss for epoch7962: 1.8989671468734741\n",
            "Test loss for epoch7962: 2.085310459136963\n",
            "Train loss for epoch7963: 1.9374889135360718\n",
            "Test loss for epoch7963: 2.1381640434265137\n",
            "Train loss for epoch7964: 1.8862844705581665\n",
            "Test loss for epoch7964: 2.1096222400665283\n",
            "Train loss for epoch7965: 2.182335138320923\n",
            "Test loss for epoch7965: 1.9455244541168213\n",
            "Train loss for epoch7966: 1.8522151708602905\n",
            "Test loss for epoch7966: 2.0317461490631104\n",
            "Train loss for epoch7967: 2.003371238708496\n",
            "Test loss for epoch7967: 2.102423906326294\n",
            "Train loss for epoch7968: 1.9706618785858154\n",
            "Test loss for epoch7968: 2.0619471073150635\n",
            "Train loss for epoch7969: 2.04209041595459\n",
            "Test loss for epoch7969: 2.0650641918182373\n",
            "Train loss for epoch7970: 2.148834466934204\n",
            "Test loss for epoch7970: 2.10128116607666\n",
            "Train loss for epoch7971: 2.062662124633789\n",
            "Test loss for epoch7971: 1.9081120491027832\n",
            "Train loss for epoch7972: 2.0941405296325684\n",
            "Test loss for epoch7972: 2.1187803745269775\n",
            "Train loss for epoch7973: 2.1592817306518555\n",
            "Test loss for epoch7973: 2.091195583343506\n",
            "Train loss for epoch7974: 1.825920820236206\n",
            "Test loss for epoch7974: 1.858989953994751\n",
            "Train loss for epoch7975: 1.934893012046814\n",
            "Test loss for epoch7975: 1.9844826459884644\n",
            "Train loss for epoch7976: 1.9280946254730225\n",
            "Test loss for epoch7976: 2.0376977920532227\n",
            "Train loss for epoch7977: 2.030409336090088\n",
            "Test loss for epoch7977: 2.181988000869751\n",
            "Train loss for epoch7978: 1.9775478839874268\n",
            "Test loss for epoch7978: 2.021824836730957\n",
            "Train loss for epoch7979: 2.008410692214966\n",
            "Test loss for epoch7979: 2.0507102012634277\n",
            "Train loss for epoch7980: 1.945006012916565\n",
            "Test loss for epoch7980: 2.108828544616699\n",
            "Train loss for epoch7981: 1.848293423652649\n",
            "Test loss for epoch7981: 2.099114418029785\n",
            "Train loss for epoch7982: 1.986242413520813\n",
            "Test loss for epoch7982: 1.7955505847930908\n",
            "Train loss for epoch7983: 2.0585174560546875\n",
            "Test loss for epoch7983: 2.0890915393829346\n",
            "Train loss for epoch7984: 1.94516122341156\n",
            "Test loss for epoch7984: 1.959311842918396\n",
            "Train loss for epoch7985: 1.7582595348358154\n",
            "Test loss for epoch7985: 2.0587687492370605\n",
            "Train loss for epoch7986: 1.9535273313522339\n",
            "Test loss for epoch7986: 2.189836263656616\n",
            "Train loss for epoch7987: 2.070894718170166\n",
            "Test loss for epoch7987: 2.1126303672790527\n",
            "Train loss for epoch7988: 1.787081241607666\n",
            "Test loss for epoch7988: 1.9999275207519531\n",
            "Train loss for epoch7989: 2.0082011222839355\n",
            "Test loss for epoch7989: 1.9998054504394531\n",
            "Train loss for epoch7990: 1.8244953155517578\n",
            "Test loss for epoch7990: 1.9135305881500244\n",
            "Train loss for epoch7991: 1.9734010696411133\n",
            "Test loss for epoch7991: 2.074437141418457\n",
            "Train loss for epoch7992: 1.9505828619003296\n",
            "Test loss for epoch7992: 2.188163995742798\n",
            "Train loss for epoch7993: 1.8544780015945435\n",
            "Test loss for epoch7993: 2.119729518890381\n",
            "Train loss for epoch7994: 2.3228776454925537\n",
            "Test loss for epoch7994: 2.154564619064331\n",
            "Train loss for epoch7995: 2.1227829456329346\n",
            "Test loss for epoch7995: 1.9337809085845947\n",
            "Train loss for epoch7996: 2.092268228530884\n",
            "Test loss for epoch7996: 2.1191937923431396\n",
            "Train loss for epoch7997: 1.949552297592163\n",
            "Test loss for epoch7997: 2.0065133571624756\n",
            "Train loss for epoch7998: 2.0594561100006104\n",
            "Test loss for epoch7998: 2.0960519313812256\n",
            "Train loss for epoch7999: 2.0157761573791504\n",
            "Test loss for epoch7999: 1.9532382488250732\n",
            "Train loss for epoch8000: 1.8813408613204956\n",
            "Test loss for epoch8000: 2.327477216720581\n",
            "Train loss for epoch8001: 1.9724841117858887\n",
            "Test loss for epoch8001: 2.055790901184082\n",
            "Train loss for epoch8002: 1.8035619258880615\n",
            "Test loss for epoch8002: 2.3266761302948\n",
            "Train loss for epoch8003: 1.9492769241333008\n",
            "Test loss for epoch8003: 2.1489205360412598\n",
            "Train loss for epoch8004: 2.240095376968384\n",
            "Test loss for epoch8004: 2.073086977005005\n",
            "Train loss for epoch8005: 1.8546572923660278\n",
            "Test loss for epoch8005: 2.1870129108428955\n",
            "Train loss for epoch8006: 1.803231954574585\n",
            "Test loss for epoch8006: 2.0058820247650146\n",
            "Train loss for epoch8007: 1.9152145385742188\n",
            "Test loss for epoch8007: 1.9930130243301392\n",
            "Train loss for epoch8008: 1.9006327390670776\n",
            "Test loss for epoch8008: 2.260869026184082\n",
            "Train loss for epoch8009: 2.00026273727417\n",
            "Test loss for epoch8009: 1.9875493049621582\n",
            "Train loss for epoch8010: 1.9952985048294067\n",
            "Test loss for epoch8010: 2.0065536499023438\n",
            "Train loss for epoch8011: 1.9156701564788818\n",
            "Test loss for epoch8011: 2.0874760150909424\n",
            "Train loss for epoch8012: 1.9766077995300293\n",
            "Test loss for epoch8012: 2.1048176288604736\n",
            "Train loss for epoch8013: 2.115652322769165\n",
            "Test loss for epoch8013: 1.931592583656311\n",
            "Train loss for epoch8014: 2.0150394439697266\n",
            "Test loss for epoch8014: 2.5061745643615723\n",
            "Train loss for epoch8015: 1.9932574033737183\n",
            "Test loss for epoch8015: 2.09659743309021\n",
            "Train loss for epoch8016: 1.9637330770492554\n",
            "Test loss for epoch8016: 2.0665600299835205\n",
            "Train loss for epoch8017: 2.149162769317627\n",
            "Test loss for epoch8017: 1.999337911605835\n",
            "Train loss for epoch8018: 1.7370768785476685\n",
            "Test loss for epoch8018: 1.8790568113327026\n",
            "Train loss for epoch8019: 1.9211457967758179\n",
            "Test loss for epoch8019: 2.1512744426727295\n",
            "Train loss for epoch8020: 1.9182679653167725\n",
            "Test loss for epoch8020: 1.8936830759048462\n",
            "Train loss for epoch8021: 1.9744155406951904\n",
            "Test loss for epoch8021: 1.9775193929672241\n",
            "Train loss for epoch8022: 1.864285945892334\n",
            "Test loss for epoch8022: 2.0467286109924316\n",
            "Train loss for epoch8023: 1.905508041381836\n",
            "Test loss for epoch8023: 1.9121038913726807\n",
            "Train loss for epoch8024: 1.9176193475723267\n",
            "Test loss for epoch8024: 2.0283708572387695\n",
            "Train loss for epoch8025: 1.9770088195800781\n",
            "Test loss for epoch8025: 2.013796091079712\n",
            "Train loss for epoch8026: 2.094057559967041\n",
            "Test loss for epoch8026: 2.376460552215576\n",
            "Train loss for epoch8027: 1.9587572813034058\n",
            "Test loss for epoch8027: 2.1475088596343994\n",
            "Train loss for epoch8028: 1.840501070022583\n",
            "Test loss for epoch8028: 2.0198402404785156\n",
            "Train loss for epoch8029: 2.0055277347564697\n",
            "Test loss for epoch8029: 1.8911820650100708\n",
            "Train loss for epoch8030: 1.8328067064285278\n",
            "Test loss for epoch8030: 2.0521762371063232\n",
            "Train loss for epoch8031: 1.8264240026474\n",
            "Test loss for epoch8031: 2.15461802482605\n",
            "Train loss for epoch8032: 1.9486136436462402\n",
            "Test loss for epoch8032: 1.9477221965789795\n",
            "Train loss for epoch8033: 1.8762418031692505\n",
            "Test loss for epoch8033: 2.1253161430358887\n",
            "Train loss for epoch8034: 1.8773579597473145\n",
            "Test loss for epoch8034: 2.0755538940429688\n",
            "Train loss for epoch8035: 2.1564419269561768\n",
            "Test loss for epoch8035: 1.9488155841827393\n",
            "Train loss for epoch8036: 1.9357309341430664\n",
            "Test loss for epoch8036: 2.208458662033081\n",
            "Train loss for epoch8037: 1.9971928596496582\n",
            "Test loss for epoch8037: 1.959153413772583\n",
            "Train loss for epoch8038: 1.9847054481506348\n",
            "Test loss for epoch8038: 2.0278117656707764\n",
            "Train loss for epoch8039: 1.802087426185608\n",
            "Test loss for epoch8039: 2.032398223876953\n",
            "Train loss for epoch8040: 2.195108652114868\n",
            "Test loss for epoch8040: 2.0912275314331055\n",
            "Train loss for epoch8041: 1.9971790313720703\n",
            "Test loss for epoch8041: 2.2306926250457764\n",
            "Train loss for epoch8042: 2.144014835357666\n",
            "Test loss for epoch8042: 2.0279860496520996\n",
            "Train loss for epoch8043: 1.7920324802398682\n",
            "Test loss for epoch8043: 1.9114036560058594\n",
            "Train loss for epoch8044: 1.88600492477417\n",
            "Test loss for epoch8044: 1.957077980041504\n",
            "Train loss for epoch8045: 1.7153629064559937\n",
            "Test loss for epoch8045: 2.038939952850342\n",
            "Train loss for epoch8046: 1.8863205909729004\n",
            "Test loss for epoch8046: 2.0533604621887207\n",
            "Train loss for epoch8047: 1.8125489950180054\n",
            "Test loss for epoch8047: 2.141232490539551\n",
            "Train loss for epoch8048: 1.690732717514038\n",
            "Test loss for epoch8048: 2.153649091720581\n",
            "Train loss for epoch8049: 1.9666006565093994\n",
            "Test loss for epoch8049: 2.0062975883483887\n",
            "Train loss for epoch8050: 1.8305532932281494\n",
            "Test loss for epoch8050: 2.118109703063965\n",
            "Train loss for epoch8051: 1.8708295822143555\n",
            "Test loss for epoch8051: 2.0757033824920654\n",
            "Train loss for epoch8052: 1.7559775114059448\n",
            "Test loss for epoch8052: 2.325441360473633\n",
            "Train loss for epoch8053: 1.9268070459365845\n",
            "Test loss for epoch8053: 1.9770814180374146\n",
            "Train loss for epoch8054: 1.9555072784423828\n",
            "Test loss for epoch8054: 2.071683645248413\n",
            "Train loss for epoch8055: 1.9294382333755493\n",
            "Test loss for epoch8055: 2.0496692657470703\n",
            "Train loss for epoch8056: 1.9153467416763306\n",
            "Test loss for epoch8056: 1.855150818824768\n",
            "Train loss for epoch8057: 1.9006280899047852\n",
            "Test loss for epoch8057: 1.9248168468475342\n",
            "Train loss for epoch8058: 2.0388095378875732\n",
            "Test loss for epoch8058: 2.132446527481079\n",
            "Train loss for epoch8059: 2.2364697456359863\n",
            "Test loss for epoch8059: 2.1540651321411133\n",
            "Train loss for epoch8060: 1.8826193809509277\n",
            "Test loss for epoch8060: 2.1602768898010254\n",
            "Train loss for epoch8061: 1.9340643882751465\n",
            "Test loss for epoch8061: 2.148108959197998\n",
            "Train loss for epoch8062: 1.8514455556869507\n",
            "Test loss for epoch8062: 2.005471706390381\n",
            "Train loss for epoch8063: 1.9625378847122192\n",
            "Test loss for epoch8063: 2.194605588912964\n",
            "Train loss for epoch8064: 1.9492226839065552\n",
            "Test loss for epoch8064: 1.9840227365493774\n",
            "Train loss for epoch8065: 2.1421241760253906\n",
            "Test loss for epoch8065: 1.9344779253005981\n",
            "Train loss for epoch8066: 1.8120447397232056\n",
            "Test loss for epoch8066: 2.1934032440185547\n",
            "Train loss for epoch8067: 1.8156883716583252\n",
            "Test loss for epoch8067: 2.284820556640625\n",
            "Train loss for epoch8068: 1.917588472366333\n",
            "Test loss for epoch8068: 2.0240914821624756\n",
            "Train loss for epoch8069: 2.0158448219299316\n",
            "Test loss for epoch8069: 2.0064196586608887\n",
            "Train loss for epoch8070: 1.944528579711914\n",
            "Test loss for epoch8070: 2.0696065425872803\n",
            "Train loss for epoch8071: 1.893569827079773\n",
            "Test loss for epoch8071: 2.132161855697632\n",
            "Train loss for epoch8072: 1.9070065021514893\n",
            "Test loss for epoch8072: 2.101670026779175\n",
            "Train loss for epoch8073: 1.9410382509231567\n",
            "Test loss for epoch8073: 2.1727912425994873\n",
            "Train loss for epoch8074: 1.8985251188278198\n",
            "Test loss for epoch8074: 2.025888442993164\n",
            "Train loss for epoch8075: 1.9887819290161133\n",
            "Test loss for epoch8075: 2.0974934101104736\n",
            "Train loss for epoch8076: 2.0880212783813477\n",
            "Test loss for epoch8076: 2.1158008575439453\n",
            "Train loss for epoch8077: 1.8500285148620605\n",
            "Test loss for epoch8077: 2.074002981185913\n",
            "Train loss for epoch8078: 2.0222692489624023\n",
            "Test loss for epoch8078: 1.8902485370635986\n",
            "Train loss for epoch8079: 2.0021982192993164\n",
            "Test loss for epoch8079: 2.1485109329223633\n",
            "Train loss for epoch8080: 1.9220519065856934\n",
            "Test loss for epoch8080: 2.1703009605407715\n",
            "Train loss for epoch8081: 1.9047150611877441\n",
            "Test loss for epoch8081: 2.0703277587890625\n",
            "Train loss for epoch8082: 2.070046901702881\n",
            "Test loss for epoch8082: 2.05285906791687\n",
            "Train loss for epoch8083: 1.8066750764846802\n",
            "Test loss for epoch8083: 2.0247864723205566\n",
            "Train loss for epoch8084: 1.989230990409851\n",
            "Test loss for epoch8084: 2.055684804916382\n",
            "Train loss for epoch8085: 2.028590679168701\n",
            "Test loss for epoch8085: 1.8793810606002808\n",
            "Train loss for epoch8086: 2.1428539752960205\n",
            "Test loss for epoch8086: 2.042489767074585\n",
            "Train loss for epoch8087: 2.0074236392974854\n",
            "Test loss for epoch8087: 2.1151325702667236\n",
            "Train loss for epoch8088: 1.9058984518051147\n",
            "Test loss for epoch8088: 2.303013801574707\n",
            "Train loss for epoch8089: 2.13081955909729\n",
            "Test loss for epoch8089: 1.8976937532424927\n",
            "Train loss for epoch8090: 1.902426838874817\n",
            "Test loss for epoch8090: 1.950374960899353\n",
            "Train loss for epoch8091: 1.9752494096755981\n",
            "Test loss for epoch8091: 2.124927282333374\n",
            "Train loss for epoch8092: 2.0610084533691406\n",
            "Test loss for epoch8092: 2.1083579063415527\n",
            "Train loss for epoch8093: 2.105133295059204\n",
            "Test loss for epoch8093: 1.863089919090271\n",
            "Train loss for epoch8094: 1.872523546218872\n",
            "Test loss for epoch8094: 1.8632311820983887\n",
            "Train loss for epoch8095: 1.961044192314148\n",
            "Test loss for epoch8095: 2.0578930377960205\n",
            "Train loss for epoch8096: 1.864249348640442\n",
            "Test loss for epoch8096: 1.8669712543487549\n",
            "Train loss for epoch8097: 1.8808708190917969\n",
            "Test loss for epoch8097: 1.9863497018814087\n",
            "Train loss for epoch8098: 2.000598907470703\n",
            "Test loss for epoch8098: 2.0090606212615967\n",
            "Train loss for epoch8099: 1.9369230270385742\n",
            "Test loss for epoch8099: 1.9070714712142944\n",
            "Train loss for epoch8100: 1.8601200580596924\n",
            "Test loss for epoch8100: 2.2886834144592285\n",
            "Train loss for epoch8101: 2.1200506687164307\n",
            "Test loss for epoch8101: 1.9345283508300781\n",
            "Train loss for epoch8102: 2.0320568084716797\n",
            "Test loss for epoch8102: 2.0614144802093506\n",
            "Train loss for epoch8103: 2.053670883178711\n",
            "Test loss for epoch8103: 1.9918023347854614\n",
            "Train loss for epoch8104: 2.011735439300537\n",
            "Test loss for epoch8104: 1.9870859384536743\n",
            "Train loss for epoch8105: 1.9860748052597046\n",
            "Test loss for epoch8105: 2.1579272747039795\n",
            "Train loss for epoch8106: 2.1855738162994385\n",
            "Test loss for epoch8106: 2.0903284549713135\n",
            "Train loss for epoch8107: 1.8990614414215088\n",
            "Test loss for epoch8107: 1.967148780822754\n",
            "Train loss for epoch8108: 2.1261515617370605\n",
            "Test loss for epoch8108: 1.7933447360992432\n",
            "Train loss for epoch8109: 2.076960563659668\n",
            "Test loss for epoch8109: 1.8150581121444702\n",
            "Train loss for epoch8110: 1.938627004623413\n",
            "Test loss for epoch8110: 1.9844214916229248\n",
            "Train loss for epoch8111: 2.2201521396636963\n",
            "Test loss for epoch8111: 2.004018783569336\n",
            "Train loss for epoch8112: 1.9516749382019043\n",
            "Test loss for epoch8112: 2.0546250343322754\n",
            "Train loss for epoch8113: 2.095745325088501\n",
            "Test loss for epoch8113: 1.972176194190979\n",
            "Train loss for epoch8114: 2.0439774990081787\n",
            "Test loss for epoch8114: 2.1738295555114746\n",
            "Train loss for epoch8115: 1.950049638748169\n",
            "Test loss for epoch8115: 2.063646078109741\n",
            "Train loss for epoch8116: 2.0782487392425537\n",
            "Test loss for epoch8116: 2.141112804412842\n",
            "Train loss for epoch8117: 1.8294950723648071\n",
            "Test loss for epoch8117: 2.135403633117676\n",
            "Train loss for epoch8118: 1.9673182964324951\n",
            "Test loss for epoch8118: 2.084183692932129\n",
            "Train loss for epoch8119: 1.9377753734588623\n",
            "Test loss for epoch8119: 2.053251266479492\n",
            "Train loss for epoch8120: 2.021390914916992\n",
            "Test loss for epoch8120: 2.0559239387512207\n",
            "Train loss for epoch8121: 2.067918062210083\n",
            "Test loss for epoch8121: 2.0712926387786865\n",
            "Train loss for epoch8122: 1.8884536027908325\n",
            "Test loss for epoch8122: 2.075981616973877\n",
            "Train loss for epoch8123: 1.8570762872695923\n",
            "Test loss for epoch8123: 2.09946608543396\n",
            "Train loss for epoch8124: 2.167496681213379\n",
            "Test loss for epoch8124: 2.302027940750122\n",
            "Train loss for epoch8125: 1.9497772455215454\n",
            "Test loss for epoch8125: 1.8953056335449219\n",
            "Train loss for epoch8126: 2.022507429122925\n",
            "Test loss for epoch8126: 2.1275646686553955\n",
            "Train loss for epoch8127: 1.9178719520568848\n",
            "Test loss for epoch8127: 1.8687713146209717\n",
            "Train loss for epoch8128: 2.037184715270996\n",
            "Test loss for epoch8128: 1.990861415863037\n",
            "Train loss for epoch8129: 1.9855011701583862\n",
            "Test loss for epoch8129: 2.01651930809021\n",
            "Train loss for epoch8130: 1.8375152349472046\n",
            "Test loss for epoch8130: 2.0411903858184814\n",
            "Train loss for epoch8131: 1.9077379703521729\n",
            "Test loss for epoch8131: 2.302661418914795\n",
            "Train loss for epoch8132: 1.984179973602295\n",
            "Test loss for epoch8132: 2.0858993530273438\n",
            "Train loss for epoch8133: 1.9426764249801636\n",
            "Test loss for epoch8133: 2.063321352005005\n",
            "Train loss for epoch8134: 2.0974087715148926\n",
            "Test loss for epoch8134: 1.8712222576141357\n",
            "Train loss for epoch8135: 1.9973294734954834\n",
            "Test loss for epoch8135: 1.8531763553619385\n",
            "Train loss for epoch8136: 2.003740072250366\n",
            "Test loss for epoch8136: 2.0980067253112793\n",
            "Train loss for epoch8137: 1.9664514064788818\n",
            "Test loss for epoch8137: 1.9649624824523926\n",
            "Train loss for epoch8138: 2.02089786529541\n",
            "Test loss for epoch8138: 2.053269624710083\n",
            "Train loss for epoch8139: 1.8574811220169067\n",
            "Test loss for epoch8139: 1.9800959825515747\n",
            "Train loss for epoch8140: 1.8959269523620605\n",
            "Test loss for epoch8140: 1.878119945526123\n",
            "Train loss for epoch8141: 1.9642287492752075\n",
            "Test loss for epoch8141: 2.169883966445923\n",
            "Train loss for epoch8142: 1.7806460857391357\n",
            "Test loss for epoch8142: 1.9797641038894653\n",
            "Train loss for epoch8143: 2.1350114345550537\n",
            "Test loss for epoch8143: 1.9074522256851196\n",
            "Train loss for epoch8144: 1.9173508882522583\n",
            "Test loss for epoch8144: 1.8687876462936401\n",
            "Train loss for epoch8145: 1.9940986633300781\n",
            "Test loss for epoch8145: 2.1914052963256836\n",
            "Train loss for epoch8146: 1.9816211462020874\n",
            "Test loss for epoch8146: 2.0921928882598877\n",
            "Train loss for epoch8147: 1.971439003944397\n",
            "Test loss for epoch8147: 2.011366367340088\n",
            "Train loss for epoch8148: 2.0120368003845215\n",
            "Test loss for epoch8148: 2.1613824367523193\n",
            "Train loss for epoch8149: 1.9646806716918945\n",
            "Test loss for epoch8149: 2.2143781185150146\n",
            "Train loss for epoch8150: 1.9832994937896729\n",
            "Test loss for epoch8150: 2.2218077182769775\n",
            "Train loss for epoch8151: 2.096663475036621\n",
            "Test loss for epoch8151: 1.892594814300537\n",
            "Train loss for epoch8152: 2.059015989303589\n",
            "Test loss for epoch8152: 2.0986762046813965\n",
            "Train loss for epoch8153: 2.0736947059631348\n",
            "Test loss for epoch8153: 2.160281181335449\n",
            "Train loss for epoch8154: 2.040173292160034\n",
            "Test loss for epoch8154: 2.1529147624969482\n",
            "Train loss for epoch8155: 1.843104362487793\n",
            "Test loss for epoch8155: 2.0759716033935547\n",
            "Train loss for epoch8156: 2.026094675064087\n",
            "Test loss for epoch8156: 2.0562124252319336\n",
            "Train loss for epoch8157: 1.9737601280212402\n",
            "Test loss for epoch8157: 2.0639827251434326\n",
            "Train loss for epoch8158: 1.9615609645843506\n",
            "Test loss for epoch8158: 2.10476016998291\n",
            "Train loss for epoch8159: 1.9660316705703735\n",
            "Test loss for epoch8159: 2.249394655227661\n",
            "Train loss for epoch8160: 1.737199068069458\n",
            "Test loss for epoch8160: 1.9528791904449463\n",
            "Train loss for epoch8161: 1.9178599119186401\n",
            "Test loss for epoch8161: 2.2383298873901367\n",
            "Train loss for epoch8162: 1.957817554473877\n",
            "Test loss for epoch8162: 2.0381879806518555\n",
            "Train loss for epoch8163: 2.0062429904937744\n",
            "Test loss for epoch8163: 1.9359673261642456\n",
            "Train loss for epoch8164: 1.943919062614441\n",
            "Test loss for epoch8164: 2.1531782150268555\n",
            "Train loss for epoch8165: 2.113257646560669\n",
            "Test loss for epoch8165: 1.974419116973877\n",
            "Train loss for epoch8166: 1.9482579231262207\n",
            "Test loss for epoch8166: 2.1324474811553955\n",
            "Train loss for epoch8167: 1.7352087497711182\n",
            "Test loss for epoch8167: 1.9310681819915771\n",
            "Train loss for epoch8168: 1.9933674335479736\n",
            "Test loss for epoch8168: 2.136380434036255\n",
            "Train loss for epoch8169: 1.8562339544296265\n",
            "Test loss for epoch8169: 2.0338878631591797\n",
            "Train loss for epoch8170: 2.078727960586548\n",
            "Test loss for epoch8170: 1.918903112411499\n",
            "Train loss for epoch8171: 1.8602113723754883\n",
            "Test loss for epoch8171: 1.955275058746338\n",
            "Train loss for epoch8172: 1.9702271223068237\n",
            "Test loss for epoch8172: 2.1200332641601562\n",
            "Train loss for epoch8173: 1.9818862676620483\n",
            "Test loss for epoch8173: 2.073366165161133\n",
            "Train loss for epoch8174: 1.8248544931411743\n",
            "Test loss for epoch8174: 2.052924871444702\n",
            "Train loss for epoch8175: 1.937568187713623\n",
            "Test loss for epoch8175: 1.989474892616272\n",
            "Train loss for epoch8176: 1.9857949018478394\n",
            "Test loss for epoch8176: 2.0904736518859863\n",
            "Train loss for epoch8177: 1.8908079862594604\n",
            "Test loss for epoch8177: 2.096015214920044\n",
            "Train loss for epoch8178: 2.0501911640167236\n",
            "Test loss for epoch8178: 2.1075551509857178\n",
            "Train loss for epoch8179: 1.889355182647705\n",
            "Test loss for epoch8179: 1.9332785606384277\n",
            "Train loss for epoch8180: 2.0126256942749023\n",
            "Test loss for epoch8180: 1.91434907913208\n",
            "Train loss for epoch8181: 2.0416207313537598\n",
            "Test loss for epoch8181: 2.2614939212799072\n",
            "Train loss for epoch8182: 1.934908390045166\n",
            "Test loss for epoch8182: 2.0252108573913574\n",
            "Train loss for epoch8183: 1.8504143953323364\n",
            "Test loss for epoch8183: 1.8932455778121948\n",
            "Train loss for epoch8184: 2.291468858718872\n",
            "Test loss for epoch8184: 1.9420892000198364\n",
            "Train loss for epoch8185: 2.059487819671631\n",
            "Test loss for epoch8185: 1.9138267040252686\n",
            "Train loss for epoch8186: 1.8976705074310303\n",
            "Test loss for epoch8186: 2.122755289077759\n",
            "Train loss for epoch8187: 1.9766789674758911\n",
            "Test loss for epoch8187: 2.1151866912841797\n",
            "Train loss for epoch8188: 2.192927360534668\n",
            "Test loss for epoch8188: 1.9977473020553589\n",
            "Train loss for epoch8189: 2.16664981842041\n",
            "Test loss for epoch8189: 2.0668983459472656\n",
            "Train loss for epoch8190: 1.8898345232009888\n",
            "Test loss for epoch8190: 2.056583881378174\n",
            "Train loss for epoch8191: 2.013122797012329\n",
            "Test loss for epoch8191: 2.159095525741577\n",
            "Train loss for epoch8192: 2.0749666690826416\n",
            "Test loss for epoch8192: 2.0179920196533203\n",
            "Train loss for epoch8193: 2.096370220184326\n",
            "Test loss for epoch8193: 1.971631407737732\n",
            "Train loss for epoch8194: 2.037752389907837\n",
            "Test loss for epoch8194: 2.184251546859741\n",
            "Train loss for epoch8195: 1.9842191934585571\n",
            "Test loss for epoch8195: 2.111475706100464\n",
            "Train loss for epoch8196: 1.9002574682235718\n",
            "Test loss for epoch8196: 2.0830562114715576\n",
            "Train loss for epoch8197: 2.0049476623535156\n",
            "Test loss for epoch8197: 2.091397762298584\n",
            "Train loss for epoch8198: 1.8091586828231812\n",
            "Test loss for epoch8198: 2.228865623474121\n",
            "Train loss for epoch8199: 1.9691816568374634\n",
            "Test loss for epoch8199: 1.8834596872329712\n",
            "Train loss for epoch8200: 2.1094536781311035\n",
            "Test loss for epoch8200: 2.0813682079315186\n",
            "Train loss for epoch8201: 1.9824597835540771\n",
            "Test loss for epoch8201: 2.1321029663085938\n",
            "Train loss for epoch8202: 1.961575984954834\n",
            "Test loss for epoch8202: 2.1664958000183105\n",
            "Train loss for epoch8203: 1.8467881679534912\n",
            "Test loss for epoch8203: 2.004152297973633\n",
            "Train loss for epoch8204: 1.9794631004333496\n",
            "Test loss for epoch8204: 1.906630039215088\n",
            "Train loss for epoch8205: 1.8605823516845703\n",
            "Test loss for epoch8205: 1.976251482963562\n",
            "Train loss for epoch8206: 1.802626132965088\n",
            "Test loss for epoch8206: 1.7710033655166626\n",
            "Train loss for epoch8207: 2.0323922634124756\n",
            "Test loss for epoch8207: 1.9781194925308228\n",
            "Train loss for epoch8208: 1.9867665767669678\n",
            "Test loss for epoch8208: 2.0619747638702393\n",
            "Train loss for epoch8209: 1.9181170463562012\n",
            "Test loss for epoch8209: 1.9616307020187378\n",
            "Train loss for epoch8210: 1.9884294271469116\n",
            "Test loss for epoch8210: 1.971602201461792\n",
            "Train loss for epoch8211: 2.094128131866455\n",
            "Test loss for epoch8211: 2.0724618434906006\n",
            "Train loss for epoch8212: 2.1041345596313477\n",
            "Test loss for epoch8212: 1.870094895362854\n",
            "Train loss for epoch8213: 1.7681865692138672\n",
            "Test loss for epoch8213: 1.9438037872314453\n",
            "Train loss for epoch8214: 1.9202781915664673\n",
            "Test loss for epoch8214: 2.1856436729431152\n",
            "Train loss for epoch8215: 1.9101855754852295\n",
            "Test loss for epoch8215: 2.130800247192383\n",
            "Train loss for epoch8216: 1.9310625791549683\n",
            "Test loss for epoch8216: 1.9361834526062012\n",
            "Train loss for epoch8217: 2.0298349857330322\n",
            "Test loss for epoch8217: 2.1379857063293457\n",
            "Train loss for epoch8218: 1.820364236831665\n",
            "Test loss for epoch8218: 1.918150782585144\n",
            "Train loss for epoch8219: 2.043778419494629\n",
            "Test loss for epoch8219: 2.01289439201355\n",
            "Train loss for epoch8220: 2.1863768100738525\n",
            "Test loss for epoch8220: 1.9294897317886353\n",
            "Train loss for epoch8221: 1.9805563688278198\n",
            "Test loss for epoch8221: 2.0924019813537598\n",
            "Train loss for epoch8222: 1.9025659561157227\n",
            "Test loss for epoch8222: 2.101447105407715\n",
            "Train loss for epoch8223: 1.8876163959503174\n",
            "Test loss for epoch8223: 2.0467329025268555\n",
            "Train loss for epoch8224: 2.05167293548584\n",
            "Test loss for epoch8224: 2.0670831203460693\n",
            "Train loss for epoch8225: 2.0719659328460693\n",
            "Test loss for epoch8225: 2.083777904510498\n",
            "Train loss for epoch8226: 1.9749053716659546\n",
            "Test loss for epoch8226: 1.7922430038452148\n",
            "Train loss for epoch8227: 2.151608467102051\n",
            "Test loss for epoch8227: 1.8165991306304932\n",
            "Train loss for epoch8228: 1.9562128782272339\n",
            "Test loss for epoch8228: 2.2609877586364746\n",
            "Train loss for epoch8229: 1.9017236232757568\n",
            "Test loss for epoch8229: 1.904181718826294\n",
            "Train loss for epoch8230: 1.926476001739502\n",
            "Test loss for epoch8230: 2.003093719482422\n",
            "Train loss for epoch8231: 1.9656546115875244\n",
            "Test loss for epoch8231: 2.106712818145752\n",
            "Train loss for epoch8232: 2.0245933532714844\n",
            "Test loss for epoch8232: 2.07485032081604\n",
            "Train loss for epoch8233: 2.087444543838501\n",
            "Test loss for epoch8233: 2.0046701431274414\n",
            "Train loss for epoch8234: 1.8519067764282227\n",
            "Test loss for epoch8234: 2.0793917179107666\n",
            "Train loss for epoch8235: 1.985545039176941\n",
            "Test loss for epoch8235: 1.974966049194336\n",
            "Train loss for epoch8236: 2.0820164680480957\n",
            "Test loss for epoch8236: 2.16616153717041\n",
            "Train loss for epoch8237: 1.7641090154647827\n",
            "Test loss for epoch8237: 2.1123790740966797\n",
            "Train loss for epoch8238: 1.7701702117919922\n",
            "Test loss for epoch8238: 2.0464060306549072\n",
            "Train loss for epoch8239: 2.120915174484253\n",
            "Test loss for epoch8239: 2.075531482696533\n",
            "Train loss for epoch8240: 1.8206778764724731\n",
            "Test loss for epoch8240: 1.986932396888733\n",
            "Train loss for epoch8241: 1.9459123611450195\n",
            "Test loss for epoch8241: 2.194352865219116\n",
            "Train loss for epoch8242: 2.0223300457000732\n",
            "Test loss for epoch8242: 1.8920865058898926\n",
            "Train loss for epoch8243: 2.01033616065979\n",
            "Test loss for epoch8243: 2.109679937362671\n",
            "Train loss for epoch8244: 2.0425541400909424\n",
            "Test loss for epoch8244: 2.1106185913085938\n",
            "Train loss for epoch8245: 1.8009841442108154\n",
            "Test loss for epoch8245: 2.0295183658599854\n",
            "Train loss for epoch8246: 1.8791548013687134\n",
            "Test loss for epoch8246: 2.234380006790161\n",
            "Train loss for epoch8247: 1.9589468240737915\n",
            "Test loss for epoch8247: 1.838484287261963\n",
            "Train loss for epoch8248: 2.1270339488983154\n",
            "Test loss for epoch8248: 2.208195447921753\n",
            "Train loss for epoch8249: 2.009458303451538\n",
            "Test loss for epoch8249: 1.8851878643035889\n",
            "Train loss for epoch8250: 1.9415209293365479\n",
            "Test loss for epoch8250: 2.25429368019104\n",
            "Train loss for epoch8251: 2.108412027359009\n",
            "Test loss for epoch8251: 1.8273998498916626\n",
            "Train loss for epoch8252: 1.9693516492843628\n",
            "Test loss for epoch8252: 1.9287221431732178\n",
            "Train loss for epoch8253: 1.8838924169540405\n",
            "Test loss for epoch8253: 1.833103895187378\n",
            "Train loss for epoch8254: 1.864943504333496\n",
            "Test loss for epoch8254: 2.1846671104431152\n",
            "Train loss for epoch8255: 1.8589470386505127\n",
            "Test loss for epoch8255: 1.8530772924423218\n",
            "Train loss for epoch8256: 1.8338744640350342\n",
            "Test loss for epoch8256: 1.970312476158142\n",
            "Train loss for epoch8257: 1.880854606628418\n",
            "Test loss for epoch8257: 2.008356809616089\n",
            "Train loss for epoch8258: 2.0723655223846436\n",
            "Test loss for epoch8258: 2.0727970600128174\n",
            "Train loss for epoch8259: 1.9935048818588257\n",
            "Test loss for epoch8259: 2.0772640705108643\n",
            "Train loss for epoch8260: 1.823134183883667\n",
            "Test loss for epoch8260: 1.8650094270706177\n",
            "Train loss for epoch8261: 1.916331171989441\n",
            "Test loss for epoch8261: 2.0604867935180664\n",
            "Train loss for epoch8262: 1.8748667240142822\n",
            "Test loss for epoch8262: 2.1651339530944824\n",
            "Train loss for epoch8263: 1.9404569864273071\n",
            "Test loss for epoch8263: 1.9906365871429443\n",
            "Train loss for epoch8264: 2.027279853820801\n",
            "Test loss for epoch8264: 2.1842825412750244\n",
            "Train loss for epoch8265: 1.7937047481536865\n",
            "Test loss for epoch8265: 1.9103988409042358\n",
            "Train loss for epoch8266: 1.8289003372192383\n",
            "Test loss for epoch8266: 1.952212929725647\n",
            "Train loss for epoch8267: 2.0230870246887207\n",
            "Test loss for epoch8267: 1.9509804248809814\n",
            "Train loss for epoch8268: 1.9290465116500854\n",
            "Test loss for epoch8268: 2.2189602851867676\n",
            "Train loss for epoch8269: 1.955241322517395\n",
            "Test loss for epoch8269: 1.9675527811050415\n",
            "Train loss for epoch8270: 1.9507149457931519\n",
            "Test loss for epoch8270: 2.0676469802856445\n",
            "Train loss for epoch8271: 2.078993082046509\n",
            "Test loss for epoch8271: 2.0383737087249756\n",
            "Train loss for epoch8272: 1.9600948095321655\n",
            "Test loss for epoch8272: 2.0814459323883057\n",
            "Train loss for epoch8273: 1.9583086967468262\n",
            "Test loss for epoch8273: 1.943792700767517\n",
            "Train loss for epoch8274: 1.878313422203064\n",
            "Test loss for epoch8274: 1.9996650218963623\n",
            "Train loss for epoch8275: 1.7550227642059326\n",
            "Test loss for epoch8275: 1.953049659729004\n",
            "Train loss for epoch8276: 1.9867794513702393\n",
            "Test loss for epoch8276: 2.129453182220459\n",
            "Train loss for epoch8277: 2.1057002544403076\n",
            "Test loss for epoch8277: 1.918522596359253\n",
            "Train loss for epoch8278: 1.8688163757324219\n",
            "Test loss for epoch8278: 2.2733466625213623\n",
            "Train loss for epoch8279: 1.9988956451416016\n",
            "Test loss for epoch8279: 2.1718244552612305\n",
            "Train loss for epoch8280: 1.9369419813156128\n",
            "Test loss for epoch8280: 2.0971767902374268\n",
            "Train loss for epoch8281: 1.889681339263916\n",
            "Test loss for epoch8281: 2.4248626232147217\n",
            "Train loss for epoch8282: 1.8881700038909912\n",
            "Test loss for epoch8282: 1.9918524026870728\n",
            "Train loss for epoch8283: 1.9180033206939697\n",
            "Test loss for epoch8283: 2.066089630126953\n",
            "Train loss for epoch8284: 2.0471930503845215\n",
            "Test loss for epoch8284: 2.082411766052246\n",
            "Train loss for epoch8285: 2.140878915786743\n",
            "Test loss for epoch8285: 2.206162214279175\n",
            "Train loss for epoch8286: 1.8023377656936646\n",
            "Test loss for epoch8286: 2.3047335147857666\n",
            "Train loss for epoch8287: 2.0906765460968018\n",
            "Test loss for epoch8287: 2.1025094985961914\n",
            "Train loss for epoch8288: 1.9802583456039429\n",
            "Test loss for epoch8288: 2.084782123565674\n",
            "Train loss for epoch8289: 2.0633764266967773\n",
            "Test loss for epoch8289: 2.1863937377929688\n",
            "Train loss for epoch8290: 2.132542133331299\n",
            "Test loss for epoch8290: 1.9343757629394531\n",
            "Train loss for epoch8291: 2.158897638320923\n",
            "Test loss for epoch8291: 2.1182548999786377\n",
            "Train loss for epoch8292: 2.0907177925109863\n",
            "Test loss for epoch8292: 2.1319124698638916\n",
            "Train loss for epoch8293: 1.6570318937301636\n",
            "Test loss for epoch8293: 1.9252359867095947\n",
            "Train loss for epoch8294: 1.9798539876937866\n",
            "Test loss for epoch8294: 2.0647592544555664\n",
            "Train loss for epoch8295: 2.033848762512207\n",
            "Test loss for epoch8295: 2.0657002925872803\n",
            "Train loss for epoch8296: 1.7340563535690308\n",
            "Test loss for epoch8296: 2.017923355102539\n",
            "Train loss for epoch8297: 1.985431432723999\n",
            "Test loss for epoch8297: 2.2640039920806885\n",
            "Train loss for epoch8298: 2.0507476329803467\n",
            "Test loss for epoch8298: 2.1181108951568604\n",
            "Train loss for epoch8299: 2.1030399799346924\n",
            "Test loss for epoch8299: 2.192631721496582\n",
            "Train loss for epoch8300: 1.9373525381088257\n",
            "Test loss for epoch8300: 2.4081079959869385\n",
            "Train loss for epoch8301: 1.973335862159729\n",
            "Test loss for epoch8301: 2.2035574913024902\n",
            "Train loss for epoch8302: 1.8407896757125854\n",
            "Test loss for epoch8302: 2.0393006801605225\n",
            "Train loss for epoch8303: 1.8352466821670532\n",
            "Test loss for epoch8303: 2.055422067642212\n",
            "Train loss for epoch8304: 1.993035912513733\n",
            "Test loss for epoch8304: 2.162946939468384\n",
            "Train loss for epoch8305: 1.998685598373413\n",
            "Test loss for epoch8305: 2.02461838722229\n",
            "Train loss for epoch8306: 1.788530945777893\n",
            "Test loss for epoch8306: 1.882141351699829\n",
            "Train loss for epoch8307: 2.0029854774475098\n",
            "Test loss for epoch8307: 1.9555673599243164\n",
            "Train loss for epoch8308: 1.9973984956741333\n",
            "Test loss for epoch8308: 2.07896089553833\n",
            "Train loss for epoch8309: 2.1282525062561035\n",
            "Test loss for epoch8309: 1.9942649602890015\n",
            "Train loss for epoch8310: 1.8666249513626099\n",
            "Test loss for epoch8310: 2.1056277751922607\n",
            "Train loss for epoch8311: 1.865079402923584\n",
            "Test loss for epoch8311: 2.165759563446045\n",
            "Train loss for epoch8312: 1.917891263961792\n",
            "Test loss for epoch8312: 2.5433731079101562\n",
            "Train loss for epoch8313: 1.9190324544906616\n",
            "Test loss for epoch8313: 2.0886619091033936\n",
            "Train loss for epoch8314: 2.167865514755249\n",
            "Test loss for epoch8314: 1.9528688192367554\n",
            "Train loss for epoch8315: 1.8639986515045166\n",
            "Test loss for epoch8315: 1.9717793464660645\n",
            "Train loss for epoch8316: 1.9299225807189941\n",
            "Test loss for epoch8316: 2.014704465866089\n",
            "Train loss for epoch8317: 2.000105619430542\n",
            "Test loss for epoch8317: 2.013273239135742\n",
            "Train loss for epoch8318: 2.0645253658294678\n",
            "Test loss for epoch8318: 2.04154109954834\n",
            "Train loss for epoch8319: 2.0530619621276855\n",
            "Test loss for epoch8319: 2.125255584716797\n",
            "Train loss for epoch8320: 1.999943494796753\n",
            "Test loss for epoch8320: 2.4168951511383057\n",
            "Train loss for epoch8321: 1.8650922775268555\n",
            "Test loss for epoch8321: 1.9977625608444214\n",
            "Train loss for epoch8322: 1.924006462097168\n",
            "Test loss for epoch8322: 1.9648486375808716\n",
            "Train loss for epoch8323: 2.06823992729187\n",
            "Test loss for epoch8323: 2.0679728984832764\n",
            "Train loss for epoch8324: 1.8701608180999756\n",
            "Test loss for epoch8324: 2.2408111095428467\n",
            "Train loss for epoch8325: 2.020329236984253\n",
            "Test loss for epoch8325: 2.1971275806427\n",
            "Train loss for epoch8326: 1.9483946561813354\n",
            "Test loss for epoch8326: 1.9774589538574219\n",
            "Train loss for epoch8327: 1.8803070783615112\n",
            "Test loss for epoch8327: 1.9151291847229004\n",
            "Train loss for epoch8328: 1.8768519163131714\n",
            "Test loss for epoch8328: 1.9386720657348633\n",
            "Train loss for epoch8329: 2.1701266765594482\n",
            "Test loss for epoch8329: 1.9719815254211426\n",
            "Train loss for epoch8330: 1.9141361713409424\n",
            "Test loss for epoch8330: 2.092740297317505\n",
            "Train loss for epoch8331: 1.929442048072815\n",
            "Test loss for epoch8331: 2.002183437347412\n",
            "Train loss for epoch8332: 2.163151502609253\n",
            "Test loss for epoch8332: 2.255781888961792\n",
            "Train loss for epoch8333: 1.8898134231567383\n",
            "Test loss for epoch8333: 2.0998427867889404\n",
            "Train loss for epoch8334: 1.8376027345657349\n",
            "Test loss for epoch8334: 1.976783275604248\n",
            "Train loss for epoch8335: 2.05496883392334\n",
            "Test loss for epoch8335: 1.8868134021759033\n",
            "Train loss for epoch8336: 1.9290754795074463\n",
            "Test loss for epoch8336: 2.1278371810913086\n",
            "Train loss for epoch8337: 2.065438747406006\n",
            "Test loss for epoch8337: 1.9601632356643677\n",
            "Train loss for epoch8338: 2.0227887630462646\n",
            "Test loss for epoch8338: 1.866494059562683\n",
            "Train loss for epoch8339: 1.892418622970581\n",
            "Test loss for epoch8339: 1.9585026502609253\n",
            "Train loss for epoch8340: 2.1896893978118896\n",
            "Test loss for epoch8340: 2.0554637908935547\n",
            "Train loss for epoch8341: 2.1085939407348633\n",
            "Test loss for epoch8341: 2.10581111907959\n",
            "Train loss for epoch8342: 1.8617967367172241\n",
            "Test loss for epoch8342: 2.063148260116577\n",
            "Train loss for epoch8343: 1.9111520051956177\n",
            "Test loss for epoch8343: 2.0154647827148438\n",
            "Train loss for epoch8344: 1.9165475368499756\n",
            "Test loss for epoch8344: 2.1730239391326904\n",
            "Train loss for epoch8345: 1.912646770477295\n",
            "Test loss for epoch8345: 2.1199262142181396\n",
            "Train loss for epoch8346: 1.9515548944473267\n",
            "Test loss for epoch8346: 2.1485440731048584\n",
            "Train loss for epoch8347: 1.9612997770309448\n",
            "Test loss for epoch8347: 2.090759754180908\n",
            "Train loss for epoch8348: 1.8971344232559204\n",
            "Test loss for epoch8348: 2.151383399963379\n",
            "Train loss for epoch8349: 1.8680152893066406\n",
            "Test loss for epoch8349: 2.026703357696533\n",
            "Train loss for epoch8350: 1.9964449405670166\n",
            "Test loss for epoch8350: 1.9874017238616943\n",
            "Train loss for epoch8351: 1.9783718585968018\n",
            "Test loss for epoch8351: 1.9939210414886475\n",
            "Train loss for epoch8352: 2.123579740524292\n",
            "Test loss for epoch8352: 2.152395486831665\n",
            "Train loss for epoch8353: 2.016876459121704\n",
            "Test loss for epoch8353: 2.1834464073181152\n",
            "Train loss for epoch8354: 1.792564034461975\n",
            "Test loss for epoch8354: 1.998788595199585\n",
            "Train loss for epoch8355: 2.142681360244751\n",
            "Test loss for epoch8355: 2.1433980464935303\n",
            "Train loss for epoch8356: 1.9642665386199951\n",
            "Test loss for epoch8356: 2.0647025108337402\n",
            "Train loss for epoch8357: 2.077430486679077\n",
            "Test loss for epoch8357: 1.9978828430175781\n",
            "Train loss for epoch8358: 1.89036226272583\n",
            "Test loss for epoch8358: 1.994829773902893\n",
            "Train loss for epoch8359: 1.982518196105957\n",
            "Test loss for epoch8359: 2.1320736408233643\n",
            "Train loss for epoch8360: 1.9490931034088135\n",
            "Test loss for epoch8360: 2.1589720249176025\n",
            "Train loss for epoch8361: 2.0375559329986572\n",
            "Test loss for epoch8361: 2.0173113346099854\n",
            "Train loss for epoch8362: 2.084867000579834\n",
            "Test loss for epoch8362: 2.129882574081421\n",
            "Train loss for epoch8363: 1.8115507364273071\n",
            "Test loss for epoch8363: 2.067844867706299\n",
            "Train loss for epoch8364: 1.9744256734848022\n",
            "Test loss for epoch8364: 2.2830421924591064\n",
            "Train loss for epoch8365: 2.031233310699463\n",
            "Test loss for epoch8365: 1.9317997694015503\n",
            "Train loss for epoch8366: 2.2149946689605713\n",
            "Test loss for epoch8366: 2.2001073360443115\n",
            "Train loss for epoch8367: 2.092869758605957\n",
            "Test loss for epoch8367: 2.028275489807129\n",
            "Train loss for epoch8368: 1.932056188583374\n",
            "Test loss for epoch8368: 2.071314573287964\n",
            "Train loss for epoch8369: 1.9482561349868774\n",
            "Test loss for epoch8369: 2.072645425796509\n",
            "Train loss for epoch8370: 2.1447153091430664\n",
            "Test loss for epoch8370: 1.7571744918823242\n",
            "Train loss for epoch8371: 1.8472495079040527\n",
            "Test loss for epoch8371: 1.9734711647033691\n",
            "Train loss for epoch8372: 1.9789034128189087\n",
            "Test loss for epoch8372: 2.06563401222229\n",
            "Train loss for epoch8373: 2.1221933364868164\n",
            "Test loss for epoch8373: 2.1817142963409424\n",
            "Train loss for epoch8374: 2.1146175861358643\n",
            "Test loss for epoch8374: 1.9809670448303223\n",
            "Train loss for epoch8375: 1.893115758895874\n",
            "Test loss for epoch8375: 1.9192508459091187\n",
            "Train loss for epoch8376: 2.0060553550720215\n",
            "Test loss for epoch8376: 2.1032912731170654\n",
            "Train loss for epoch8377: 1.9016305208206177\n",
            "Test loss for epoch8377: 1.9720890522003174\n",
            "Train loss for epoch8378: 1.9715027809143066\n",
            "Test loss for epoch8378: 1.8131877183914185\n",
            "Train loss for epoch8379: 2.147984504699707\n",
            "Test loss for epoch8379: 1.9574533700942993\n",
            "Train loss for epoch8380: 1.8094682693481445\n",
            "Test loss for epoch8380: 2.135718584060669\n",
            "Train loss for epoch8381: 2.0512259006500244\n",
            "Test loss for epoch8381: 2.1264126300811768\n",
            "Train loss for epoch8382: 2.0672333240509033\n",
            "Test loss for epoch8382: 2.003133773803711\n",
            "Train loss for epoch8383: 1.9656026363372803\n",
            "Test loss for epoch8383: 2.134730339050293\n",
            "Train loss for epoch8384: 1.7374241352081299\n",
            "Test loss for epoch8384: 2.1229732036590576\n",
            "Train loss for epoch8385: 1.6636009216308594\n",
            "Test loss for epoch8385: 1.9631162881851196\n",
            "Train loss for epoch8386: 2.013453483581543\n",
            "Test loss for epoch8386: 1.9764196872711182\n",
            "Train loss for epoch8387: 2.0904221534729004\n",
            "Test loss for epoch8387: 2.07716965675354\n",
            "Train loss for epoch8388: 1.8491618633270264\n",
            "Test loss for epoch8388: 1.9058786630630493\n",
            "Train loss for epoch8389: 2.031182050704956\n",
            "Test loss for epoch8389: 2.187026023864746\n",
            "Train loss for epoch8390: 1.8922394514083862\n",
            "Test loss for epoch8390: 1.9564741849899292\n",
            "Train loss for epoch8391: 1.9019302129745483\n",
            "Test loss for epoch8391: 1.9203522205352783\n",
            "Train loss for epoch8392: 1.8631459474563599\n",
            "Test loss for epoch8392: 2.0557329654693604\n",
            "Train loss for epoch8393: 2.0880770683288574\n",
            "Test loss for epoch8393: 1.9904049634933472\n",
            "Train loss for epoch8394: 2.068272590637207\n",
            "Test loss for epoch8394: 1.7786107063293457\n",
            "Train loss for epoch8395: 2.0350582599639893\n",
            "Test loss for epoch8395: 1.9866230487823486\n",
            "Train loss for epoch8396: 2.0179290771484375\n",
            "Test loss for epoch8396: 2.292328119277954\n",
            "Train loss for epoch8397: 2.008151054382324\n",
            "Test loss for epoch8397: 2.0928313732147217\n",
            "Train loss for epoch8398: 1.7277978658676147\n",
            "Test loss for epoch8398: 2.060737371444702\n",
            "Train loss for epoch8399: 1.9384839534759521\n",
            "Test loss for epoch8399: 2.0840253829956055\n",
            "Train loss for epoch8400: 2.0164997577667236\n",
            "Test loss for epoch8400: 2.169126033782959\n",
            "Train loss for epoch8401: 2.0100934505462646\n",
            "Test loss for epoch8401: 1.9947023391723633\n",
            "Train loss for epoch8402: 2.11260986328125\n",
            "Test loss for epoch8402: 1.8834046125411987\n",
            "Train loss for epoch8403: 2.023066520690918\n",
            "Test loss for epoch8403: 1.9523777961730957\n",
            "Train loss for epoch8404: 1.8847789764404297\n",
            "Test loss for epoch8404: 2.115832805633545\n",
            "Train loss for epoch8405: 2.173146963119507\n",
            "Test loss for epoch8405: 2.1303420066833496\n",
            "Train loss for epoch8406: 2.0193893909454346\n",
            "Test loss for epoch8406: 2.112175941467285\n",
            "Train loss for epoch8407: 1.9239044189453125\n",
            "Test loss for epoch8407: 2.2460529804229736\n",
            "Train loss for epoch8408: 2.029611587524414\n",
            "Test loss for epoch8408: 2.161388397216797\n",
            "Train loss for epoch8409: 1.7119375467300415\n",
            "Test loss for epoch8409: 2.0362112522125244\n",
            "Train loss for epoch8410: 1.9081743955612183\n",
            "Test loss for epoch8410: 2.0589261054992676\n",
            "Train loss for epoch8411: 1.9005110263824463\n",
            "Test loss for epoch8411: 2.0686514377593994\n",
            "Train loss for epoch8412: 1.8587136268615723\n",
            "Test loss for epoch8412: 2.0061051845550537\n",
            "Train loss for epoch8413: 2.0273215770721436\n",
            "Test loss for epoch8413: 2.1350862979888916\n",
            "Train loss for epoch8414: 1.879794716835022\n",
            "Test loss for epoch8414: 2.083935022354126\n",
            "Train loss for epoch8415: 1.982001781463623\n",
            "Test loss for epoch8415: 2.187443971633911\n",
            "Train loss for epoch8416: 2.003019332885742\n",
            "Test loss for epoch8416: 2.0106935501098633\n",
            "Train loss for epoch8417: 1.991132140159607\n",
            "Test loss for epoch8417: 1.993112564086914\n",
            "Train loss for epoch8418: 2.081235885620117\n",
            "Test loss for epoch8418: 1.9003781080245972\n",
            "Train loss for epoch8419: 2.137277126312256\n",
            "Test loss for epoch8419: 1.97117280960083\n",
            "Train loss for epoch8420: 2.109057664871216\n",
            "Test loss for epoch8420: 1.9434126615524292\n",
            "Train loss for epoch8421: 2.0857465267181396\n",
            "Test loss for epoch8421: 2.156264305114746\n",
            "Train loss for epoch8422: 2.2169716358184814\n",
            "Test loss for epoch8422: 2.132185935974121\n",
            "Train loss for epoch8423: 2.024174690246582\n",
            "Test loss for epoch8423: 1.919243335723877\n",
            "Train loss for epoch8424: 2.017775058746338\n",
            "Test loss for epoch8424: 2.0071492195129395\n",
            "Train loss for epoch8425: 2.069855213165283\n",
            "Test loss for epoch8425: 2.1130592823028564\n",
            "Train loss for epoch8426: 1.7810803651809692\n",
            "Test loss for epoch8426: 2.071309804916382\n",
            "Train loss for epoch8427: 1.7821677923202515\n",
            "Test loss for epoch8427: 2.168029308319092\n",
            "Train loss for epoch8428: 1.8028877973556519\n",
            "Test loss for epoch8428: 2.0643134117126465\n",
            "Train loss for epoch8429: 1.9556833505630493\n",
            "Test loss for epoch8429: 2.1791980266571045\n",
            "Train loss for epoch8430: 1.9382503032684326\n",
            "Test loss for epoch8430: 1.8676551580429077\n",
            "Train loss for epoch8431: 2.0043556690216064\n",
            "Test loss for epoch8431: 2.0195159912109375\n",
            "Train loss for epoch8432: 1.9828965663909912\n",
            "Test loss for epoch8432: 2.2237308025360107\n",
            "Train loss for epoch8433: 2.2237651348114014\n",
            "Test loss for epoch8433: 2.043487787246704\n",
            "Train loss for epoch8434: 1.6673046350479126\n",
            "Test loss for epoch8434: 1.94486665725708\n",
            "Train loss for epoch8435: 1.7791749238967896\n",
            "Test loss for epoch8435: 2.092346429824829\n",
            "Train loss for epoch8436: 1.96128249168396\n",
            "Test loss for epoch8436: 2.408704996109009\n",
            "Train loss for epoch8437: 1.8124520778656006\n",
            "Test loss for epoch8437: 1.962155818939209\n",
            "Train loss for epoch8438: 1.916351318359375\n",
            "Test loss for epoch8438: 1.9761403799057007\n",
            "Train loss for epoch8439: 2.1414220333099365\n",
            "Test loss for epoch8439: 2.15519118309021\n",
            "Train loss for epoch8440: 2.0963447093963623\n",
            "Test loss for epoch8440: 2.166435956954956\n",
            "Train loss for epoch8441: 2.2190704345703125\n",
            "Test loss for epoch8441: 2.142735242843628\n",
            "Train loss for epoch8442: 1.9363114833831787\n",
            "Test loss for epoch8442: 1.9828413724899292\n",
            "Train loss for epoch8443: 1.990997076034546\n",
            "Test loss for epoch8443: 2.0162816047668457\n",
            "Train loss for epoch8444: 2.099897623062134\n",
            "Test loss for epoch8444: 2.1859216690063477\n",
            "Train loss for epoch8445: 1.8367451429367065\n",
            "Test loss for epoch8445: 1.9039169549942017\n",
            "Train loss for epoch8446: 2.2368156909942627\n",
            "Test loss for epoch8446: 2.036066770553589\n",
            "Train loss for epoch8447: 1.8617960214614868\n",
            "Test loss for epoch8447: 2.1358280181884766\n",
            "Train loss for epoch8448: 2.214254856109619\n",
            "Test loss for epoch8448: 2.106717586517334\n",
            "Train loss for epoch8449: 1.9174782037734985\n",
            "Test loss for epoch8449: 1.8791934251785278\n",
            "Train loss for epoch8450: 1.9054539203643799\n",
            "Test loss for epoch8450: 2.1057682037353516\n",
            "Train loss for epoch8451: 1.9709877967834473\n",
            "Test loss for epoch8451: 1.9423233270645142\n",
            "Train loss for epoch8452: 1.9042963981628418\n",
            "Test loss for epoch8452: 1.8786934614181519\n",
            "Train loss for epoch8453: 1.9028944969177246\n",
            "Test loss for epoch8453: 2.2139692306518555\n",
            "Train loss for epoch8454: 1.7383815050125122\n",
            "Test loss for epoch8454: 2.1724166870117188\n",
            "Train loss for epoch8455: 1.9784103631973267\n",
            "Test loss for epoch8455: 1.9264960289001465\n",
            "Train loss for epoch8456: 1.8908255100250244\n",
            "Test loss for epoch8456: 2.0034101009368896\n",
            "Train loss for epoch8457: 1.9392839670181274\n",
            "Test loss for epoch8457: 1.908366084098816\n",
            "Train loss for epoch8458: 2.0384325981140137\n",
            "Test loss for epoch8458: 2.1027414798736572\n",
            "Train loss for epoch8459: 2.0449378490448\n",
            "Test loss for epoch8459: 1.8337676525115967\n",
            "Train loss for epoch8460: 1.9352906942367554\n",
            "Test loss for epoch8460: 1.9329473972320557\n",
            "Train loss for epoch8461: 2.1352529525756836\n",
            "Test loss for epoch8461: 2.0780880451202393\n",
            "Train loss for epoch8462: 1.8979127407073975\n",
            "Test loss for epoch8462: 1.7171878814697266\n",
            "Train loss for epoch8463: 2.0361907482147217\n",
            "Test loss for epoch8463: 1.9034860134124756\n",
            "Train loss for epoch8464: 1.8405394554138184\n",
            "Test loss for epoch8464: 2.1965010166168213\n",
            "Train loss for epoch8465: 2.088118553161621\n",
            "Test loss for epoch8465: 2.095148801803589\n",
            "Train loss for epoch8466: 2.088895082473755\n",
            "Test loss for epoch8466: 2.1638293266296387\n",
            "Train loss for epoch8467: 1.9078902006149292\n",
            "Test loss for epoch8467: 2.1256442070007324\n",
            "Train loss for epoch8468: 1.9580026865005493\n",
            "Test loss for epoch8468: 2.1177573204040527\n",
            "Train loss for epoch8469: 1.8683339357376099\n",
            "Test loss for epoch8469: 2.1596434116363525\n",
            "Train loss for epoch8470: 1.9284427165985107\n",
            "Test loss for epoch8470: 2.106569528579712\n",
            "Train loss for epoch8471: 2.240818977355957\n",
            "Test loss for epoch8471: 2.2115840911865234\n",
            "Train loss for epoch8472: 1.81415855884552\n",
            "Test loss for epoch8472: 1.9885345697402954\n",
            "Train loss for epoch8473: 1.8711596727371216\n",
            "Test loss for epoch8473: 2.0241100788116455\n",
            "Train loss for epoch8474: 2.067889451980591\n",
            "Test loss for epoch8474: 2.0226995944976807\n",
            "Train loss for epoch8475: 2.0287768840789795\n",
            "Test loss for epoch8475: 1.8864485025405884\n",
            "Train loss for epoch8476: 2.1616861820220947\n",
            "Test loss for epoch8476: 2.249051570892334\n",
            "Train loss for epoch8477: 1.8009167909622192\n",
            "Test loss for epoch8477: 2.008920669555664\n",
            "Train loss for epoch8478: 1.8421343564987183\n",
            "Test loss for epoch8478: 1.9212547540664673\n",
            "Train loss for epoch8479: 1.8566081523895264\n",
            "Test loss for epoch8479: 1.9878978729248047\n",
            "Train loss for epoch8480: 1.8900816440582275\n",
            "Test loss for epoch8480: 2.1822807788848877\n",
            "Train loss for epoch8481: 1.8171948194503784\n",
            "Test loss for epoch8481: 2.2308080196380615\n",
            "Train loss for epoch8482: 1.944804072380066\n",
            "Test loss for epoch8482: 2.101820468902588\n",
            "Train loss for epoch8483: 2.06518292427063\n",
            "Test loss for epoch8483: 2.1240389347076416\n",
            "Train loss for epoch8484: 1.866950273513794\n",
            "Test loss for epoch8484: 2.124635934829712\n",
            "Train loss for epoch8485: 2.130722761154175\n",
            "Test loss for epoch8485: 2.1447949409484863\n",
            "Train loss for epoch8486: 1.8577417135238647\n",
            "Test loss for epoch8486: 1.8890694379806519\n",
            "Train loss for epoch8487: 2.24835205078125\n",
            "Test loss for epoch8487: 1.9806932210922241\n",
            "Train loss for epoch8488: 1.943873405456543\n",
            "Test loss for epoch8488: 1.9435334205627441\n",
            "Train loss for epoch8489: 2.1822307109832764\n",
            "Test loss for epoch8489: 1.9707411527633667\n",
            "Train loss for epoch8490: 1.9462569952011108\n",
            "Test loss for epoch8490: 2.0917954444885254\n",
            "Train loss for epoch8491: 1.9046281576156616\n",
            "Test loss for epoch8491: 2.1619458198547363\n",
            "Train loss for epoch8492: 1.9476404190063477\n",
            "Test loss for epoch8492: 1.9177480936050415\n",
            "Train loss for epoch8493: 2.005422592163086\n",
            "Test loss for epoch8493: 1.9819350242614746\n",
            "Train loss for epoch8494: 1.8306357860565186\n",
            "Test loss for epoch8494: 2.076707124710083\n",
            "Train loss for epoch8495: 1.7616355419158936\n",
            "Test loss for epoch8495: 1.9835184812545776\n",
            "Train loss for epoch8496: 2.011800765991211\n",
            "Test loss for epoch8496: 2.1091842651367188\n",
            "Train loss for epoch8497: 1.9289944171905518\n",
            "Test loss for epoch8497: 2.189812183380127\n",
            "Train loss for epoch8498: 1.8386222124099731\n",
            "Test loss for epoch8498: 2.282898426055908\n",
            "Train loss for epoch8499: 1.9427498579025269\n",
            "Test loss for epoch8499: 2.0775935649871826\n",
            "Train loss for epoch8500: 2.1103451251983643\n",
            "Test loss for epoch8500: 2.2141177654266357\n",
            "Train loss for epoch8501: 1.9518312215805054\n",
            "Test loss for epoch8501: 2.013091802597046\n",
            "Train loss for epoch8502: 2.087859630584717\n",
            "Test loss for epoch8502: 1.9845259189605713\n",
            "Train loss for epoch8503: 2.018791675567627\n",
            "Test loss for epoch8503: 2.035046339035034\n",
            "Train loss for epoch8504: 2.0609776973724365\n",
            "Test loss for epoch8504: 2.048865556716919\n",
            "Train loss for epoch8505: 1.9887089729309082\n",
            "Test loss for epoch8505: 2.090196132659912\n",
            "Train loss for epoch8506: 2.1090028285980225\n",
            "Test loss for epoch8506: 1.8731859922409058\n",
            "Train loss for epoch8507: 1.9864503145217896\n",
            "Test loss for epoch8507: 1.9674845933914185\n",
            "Train loss for epoch8508: 1.810123324394226\n",
            "Test loss for epoch8508: 2.11674165725708\n",
            "Train loss for epoch8509: 1.9490070343017578\n",
            "Test loss for epoch8509: 1.951460838317871\n",
            "Train loss for epoch8510: 2.0273654460906982\n",
            "Test loss for epoch8510: 2.076914072036743\n",
            "Train loss for epoch8511: 1.8626232147216797\n",
            "Test loss for epoch8511: 2.0960705280303955\n",
            "Train loss for epoch8512: 2.245694398880005\n",
            "Test loss for epoch8512: 2.1010568141937256\n",
            "Train loss for epoch8513: 2.003873348236084\n",
            "Test loss for epoch8513: 2.0228381156921387\n",
            "Train loss for epoch8514: 2.1213512420654297\n",
            "Test loss for epoch8514: 2.1296651363372803\n",
            "Train loss for epoch8515: 1.9198119640350342\n",
            "Test loss for epoch8515: 1.9310235977172852\n",
            "Train loss for epoch8516: 1.9239819049835205\n",
            "Test loss for epoch8516: 2.0611417293548584\n",
            "Train loss for epoch8517: 2.0530200004577637\n",
            "Test loss for epoch8517: 2.1733651161193848\n",
            "Train loss for epoch8518: 1.7914544343948364\n",
            "Test loss for epoch8518: 2.2071101665496826\n",
            "Train loss for epoch8519: 1.9616451263427734\n",
            "Test loss for epoch8519: 2.024247646331787\n",
            "Train loss for epoch8520: 1.8476402759552002\n",
            "Test loss for epoch8520: 1.9076476097106934\n",
            "Train loss for epoch8521: 1.891046166419983\n",
            "Test loss for epoch8521: 1.879166841506958\n",
            "Train loss for epoch8522: 1.999000906944275\n",
            "Test loss for epoch8522: 2.050767660140991\n",
            "Train loss for epoch8523: 1.9921276569366455\n",
            "Test loss for epoch8523: 1.9788434505462646\n",
            "Train loss for epoch8524: 1.998579740524292\n",
            "Test loss for epoch8524: 1.884501576423645\n",
            "Train loss for epoch8525: 2.0639138221740723\n",
            "Test loss for epoch8525: 1.9006083011627197\n",
            "Train loss for epoch8526: 2.086282253265381\n",
            "Test loss for epoch8526: 1.9836097955703735\n",
            "Train loss for epoch8527: 2.0377395153045654\n",
            "Test loss for epoch8527: 2.1145756244659424\n",
            "Train loss for epoch8528: 2.018786907196045\n",
            "Test loss for epoch8528: 2.0949816703796387\n",
            "Train loss for epoch8529: 1.895180344581604\n",
            "Test loss for epoch8529: 2.115694522857666\n",
            "Train loss for epoch8530: 1.9315783977508545\n",
            "Test loss for epoch8530: 1.929675579071045\n",
            "Train loss for epoch8531: 1.9130223989486694\n",
            "Test loss for epoch8531: 2.0578904151916504\n",
            "Train loss for epoch8532: 1.7935514450073242\n",
            "Test loss for epoch8532: 2.0500717163085938\n",
            "Train loss for epoch8533: 1.8511936664581299\n",
            "Test loss for epoch8533: 2.000803232192993\n",
            "Train loss for epoch8534: 1.8953241109848022\n",
            "Test loss for epoch8534: 2.0485317707061768\n",
            "Train loss for epoch8535: 1.8818669319152832\n",
            "Test loss for epoch8535: 1.939153790473938\n",
            "Train loss for epoch8536: 1.9547159671783447\n",
            "Test loss for epoch8536: 1.9671686887741089\n",
            "Train loss for epoch8537: 1.967411994934082\n",
            "Test loss for epoch8537: 1.948171615600586\n",
            "Train loss for epoch8538: 2.052584409713745\n",
            "Test loss for epoch8538: 2.075296640396118\n",
            "Train loss for epoch8539: 1.8756978511810303\n",
            "Test loss for epoch8539: 1.9766955375671387\n",
            "Train loss for epoch8540: 1.9773437976837158\n",
            "Test loss for epoch8540: 1.8050187826156616\n",
            "Train loss for epoch8541: 2.196929454803467\n",
            "Test loss for epoch8541: 1.932422399520874\n",
            "Train loss for epoch8542: 2.1477396488189697\n",
            "Test loss for epoch8542: 1.941516637802124\n",
            "Train loss for epoch8543: 1.856787085533142\n",
            "Test loss for epoch8543: 1.9424409866333008\n",
            "Train loss for epoch8544: 2.078150987625122\n",
            "Test loss for epoch8544: 2.1128036975860596\n",
            "Train loss for epoch8545: 1.9845106601715088\n",
            "Test loss for epoch8545: 2.1940364837646484\n",
            "Train loss for epoch8546: 2.083540916442871\n",
            "Test loss for epoch8546: 1.9891616106033325\n",
            "Train loss for epoch8547: 2.067960500717163\n",
            "Test loss for epoch8547: 2.030170440673828\n",
            "Train loss for epoch8548: 2.0388875007629395\n",
            "Test loss for epoch8548: 1.945344090461731\n",
            "Train loss for epoch8549: 1.9780911207199097\n",
            "Test loss for epoch8549: 2.0822439193725586\n",
            "Train loss for epoch8550: 1.916603922843933\n",
            "Test loss for epoch8550: 1.9538540840148926\n",
            "Train loss for epoch8551: 1.9298427104949951\n",
            "Test loss for epoch8551: 1.9901307821273804\n",
            "Train loss for epoch8552: 1.7989994287490845\n",
            "Test loss for epoch8552: 2.029604196548462\n",
            "Train loss for epoch8553: 2.0348241329193115\n",
            "Test loss for epoch8553: 1.978480339050293\n",
            "Train loss for epoch8554: 2.017702579498291\n",
            "Test loss for epoch8554: 2.3535943031311035\n",
            "Train loss for epoch8555: 1.9949443340301514\n",
            "Test loss for epoch8555: 2.1183531284332275\n",
            "Train loss for epoch8556: 2.0523312091827393\n",
            "Test loss for epoch8556: 2.074970245361328\n",
            "Train loss for epoch8557: 1.988451600074768\n",
            "Test loss for epoch8557: 2.131197690963745\n",
            "Train loss for epoch8558: 1.9095805883407593\n",
            "Test loss for epoch8558: 2.02744460105896\n",
            "Train loss for epoch8559: 2.1088476181030273\n",
            "Test loss for epoch8559: 2.0926144123077393\n",
            "Train loss for epoch8560: 2.006244421005249\n",
            "Test loss for epoch8560: 2.1575441360473633\n",
            "Train loss for epoch8561: 2.0153005123138428\n",
            "Test loss for epoch8561: 2.225338935852051\n",
            "Train loss for epoch8562: 1.953171730041504\n",
            "Test loss for epoch8562: 1.8864712715148926\n",
            "Train loss for epoch8563: 1.9114341735839844\n",
            "Test loss for epoch8563: 2.2683181762695312\n",
            "Train loss for epoch8564: 2.06026554107666\n",
            "Test loss for epoch8564: 1.9939186573028564\n",
            "Train loss for epoch8565: 2.114732503890991\n",
            "Test loss for epoch8565: 2.125347852706909\n",
            "Train loss for epoch8566: 1.9126331806182861\n",
            "Test loss for epoch8566: 2.0251758098602295\n",
            "Train loss for epoch8567: 1.9629677534103394\n",
            "Test loss for epoch8567: 2.1905322074890137\n",
            "Train loss for epoch8568: 2.0103161334991455\n",
            "Test loss for epoch8568: 2.008061170578003\n",
            "Train loss for epoch8569: 2.070070266723633\n",
            "Test loss for epoch8569: 1.9961941242218018\n",
            "Train loss for epoch8570: 1.9177781343460083\n",
            "Test loss for epoch8570: 1.950263500213623\n",
            "Train loss for epoch8571: 2.0161774158477783\n",
            "Test loss for epoch8571: 2.123013734817505\n",
            "Train loss for epoch8572: 1.9650896787643433\n",
            "Test loss for epoch8572: 2.1313371658325195\n",
            "Train loss for epoch8573: 2.006545066833496\n",
            "Test loss for epoch8573: 2.078857898712158\n",
            "Train loss for epoch8574: 1.7750102281570435\n",
            "Test loss for epoch8574: 2.029238224029541\n",
            "Train loss for epoch8575: 1.857609510421753\n",
            "Test loss for epoch8575: 2.1586644649505615\n",
            "Train loss for epoch8576: 1.9934523105621338\n",
            "Test loss for epoch8576: 1.8984144926071167\n",
            "Train loss for epoch8577: 1.9024866819381714\n",
            "Test loss for epoch8577: 1.9612253904342651\n",
            "Train loss for epoch8578: 1.8733855485916138\n",
            "Test loss for epoch8578: 2.1004457473754883\n",
            "Train loss for epoch8579: 1.9411641359329224\n",
            "Test loss for epoch8579: 2.0967931747436523\n",
            "Train loss for epoch8580: 1.8997185230255127\n",
            "Test loss for epoch8580: 2.0048813819885254\n",
            "Train loss for epoch8581: 1.9816049337387085\n",
            "Test loss for epoch8581: 2.109375\n",
            "Train loss for epoch8582: 2.013050079345703\n",
            "Test loss for epoch8582: 1.9190834760665894\n",
            "Train loss for epoch8583: 1.959348201751709\n",
            "Test loss for epoch8583: 2.166135549545288\n",
            "Train loss for epoch8584: 1.8921115398406982\n",
            "Test loss for epoch8584: 2.0076911449432373\n",
            "Train loss for epoch8585: 2.132371187210083\n",
            "Test loss for epoch8585: 2.2407240867614746\n",
            "Train loss for epoch8586: 1.9700042009353638\n",
            "Test loss for epoch8586: 2.133312225341797\n",
            "Train loss for epoch8587: 2.046842098236084\n",
            "Test loss for epoch8587: 1.9718832969665527\n",
            "Train loss for epoch8588: 2.1109676361083984\n",
            "Test loss for epoch8588: 1.8912907838821411\n",
            "Train loss for epoch8589: 1.848205327987671\n",
            "Test loss for epoch8589: 1.9137948751449585\n",
            "Train loss for epoch8590: 2.16432523727417\n",
            "Test loss for epoch8590: 2.122283935546875\n",
            "Train loss for epoch8591: 1.9900392293930054\n",
            "Test loss for epoch8591: 2.0511443614959717\n",
            "Train loss for epoch8592: 2.1078274250030518\n",
            "Test loss for epoch8592: 2.0025668144226074\n",
            "Train loss for epoch8593: 1.9764350652694702\n",
            "Test loss for epoch8593: 2.1452674865722656\n",
            "Train loss for epoch8594: 1.9933545589447021\n",
            "Test loss for epoch8594: 1.9718222618103027\n",
            "Train loss for epoch8595: 1.8249253034591675\n",
            "Test loss for epoch8595: 1.8229801654815674\n",
            "Train loss for epoch8596: 1.8973346948623657\n",
            "Test loss for epoch8596: 2.1642792224884033\n",
            "Train loss for epoch8597: 1.9035615921020508\n",
            "Test loss for epoch8597: 1.9397387504577637\n",
            "Train loss for epoch8598: 2.060330867767334\n",
            "Test loss for epoch8598: 1.9521734714508057\n",
            "Train loss for epoch8599: 1.9493567943572998\n",
            "Test loss for epoch8599: 2.0389511585235596\n",
            "Train loss for epoch8600: 1.96083402633667\n",
            "Test loss for epoch8600: 2.0121519565582275\n",
            "Train loss for epoch8601: 1.9403780698776245\n",
            "Test loss for epoch8601: 1.9844868183135986\n",
            "Train loss for epoch8602: 1.860385537147522\n",
            "Test loss for epoch8602: 2.040517568588257\n",
            "Train loss for epoch8603: 2.2322700023651123\n",
            "Test loss for epoch8603: 1.9754996299743652\n",
            "Train loss for epoch8604: 2.121513605117798\n",
            "Test loss for epoch8604: 2.15226411819458\n",
            "Train loss for epoch8605: 1.8889179229736328\n",
            "Test loss for epoch8605: 2.077679395675659\n",
            "Train loss for epoch8606: 1.9897457361221313\n",
            "Test loss for epoch8606: 2.105546474456787\n",
            "Train loss for epoch8607: 2.010561466217041\n",
            "Test loss for epoch8607: 1.8856395483016968\n",
            "Train loss for epoch8608: 1.8903470039367676\n",
            "Test loss for epoch8608: 2.047893524169922\n",
            "Train loss for epoch8609: 2.060004711151123\n",
            "Test loss for epoch8609: 2.0844991207122803\n",
            "Train loss for epoch8610: 1.9132953882217407\n",
            "Test loss for epoch8610: 2.198807954788208\n",
            "Train loss for epoch8611: 1.983915090560913\n",
            "Test loss for epoch8611: 2.0576391220092773\n",
            "Train loss for epoch8612: 2.09228515625\n",
            "Test loss for epoch8612: 1.9749605655670166\n",
            "Train loss for epoch8613: 2.0478644371032715\n",
            "Test loss for epoch8613: 2.0522866249084473\n",
            "Train loss for epoch8614: 2.002866744995117\n",
            "Test loss for epoch8614: 1.862410306930542\n",
            "Train loss for epoch8615: 1.9968904256820679\n",
            "Test loss for epoch8615: 2.040475845336914\n",
            "Train loss for epoch8616: 1.7510141134262085\n",
            "Test loss for epoch8616: 2.1882643699645996\n",
            "Train loss for epoch8617: 1.9239884614944458\n",
            "Test loss for epoch8617: 1.9428937435150146\n",
            "Train loss for epoch8618: 2.0055882930755615\n",
            "Test loss for epoch8618: 2.108248233795166\n",
            "Train loss for epoch8619: 1.752036690711975\n",
            "Test loss for epoch8619: 2.057408332824707\n",
            "Train loss for epoch8620: 2.0183866024017334\n",
            "Test loss for epoch8620: 1.974708080291748\n",
            "Train loss for epoch8621: 2.0117435455322266\n",
            "Test loss for epoch8621: 2.441679000854492\n",
            "Train loss for epoch8622: 1.978629231452942\n",
            "Test loss for epoch8622: 1.9654874801635742\n",
            "Train loss for epoch8623: 1.8032456636428833\n",
            "Test loss for epoch8623: 2.0709125995635986\n",
            "Train loss for epoch8624: 2.167224884033203\n",
            "Test loss for epoch8624: 2.0623369216918945\n",
            "Train loss for epoch8625: 1.8576523065567017\n",
            "Test loss for epoch8625: 2.062385320663452\n",
            "Train loss for epoch8626: 1.8399217128753662\n",
            "Test loss for epoch8626: 2.0071284770965576\n",
            "Train loss for epoch8627: 1.9787027835845947\n",
            "Test loss for epoch8627: 2.013096809387207\n",
            "Train loss for epoch8628: 2.010911703109741\n",
            "Test loss for epoch8628: 2.1007070541381836\n",
            "Train loss for epoch8629: 1.7204769849777222\n",
            "Test loss for epoch8629: 2.075986862182617\n",
            "Train loss for epoch8630: 1.9733953475952148\n",
            "Test loss for epoch8630: 2.078705072402954\n",
            "Train loss for epoch8631: 2.0658512115478516\n",
            "Test loss for epoch8631: 2.1768527030944824\n",
            "Train loss for epoch8632: 1.8096858263015747\n",
            "Test loss for epoch8632: 2.1484503746032715\n",
            "Train loss for epoch8633: 1.8664593696594238\n",
            "Test loss for epoch8633: 2.0223639011383057\n",
            "Train loss for epoch8634: 1.832249641418457\n",
            "Test loss for epoch8634: 2.1960606575012207\n",
            "Train loss for epoch8635: 1.8399876356124878\n",
            "Test loss for epoch8635: 2.253865957260132\n",
            "Train loss for epoch8636: 1.9138855934143066\n",
            "Test loss for epoch8636: 2.1531803607940674\n",
            "Train loss for epoch8637: 1.9861804246902466\n",
            "Test loss for epoch8637: 1.9010454416275024\n",
            "Train loss for epoch8638: 2.075637102127075\n",
            "Test loss for epoch8638: 1.9655566215515137\n",
            "Train loss for epoch8639: 1.723525047302246\n",
            "Test loss for epoch8639: 1.8240461349487305\n",
            "Train loss for epoch8640: 1.8596864938735962\n",
            "Test loss for epoch8640: 1.8537116050720215\n",
            "Train loss for epoch8641: 1.9053471088409424\n",
            "Test loss for epoch8641: 2.147716999053955\n",
            "Train loss for epoch8642: 2.0845344066619873\n",
            "Test loss for epoch8642: 2.1784884929656982\n",
            "Train loss for epoch8643: 1.9613970518112183\n",
            "Test loss for epoch8643: 2.1287240982055664\n",
            "Train loss for epoch8644: 1.9527268409729004\n",
            "Test loss for epoch8644: 2.0819644927978516\n",
            "Train loss for epoch8645: 1.8265669345855713\n",
            "Test loss for epoch8645: 2.0102648735046387\n",
            "Train loss for epoch8646: 2.11850905418396\n",
            "Test loss for epoch8646: 2.0067434310913086\n",
            "Train loss for epoch8647: 1.9005507230758667\n",
            "Test loss for epoch8647: 2.200636863708496\n",
            "Train loss for epoch8648: 1.8511139154434204\n",
            "Test loss for epoch8648: 1.9861211776733398\n",
            "Train loss for epoch8649: 1.9743622541427612\n",
            "Test loss for epoch8649: 2.108776569366455\n",
            "Train loss for epoch8650: 1.9673852920532227\n",
            "Test loss for epoch8650: 2.294396162033081\n",
            "Train loss for epoch8651: 1.9163765907287598\n",
            "Test loss for epoch8651: 2.119621992111206\n",
            "Train loss for epoch8652: 1.7993851900100708\n",
            "Test loss for epoch8652: 1.8845818042755127\n",
            "Train loss for epoch8653: 2.2776129245758057\n",
            "Test loss for epoch8653: 2.0895683765411377\n",
            "Train loss for epoch8654: 1.9540318250656128\n",
            "Test loss for epoch8654: 2.1539318561553955\n",
            "Train loss for epoch8655: 2.016993522644043\n",
            "Test loss for epoch8655: 1.9743590354919434\n",
            "Train loss for epoch8656: 1.9767014980316162\n",
            "Test loss for epoch8656: 1.9555401802062988\n",
            "Train loss for epoch8657: 1.827196478843689\n",
            "Test loss for epoch8657: 2.078052282333374\n",
            "Train loss for epoch8658: 1.9421157836914062\n",
            "Test loss for epoch8658: 2.1369335651397705\n",
            "Train loss for epoch8659: 2.0605862140655518\n",
            "Test loss for epoch8659: 2.123908758163452\n",
            "Train loss for epoch8660: 1.8994756937026978\n",
            "Test loss for epoch8660: 2.2374267578125\n",
            "Train loss for epoch8661: 1.7517526149749756\n",
            "Test loss for epoch8661: 2.0750808715820312\n",
            "Train loss for epoch8662: 1.8340600728988647\n",
            "Test loss for epoch8662: 2.0228891372680664\n",
            "Train loss for epoch8663: 1.997398018836975\n",
            "Test loss for epoch8663: 2.0901262760162354\n",
            "Train loss for epoch8664: 1.880988597869873\n",
            "Test loss for epoch8664: 1.9829957485198975\n",
            "Train loss for epoch8665: 1.8932185173034668\n",
            "Test loss for epoch8665: 2.175499439239502\n",
            "Train loss for epoch8666: 2.064166307449341\n",
            "Test loss for epoch8666: 2.1908555030822754\n",
            "Train loss for epoch8667: 1.9817012548446655\n",
            "Test loss for epoch8667: 1.8665862083435059\n",
            "Train loss for epoch8668: 1.9695837497711182\n",
            "Test loss for epoch8668: 1.980837345123291\n",
            "Train loss for epoch8669: 1.8717557191848755\n",
            "Test loss for epoch8669: 1.9242076873779297\n",
            "Train loss for epoch8670: 1.8760827779769897\n",
            "Test loss for epoch8670: 2.0101325511932373\n",
            "Train loss for epoch8671: 2.1850931644439697\n",
            "Test loss for epoch8671: 2.1038622856140137\n",
            "Train loss for epoch8672: 2.1202073097229004\n",
            "Test loss for epoch8672: 2.0481982231140137\n",
            "Train loss for epoch8673: 2.0288805961608887\n",
            "Test loss for epoch8673: 1.9680744409561157\n",
            "Train loss for epoch8674: 2.0126633644104004\n",
            "Test loss for epoch8674: 2.068467140197754\n",
            "Train loss for epoch8675: 1.8305422067642212\n",
            "Test loss for epoch8675: 2.2044379711151123\n",
            "Train loss for epoch8676: 2.041154146194458\n",
            "Test loss for epoch8676: 1.9873255491256714\n",
            "Train loss for epoch8677: 1.8512637615203857\n",
            "Test loss for epoch8677: 2.0277318954467773\n",
            "Train loss for epoch8678: 2.0094141960144043\n",
            "Test loss for epoch8678: 1.8591744899749756\n",
            "Train loss for epoch8679: 1.995764136314392\n",
            "Test loss for epoch8679: 1.9924346208572388\n",
            "Train loss for epoch8680: 1.8781615495681763\n",
            "Test loss for epoch8680: 2.043351888656616\n",
            "Train loss for epoch8681: 1.8705850839614868\n",
            "Test loss for epoch8681: 1.9597671031951904\n",
            "Train loss for epoch8682: 2.021854877471924\n",
            "Test loss for epoch8682: 2.302569627761841\n",
            "Train loss for epoch8683: 1.9488399028778076\n",
            "Test loss for epoch8683: 2.0265917778015137\n",
            "Train loss for epoch8684: 1.9772285223007202\n",
            "Test loss for epoch8684: 2.0392444133758545\n",
            "Train loss for epoch8685: 2.0379626750946045\n",
            "Test loss for epoch8685: 2.246734142303467\n",
            "Train loss for epoch8686: 1.8851728439331055\n",
            "Test loss for epoch8686: 2.069838285446167\n",
            "Train loss for epoch8687: 2.1272242069244385\n",
            "Test loss for epoch8687: 2.1018784046173096\n",
            "Train loss for epoch8688: 2.1748127937316895\n",
            "Test loss for epoch8688: 1.9988594055175781\n",
            "Train loss for epoch8689: 2.0261473655700684\n",
            "Test loss for epoch8689: 2.117766857147217\n",
            "Train loss for epoch8690: 1.8826937675476074\n",
            "Test loss for epoch8690: 1.9228171110153198\n",
            "Train loss for epoch8691: 2.060004472732544\n",
            "Test loss for epoch8691: 1.9922348260879517\n",
            "Train loss for epoch8692: 2.078789472579956\n",
            "Test loss for epoch8692: 2.124612808227539\n",
            "Train loss for epoch8693: 1.9519716501235962\n",
            "Test loss for epoch8693: 2.106447458267212\n",
            "Train loss for epoch8694: 1.8492032289505005\n",
            "Test loss for epoch8694: 2.0703349113464355\n",
            "Train loss for epoch8695: 1.8009320497512817\n",
            "Test loss for epoch8695: 2.141157627105713\n",
            "Train loss for epoch8696: 2.2081844806671143\n",
            "Test loss for epoch8696: 2.000328779220581\n",
            "Train loss for epoch8697: 1.877634882926941\n",
            "Test loss for epoch8697: 1.9565373659133911\n",
            "Train loss for epoch8698: 1.8078316450119019\n",
            "Test loss for epoch8698: 1.9084614515304565\n",
            "Train loss for epoch8699: 1.9863964319229126\n",
            "Test loss for epoch8699: 2.2001395225524902\n",
            "Train loss for epoch8700: 2.003054141998291\n",
            "Test loss for epoch8700: 2.14837646484375\n",
            "Train loss for epoch8701: 2.1527628898620605\n",
            "Test loss for epoch8701: 1.9730374813079834\n",
            "Train loss for epoch8702: 1.993867039680481\n",
            "Test loss for epoch8702: 1.8924857378005981\n",
            "Train loss for epoch8703: 1.7902354001998901\n",
            "Test loss for epoch8703: 1.7957028150558472\n",
            "Train loss for epoch8704: 1.8849340677261353\n",
            "Test loss for epoch8704: 1.773594617843628\n",
            "Train loss for epoch8705: 1.8537049293518066\n",
            "Test loss for epoch8705: 2.0089163780212402\n",
            "Train loss for epoch8706: 1.860028862953186\n",
            "Test loss for epoch8706: 2.137131929397583\n",
            "Train loss for epoch8707: 2.026580333709717\n",
            "Test loss for epoch8707: 2.1392948627471924\n",
            "Train loss for epoch8708: 2.0770435333251953\n",
            "Test loss for epoch8708: 1.9124945402145386\n",
            "Train loss for epoch8709: 2.038846015930176\n",
            "Test loss for epoch8709: 2.031186103820801\n",
            "Train loss for epoch8710: 2.0795350074768066\n",
            "Test loss for epoch8710: 2.1921708583831787\n",
            "Train loss for epoch8711: 2.165369749069214\n",
            "Test loss for epoch8711: 2.077381134033203\n",
            "Train loss for epoch8712: 2.08766770362854\n",
            "Test loss for epoch8712: 1.9709155559539795\n",
            "Train loss for epoch8713: 1.9730724096298218\n",
            "Test loss for epoch8713: 2.1369261741638184\n",
            "Train loss for epoch8714: 1.9304876327514648\n",
            "Test loss for epoch8714: 1.9627330303192139\n",
            "Train loss for epoch8715: 1.9823015928268433\n",
            "Test loss for epoch8715: 1.9939978122711182\n",
            "Train loss for epoch8716: 1.9693878889083862\n",
            "Test loss for epoch8716: 2.1498775482177734\n",
            "Train loss for epoch8717: 2.0584611892700195\n",
            "Test loss for epoch8717: 2.180020332336426\n",
            "Train loss for epoch8718: 2.169318675994873\n",
            "Test loss for epoch8718: 2.0395689010620117\n",
            "Train loss for epoch8719: 2.0981481075286865\n",
            "Test loss for epoch8719: 2.0985991954803467\n",
            "Train loss for epoch8720: 1.957161784172058\n",
            "Test loss for epoch8720: 2.0629501342773438\n",
            "Train loss for epoch8721: 1.8725875616073608\n",
            "Test loss for epoch8721: 1.990003228187561\n",
            "Train loss for epoch8722: 1.9412401914596558\n",
            "Test loss for epoch8722: 1.957746982574463\n",
            "Train loss for epoch8723: 2.089617967605591\n",
            "Test loss for epoch8723: 2.16924786567688\n",
            "Train loss for epoch8724: 2.092372179031372\n",
            "Test loss for epoch8724: 2.1099934577941895\n",
            "Train loss for epoch8725: 1.8207242488861084\n",
            "Test loss for epoch8725: 1.9236279726028442\n",
            "Train loss for epoch8726: 1.8550266027450562\n",
            "Test loss for epoch8726: 2.1545019149780273\n",
            "Train loss for epoch8727: 2.0475144386291504\n",
            "Test loss for epoch8727: 1.9834855794906616\n",
            "Train loss for epoch8728: 1.9291609525680542\n",
            "Test loss for epoch8728: 1.996289610862732\n",
            "Train loss for epoch8729: 1.9386215209960938\n",
            "Test loss for epoch8729: 2.2291791439056396\n",
            "Train loss for epoch8730: 2.0975723266601562\n",
            "Test loss for epoch8730: 2.0469448566436768\n",
            "Train loss for epoch8731: 1.7695715427398682\n",
            "Test loss for epoch8731: 2.057422637939453\n",
            "Train loss for epoch8732: 1.9836030006408691\n",
            "Test loss for epoch8732: 2.2325029373168945\n",
            "Train loss for epoch8733: 2.0869596004486084\n",
            "Test loss for epoch8733: 1.921897053718567\n",
            "Train loss for epoch8734: 2.002554416656494\n",
            "Test loss for epoch8734: 2.1397693157196045\n",
            "Train loss for epoch8735: 1.7809340953826904\n",
            "Test loss for epoch8735: 1.9554777145385742\n",
            "Train loss for epoch8736: 1.8615607023239136\n",
            "Test loss for epoch8736: 2.206075429916382\n",
            "Train loss for epoch8737: 1.8985711336135864\n",
            "Test loss for epoch8737: 2.185861587524414\n",
            "Train loss for epoch8738: 1.7050703763961792\n",
            "Test loss for epoch8738: 1.9984592199325562\n",
            "Train loss for epoch8739: 2.0699048042297363\n",
            "Test loss for epoch8739: 2.0517449378967285\n",
            "Train loss for epoch8740: 1.942793846130371\n",
            "Test loss for epoch8740: 2.231484889984131\n",
            "Train loss for epoch8741: 1.845473289489746\n",
            "Test loss for epoch8741: 2.1215450763702393\n",
            "Train loss for epoch8742: 1.9403210878372192\n",
            "Test loss for epoch8742: 2.0099399089813232\n",
            "Train loss for epoch8743: 2.1528592109680176\n",
            "Test loss for epoch8743: 2.1739354133605957\n",
            "Train loss for epoch8744: 2.025829792022705\n",
            "Test loss for epoch8744: 1.8055373430252075\n",
            "Train loss for epoch8745: 2.0051512718200684\n",
            "Test loss for epoch8745: 1.9901950359344482\n",
            "Train loss for epoch8746: 2.005385398864746\n",
            "Test loss for epoch8746: 1.9803194999694824\n",
            "Train loss for epoch8747: 2.0100340843200684\n",
            "Test loss for epoch8747: 2.0690746307373047\n",
            "Train loss for epoch8748: 2.0792605876922607\n",
            "Test loss for epoch8748: 1.902430534362793\n",
            "Train loss for epoch8749: 2.0133683681488037\n",
            "Test loss for epoch8749: 2.264193296432495\n",
            "Train loss for epoch8750: 1.8246358633041382\n",
            "Test loss for epoch8750: 1.873781681060791\n",
            "Train loss for epoch8751: 1.9408084154129028\n",
            "Test loss for epoch8751: 1.860607385635376\n",
            "Train loss for epoch8752: 1.9560885429382324\n",
            "Test loss for epoch8752: 1.9670403003692627\n",
            "Train loss for epoch8753: 1.9078105688095093\n",
            "Test loss for epoch8753: 2.0962095260620117\n",
            "Train loss for epoch8754: 1.9168353080749512\n",
            "Test loss for epoch8754: 2.1427931785583496\n",
            "Train loss for epoch8755: 2.029010772705078\n",
            "Test loss for epoch8755: 2.1690521240234375\n",
            "Train loss for epoch8756: 2.1205081939697266\n",
            "Test loss for epoch8756: 2.2750282287597656\n",
            "Train loss for epoch8757: 1.814745545387268\n",
            "Test loss for epoch8757: 1.8200032711029053\n",
            "Train loss for epoch8758: 1.8883694410324097\n",
            "Test loss for epoch8758: 2.1286072731018066\n",
            "Train loss for epoch8759: 2.199570417404175\n",
            "Test loss for epoch8759: 2.040443181991577\n",
            "Train loss for epoch8760: 1.823843240737915\n",
            "Test loss for epoch8760: 2.2847349643707275\n",
            "Train loss for epoch8761: 1.9398268461227417\n",
            "Test loss for epoch8761: 1.859139084815979\n",
            "Train loss for epoch8762: 2.0864710807800293\n",
            "Test loss for epoch8762: 2.0536396503448486\n",
            "Train loss for epoch8763: 2.0904457569122314\n",
            "Test loss for epoch8763: 2.187734365463257\n",
            "Train loss for epoch8764: 2.080116033554077\n",
            "Test loss for epoch8764: 2.1973299980163574\n",
            "Train loss for epoch8765: 2.026886224746704\n",
            "Test loss for epoch8765: 1.9427378177642822\n",
            "Train loss for epoch8766: 2.0638763904571533\n",
            "Test loss for epoch8766: 1.9975124597549438\n",
            "Train loss for epoch8767: 2.092155933380127\n",
            "Test loss for epoch8767: 2.009237766265869\n",
            "Train loss for epoch8768: 1.8135312795639038\n",
            "Test loss for epoch8768: 2.1514551639556885\n",
            "Train loss for epoch8769: 1.8674434423446655\n",
            "Test loss for epoch8769: 2.031008005142212\n",
            "Train loss for epoch8770: 1.9267733097076416\n",
            "Test loss for epoch8770: 1.946433186531067\n",
            "Train loss for epoch8771: 2.1741092205047607\n",
            "Test loss for epoch8771: 2.1811869144439697\n",
            "Train loss for epoch8772: 2.028993606567383\n",
            "Test loss for epoch8772: 1.7948811054229736\n",
            "Train loss for epoch8773: 1.8358738422393799\n",
            "Test loss for epoch8773: 2.3063230514526367\n",
            "Train loss for epoch8774: 2.047034978866577\n",
            "Test loss for epoch8774: 2.101377487182617\n",
            "Train loss for epoch8775: 1.8314956426620483\n",
            "Test loss for epoch8775: 2.120382308959961\n",
            "Train loss for epoch8776: 2.01259183883667\n",
            "Test loss for epoch8776: 2.0567803382873535\n",
            "Train loss for epoch8777: 1.938117504119873\n",
            "Test loss for epoch8777: 1.9572737216949463\n",
            "Train loss for epoch8778: 2.0699987411499023\n",
            "Test loss for epoch8778: 1.965517520904541\n",
            "Train loss for epoch8779: 2.0122058391571045\n",
            "Test loss for epoch8779: 2.1019904613494873\n",
            "Train loss for epoch8780: 1.949273705482483\n",
            "Test loss for epoch8780: 2.048288583755493\n",
            "Train loss for epoch8781: 1.9835401773452759\n",
            "Test loss for epoch8781: 1.8210657835006714\n",
            "Train loss for epoch8782: 2.047865390777588\n",
            "Test loss for epoch8782: 1.9887197017669678\n",
            "Train loss for epoch8783: 2.00073504447937\n",
            "Test loss for epoch8783: 1.759708285331726\n",
            "Train loss for epoch8784: 1.911422610282898\n",
            "Test loss for epoch8784: 2.039422035217285\n",
            "Train loss for epoch8785: 2.0072968006134033\n",
            "Test loss for epoch8785: 2.1510252952575684\n",
            "Train loss for epoch8786: 2.037252902984619\n",
            "Test loss for epoch8786: 2.2834722995758057\n",
            "Train loss for epoch8787: 1.9697442054748535\n",
            "Test loss for epoch8787: 2.006071090698242\n",
            "Train loss for epoch8788: 1.7585608959197998\n",
            "Test loss for epoch8788: 2.005769729614258\n",
            "Train loss for epoch8789: 1.9674367904663086\n",
            "Test loss for epoch8789: 1.9115922451019287\n",
            "Train loss for epoch8790: 2.045058250427246\n",
            "Test loss for epoch8790: 1.9086084365844727\n",
            "Train loss for epoch8791: 1.869476318359375\n",
            "Test loss for epoch8791: 2.023179531097412\n",
            "Train loss for epoch8792: 1.9936015605926514\n",
            "Test loss for epoch8792: 2.0156805515289307\n",
            "Train loss for epoch8793: 1.9411016702651978\n",
            "Test loss for epoch8793: 2.071587324142456\n",
            "Train loss for epoch8794: 2.1501998901367188\n",
            "Test loss for epoch8794: 2.0202107429504395\n",
            "Train loss for epoch8795: 1.85966956615448\n",
            "Test loss for epoch8795: 1.9949651956558228\n",
            "Train loss for epoch8796: 1.937013864517212\n",
            "Test loss for epoch8796: 1.859886884689331\n",
            "Train loss for epoch8797: 1.826897382736206\n",
            "Test loss for epoch8797: 2.0742990970611572\n",
            "Train loss for epoch8798: 1.8946709632873535\n",
            "Test loss for epoch8798: 2.065795660018921\n",
            "Train loss for epoch8799: 1.9260292053222656\n",
            "Test loss for epoch8799: 2.071791887283325\n",
            "Train loss for epoch8800: 1.9791085720062256\n",
            "Test loss for epoch8800: 2.073629379272461\n",
            "Train loss for epoch8801: 1.8549307584762573\n",
            "Test loss for epoch8801: 1.8743984699249268\n",
            "Train loss for epoch8802: 2.052563428878784\n",
            "Test loss for epoch8802: 1.8799943923950195\n",
            "Train loss for epoch8803: 1.9985243082046509\n",
            "Test loss for epoch8803: 2.033048152923584\n",
            "Train loss for epoch8804: 2.105559825897217\n",
            "Test loss for epoch8804: 2.0589990615844727\n",
            "Train loss for epoch8805: 1.960607647895813\n",
            "Test loss for epoch8805: 2.052340269088745\n",
            "Train loss for epoch8806: 1.7807717323303223\n",
            "Test loss for epoch8806: 1.8822791576385498\n",
            "Train loss for epoch8807: 2.050755262374878\n",
            "Test loss for epoch8807: 1.9778889417648315\n",
            "Train loss for epoch8808: 1.9254344701766968\n",
            "Test loss for epoch8808: 2.049225091934204\n",
            "Train loss for epoch8809: 1.9070639610290527\n",
            "Test loss for epoch8809: 1.9643319845199585\n",
            "Train loss for epoch8810: 2.0322089195251465\n",
            "Test loss for epoch8810: 2.041417360305786\n",
            "Train loss for epoch8811: 1.8865464925765991\n",
            "Test loss for epoch8811: 2.032996654510498\n",
            "Train loss for epoch8812: 1.8770461082458496\n",
            "Test loss for epoch8812: 2.105947971343994\n",
            "Train loss for epoch8813: 1.7204986810684204\n",
            "Test loss for epoch8813: 1.9978699684143066\n",
            "Train loss for epoch8814: 1.895764946937561\n",
            "Test loss for epoch8814: 2.092740774154663\n",
            "Train loss for epoch8815: 1.942102313041687\n",
            "Test loss for epoch8815: 2.0161356925964355\n",
            "Train loss for epoch8816: 1.9375933408737183\n",
            "Test loss for epoch8816: 2.1809887886047363\n",
            "Train loss for epoch8817: 2.0873286724090576\n",
            "Test loss for epoch8817: 2.0104546546936035\n",
            "Train loss for epoch8818: 1.9467118978500366\n",
            "Test loss for epoch8818: 2.230931043624878\n",
            "Train loss for epoch8819: 2.1103174686431885\n",
            "Test loss for epoch8819: 1.9654489755630493\n",
            "Train loss for epoch8820: 2.0748372077941895\n",
            "Test loss for epoch8820: 2.056366443634033\n",
            "Train loss for epoch8821: 1.9527881145477295\n",
            "Test loss for epoch8821: 1.967308759689331\n",
            "Train loss for epoch8822: 1.717577576637268\n",
            "Test loss for epoch8822: 2.242217779159546\n",
            "Train loss for epoch8823: 2.0898828506469727\n",
            "Test loss for epoch8823: 1.9936789274215698\n",
            "Train loss for epoch8824: 2.0627949237823486\n",
            "Test loss for epoch8824: 2.2005813121795654\n",
            "Train loss for epoch8825: 1.9339938163757324\n",
            "Test loss for epoch8825: 2.0181803703308105\n",
            "Train loss for epoch8826: 1.9858978986740112\n",
            "Test loss for epoch8826: 1.9219930171966553\n",
            "Train loss for epoch8827: 1.9260776042938232\n",
            "Test loss for epoch8827: 1.9795724153518677\n",
            "Train loss for epoch8828: 2.142186403274536\n",
            "Test loss for epoch8828: 1.869523286819458\n",
            "Train loss for epoch8829: 1.9760823249816895\n",
            "Test loss for epoch8829: 1.9423130750656128\n",
            "Train loss for epoch8830: 1.9649471044540405\n",
            "Test loss for epoch8830: 2.2219040393829346\n",
            "Train loss for epoch8831: 2.1903300285339355\n",
            "Test loss for epoch8831: 2.1184608936309814\n",
            "Train loss for epoch8832: 1.964320182800293\n",
            "Test loss for epoch8832: 1.8803448677062988\n",
            "Train loss for epoch8833: 1.8750742673873901\n",
            "Test loss for epoch8833: 2.3848230838775635\n",
            "Train loss for epoch8834: 1.965444564819336\n",
            "Test loss for epoch8834: 2.1120545864105225\n",
            "Train loss for epoch8835: 1.8314162492752075\n",
            "Test loss for epoch8835: 2.269984722137451\n",
            "Train loss for epoch8836: 1.9460867643356323\n",
            "Test loss for epoch8836: 2.123248815536499\n",
            "Train loss for epoch8837: 2.098328113555908\n",
            "Test loss for epoch8837: 1.9836997985839844\n",
            "Train loss for epoch8838: 1.941270112991333\n",
            "Test loss for epoch8838: 1.974357008934021\n",
            "Train loss for epoch8839: 2.0845441818237305\n",
            "Test loss for epoch8839: 2.0995640754699707\n",
            "Train loss for epoch8840: 1.9062451124191284\n",
            "Test loss for epoch8840: 2.159693956375122\n",
            "Train loss for epoch8841: 2.1559743881225586\n",
            "Test loss for epoch8841: 1.9394795894622803\n",
            "Train loss for epoch8842: 2.06349515914917\n",
            "Test loss for epoch8842: 2.095609664916992\n",
            "Train loss for epoch8843: 1.9795777797698975\n",
            "Test loss for epoch8843: 2.133110761642456\n",
            "Train loss for epoch8844: 2.014737129211426\n",
            "Test loss for epoch8844: 1.9268040657043457\n",
            "Train loss for epoch8845: 1.8894025087356567\n",
            "Test loss for epoch8845: 1.9379528760910034\n",
            "Train loss for epoch8846: 1.8227726221084595\n",
            "Test loss for epoch8846: 1.8946524858474731\n",
            "Train loss for epoch8847: 1.9026612043380737\n",
            "Test loss for epoch8847: 2.049921751022339\n",
            "Train loss for epoch8848: 1.92063570022583\n",
            "Test loss for epoch8848: 2.0441668033599854\n",
            "Train loss for epoch8849: 1.7945051193237305\n",
            "Test loss for epoch8849: 2.0052034854888916\n",
            "Train loss for epoch8850: 2.0946834087371826\n",
            "Test loss for epoch8850: 2.039565324783325\n",
            "Train loss for epoch8851: 2.1121158599853516\n",
            "Test loss for epoch8851: 1.9290965795516968\n",
            "Train loss for epoch8852: 1.9510945081710815\n",
            "Test loss for epoch8852: 2.1244616508483887\n",
            "Train loss for epoch8853: 2.01556134223938\n",
            "Test loss for epoch8853: 2.2902026176452637\n",
            "Train loss for epoch8854: 2.030712842941284\n",
            "Test loss for epoch8854: 1.9433109760284424\n",
            "Train loss for epoch8855: 2.084188222885132\n",
            "Test loss for epoch8855: 2.165013313293457\n",
            "Train loss for epoch8856: 1.9792464971542358\n",
            "Test loss for epoch8856: 2.048090934753418\n",
            "Train loss for epoch8857: 2.1481010913848877\n",
            "Test loss for epoch8857: 1.8539674282073975\n",
            "Train loss for epoch8858: 1.9951984882354736\n",
            "Test loss for epoch8858: 1.8752176761627197\n",
            "Train loss for epoch8859: 2.1146352291107178\n",
            "Test loss for epoch8859: 1.9821979999542236\n",
            "Train loss for epoch8860: 1.960340976715088\n",
            "Test loss for epoch8860: 1.8539702892303467\n",
            "Train loss for epoch8861: 2.119157314300537\n",
            "Test loss for epoch8861: 2.101011276245117\n",
            "Train loss for epoch8862: 2.1246120929718018\n",
            "Test loss for epoch8862: 2.1118249893188477\n",
            "Train loss for epoch8863: 1.9730074405670166\n",
            "Test loss for epoch8863: 1.819379210472107\n",
            "Train loss for epoch8864: 1.785889983177185\n",
            "Test loss for epoch8864: 2.0234694480895996\n",
            "Train loss for epoch8865: 1.7219666242599487\n",
            "Test loss for epoch8865: 1.9056447744369507\n",
            "Train loss for epoch8866: 1.8681857585906982\n",
            "Test loss for epoch8866: 2.0134778022766113\n",
            "Train loss for epoch8867: 2.0162436962127686\n",
            "Test loss for epoch8867: 2.022205352783203\n",
            "Train loss for epoch8868: 2.055009365081787\n",
            "Test loss for epoch8868: 2.080437660217285\n",
            "Train loss for epoch8869: 1.8146322965621948\n",
            "Test loss for epoch8869: 2.1957221031188965\n",
            "Train loss for epoch8870: 1.8677380084991455\n",
            "Test loss for epoch8870: 2.055110216140747\n",
            "Train loss for epoch8871: 1.9059675931930542\n",
            "Test loss for epoch8871: 2.0407168865203857\n",
            "Train loss for epoch8872: 2.099390983581543\n",
            "Test loss for epoch8872: 2.0245187282562256\n",
            "Train loss for epoch8873: 1.9660793542861938\n",
            "Test loss for epoch8873: 1.9899024963378906\n",
            "Train loss for epoch8874: 1.8869426250457764\n",
            "Test loss for epoch8874: 2.0893020629882812\n",
            "Train loss for epoch8875: 1.8755526542663574\n",
            "Test loss for epoch8875: 2.1824755668640137\n",
            "Train loss for epoch8876: 2.016369581222534\n",
            "Test loss for epoch8876: 2.0032846927642822\n",
            "Train loss for epoch8877: 1.8263381719589233\n",
            "Test loss for epoch8877: 2.045477867126465\n",
            "Train loss for epoch8878: 1.8937126398086548\n",
            "Test loss for epoch8878: 1.889574646949768\n",
            "Train loss for epoch8879: 1.963081955909729\n",
            "Test loss for epoch8879: 2.016988515853882\n",
            "Train loss for epoch8880: 1.7018275260925293\n",
            "Test loss for epoch8880: 2.0681638717651367\n",
            "Train loss for epoch8881: 1.8799020051956177\n",
            "Test loss for epoch8881: 2.0825512409210205\n",
            "Train loss for epoch8882: 1.9523983001708984\n",
            "Test loss for epoch8882: 2.263974189758301\n",
            "Train loss for epoch8883: 1.9660242795944214\n",
            "Test loss for epoch8883: 2.138162136077881\n",
            "Train loss for epoch8884: 1.94271981716156\n",
            "Test loss for epoch8884: 1.8431191444396973\n",
            "Train loss for epoch8885: 1.9217575788497925\n",
            "Test loss for epoch8885: 2.2929391860961914\n",
            "Train loss for epoch8886: 2.1166014671325684\n",
            "Test loss for epoch8886: 2.0893056392669678\n",
            "Train loss for epoch8887: 1.8654825687408447\n",
            "Test loss for epoch8887: 2.038792371749878\n",
            "Train loss for epoch8888: 1.762686014175415\n",
            "Test loss for epoch8888: 2.083850860595703\n",
            "Train loss for epoch8889: 2.1343331336975098\n",
            "Test loss for epoch8889: 1.9656115770339966\n",
            "Train loss for epoch8890: 1.72023344039917\n",
            "Test loss for epoch8890: 2.2153570652008057\n",
            "Train loss for epoch8891: 1.8120324611663818\n",
            "Test loss for epoch8891: 1.9302741289138794\n",
            "Train loss for epoch8892: 1.8399642705917358\n",
            "Test loss for epoch8892: 1.8418151140213013\n",
            "Train loss for epoch8893: 2.108905792236328\n",
            "Test loss for epoch8893: 1.9870007038116455\n",
            "Train loss for epoch8894: 1.9404693841934204\n",
            "Test loss for epoch8894: 2.119537830352783\n",
            "Train loss for epoch8895: 1.9834500551223755\n",
            "Test loss for epoch8895: 1.9850314855575562\n",
            "Train loss for epoch8896: 1.9279375076293945\n",
            "Test loss for epoch8896: 2.1261401176452637\n",
            "Train loss for epoch8897: 1.8869218826293945\n",
            "Test loss for epoch8897: 1.974663496017456\n",
            "Train loss for epoch8898: 1.8613286018371582\n",
            "Test loss for epoch8898: 2.146574020385742\n",
            "Train loss for epoch8899: 1.9962210655212402\n",
            "Test loss for epoch8899: 2.213677167892456\n",
            "Train loss for epoch8900: 1.8649251461029053\n",
            "Test loss for epoch8900: 2.217820167541504\n",
            "Train loss for epoch8901: 1.930112361907959\n",
            "Test loss for epoch8901: 1.9089055061340332\n",
            "Train loss for epoch8902: 1.8987046480178833\n",
            "Test loss for epoch8902: 2.0870375633239746\n",
            "Train loss for epoch8903: 1.8900563716888428\n",
            "Test loss for epoch8903: 1.7847294807434082\n",
            "Train loss for epoch8904: 1.73294198513031\n",
            "Test loss for epoch8904: 1.9169234037399292\n",
            "Train loss for epoch8905: 1.9107869863510132\n",
            "Test loss for epoch8905: 2.0904572010040283\n",
            "Train loss for epoch8906: 1.9944050312042236\n",
            "Test loss for epoch8906: 2.170438766479492\n",
            "Train loss for epoch8907: 1.6106088161468506\n",
            "Test loss for epoch8907: 2.014498233795166\n",
            "Train loss for epoch8908: 1.9194467067718506\n",
            "Test loss for epoch8908: 1.833733320236206\n",
            "Train loss for epoch8909: 1.8610895872116089\n",
            "Test loss for epoch8909: 2.049739360809326\n",
            "Train loss for epoch8910: 1.946951150894165\n",
            "Test loss for epoch8910: 2.0942375659942627\n",
            "Train loss for epoch8911: 1.832265019416809\n",
            "Test loss for epoch8911: 1.9672993421554565\n",
            "Train loss for epoch8912: 2.132148504257202\n",
            "Test loss for epoch8912: 2.015117645263672\n",
            "Train loss for epoch8913: 1.7948875427246094\n",
            "Test loss for epoch8913: 1.9089221954345703\n",
            "Train loss for epoch8914: 1.8520580530166626\n",
            "Test loss for epoch8914: 2.126765251159668\n",
            "Train loss for epoch8915: 1.9544652700424194\n",
            "Test loss for epoch8915: 2.0961811542510986\n",
            "Train loss for epoch8916: 1.8873313665390015\n",
            "Test loss for epoch8916: 2.2105724811553955\n",
            "Train loss for epoch8917: 2.1069045066833496\n",
            "Test loss for epoch8917: 1.9655944108963013\n",
            "Train loss for epoch8918: 2.0364909172058105\n",
            "Test loss for epoch8918: 1.9984924793243408\n",
            "Train loss for epoch8919: 1.9025856256484985\n",
            "Test loss for epoch8919: 2.1642343997955322\n",
            "Train loss for epoch8920: 2.0085208415985107\n",
            "Test loss for epoch8920: 1.9116137027740479\n",
            "Train loss for epoch8921: 1.9699530601501465\n",
            "Test loss for epoch8921: 2.0198686122894287\n",
            "Train loss for epoch8922: 1.98378324508667\n",
            "Test loss for epoch8922: 2.0721700191497803\n",
            "Train loss for epoch8923: 1.9184212684631348\n",
            "Test loss for epoch8923: 2.120145320892334\n",
            "Train loss for epoch8924: 1.9297951459884644\n",
            "Test loss for epoch8924: 2.1281464099884033\n",
            "Train loss for epoch8925: 1.9348602294921875\n",
            "Test loss for epoch8925: 1.9794021844863892\n",
            "Train loss for epoch8926: 1.90879487991333\n",
            "Test loss for epoch8926: 2.0676515102386475\n",
            "Train loss for epoch8927: 1.7993396520614624\n",
            "Test loss for epoch8927: 1.8892309665679932\n",
            "Train loss for epoch8928: 1.788903832435608\n",
            "Test loss for epoch8928: 2.1398658752441406\n",
            "Train loss for epoch8929: 1.95126211643219\n",
            "Test loss for epoch8929: 2.25361967086792\n",
            "Train loss for epoch8930: 1.9771363735198975\n",
            "Test loss for epoch8930: 1.919741153717041\n",
            "Train loss for epoch8931: 2.1189939975738525\n",
            "Test loss for epoch8931: 1.9227890968322754\n",
            "Train loss for epoch8932: 1.651323676109314\n",
            "Test loss for epoch8932: 2.1163275241851807\n",
            "Train loss for epoch8933: 1.778747797012329\n",
            "Test loss for epoch8933: 2.0841197967529297\n",
            "Train loss for epoch8934: 1.920242190361023\n",
            "Test loss for epoch8934: 1.9678514003753662\n",
            "Train loss for epoch8935: 1.8951280117034912\n",
            "Test loss for epoch8935: 2.1833713054656982\n",
            "Train loss for epoch8936: 1.943977952003479\n",
            "Test loss for epoch8936: 1.932541012763977\n",
            "Train loss for epoch8937: 1.9615870714187622\n",
            "Test loss for epoch8937: 1.8802542686462402\n",
            "Train loss for epoch8938: 1.7076841592788696\n",
            "Test loss for epoch8938: 2.051891326904297\n",
            "Train loss for epoch8939: 2.047791004180908\n",
            "Test loss for epoch8939: 1.963219404220581\n",
            "Train loss for epoch8940: 1.8831135034561157\n",
            "Test loss for epoch8940: 2.0374128818511963\n",
            "Train loss for epoch8941: 1.7521791458129883\n",
            "Test loss for epoch8941: 2.0869412422180176\n",
            "Train loss for epoch8942: 1.8569775819778442\n",
            "Test loss for epoch8942: 1.9780186414718628\n",
            "Train loss for epoch8943: 1.9121826887130737\n",
            "Test loss for epoch8943: 2.080244779586792\n",
            "Train loss for epoch8944: 1.8115874528884888\n",
            "Test loss for epoch8944: 1.8186378479003906\n",
            "Train loss for epoch8945: 1.911582350730896\n",
            "Test loss for epoch8945: 2.0282058715820312\n",
            "Train loss for epoch8946: 1.9639854431152344\n",
            "Test loss for epoch8946: 1.9159551858901978\n",
            "Train loss for epoch8947: 2.1318562030792236\n",
            "Test loss for epoch8947: 2.089520215988159\n",
            "Train loss for epoch8948: 1.8443228006362915\n",
            "Test loss for epoch8948: 2.1463122367858887\n",
            "Train loss for epoch8949: 2.091028928756714\n",
            "Test loss for epoch8949: 2.3084444999694824\n",
            "Train loss for epoch8950: 1.7505371570587158\n",
            "Test loss for epoch8950: 2.1147866249084473\n",
            "Train loss for epoch8951: 2.112532138824463\n",
            "Test loss for epoch8951: 2.097733497619629\n",
            "Train loss for epoch8952: 1.9309959411621094\n",
            "Test loss for epoch8952: 2.063264846801758\n",
            "Train loss for epoch8953: 1.9575902223587036\n",
            "Test loss for epoch8953: 2.1537094116210938\n",
            "Train loss for epoch8954: 1.9841530323028564\n",
            "Test loss for epoch8954: 2.0682735443115234\n",
            "Train loss for epoch8955: 2.0651869773864746\n",
            "Test loss for epoch8955: 2.0188024044036865\n",
            "Train loss for epoch8956: 2.0264840126037598\n",
            "Test loss for epoch8956: 2.341660737991333\n",
            "Train loss for epoch8957: 1.926418662071228\n",
            "Test loss for epoch8957: 1.9407529830932617\n",
            "Train loss for epoch8958: 1.9316648244857788\n",
            "Test loss for epoch8958: 2.1055586338043213\n",
            "Train loss for epoch8959: 1.9390227794647217\n",
            "Test loss for epoch8959: 2.1712169647216797\n",
            "Train loss for epoch8960: 1.9080756902694702\n",
            "Test loss for epoch8960: 2.1657567024230957\n",
            "Train loss for epoch8961: 1.9016917943954468\n",
            "Test loss for epoch8961: 1.9505276679992676\n",
            "Train loss for epoch8962: 2.04573392868042\n",
            "Test loss for epoch8962: 1.996526837348938\n",
            "Train loss for epoch8963: 1.936809778213501\n",
            "Test loss for epoch8963: 1.9772049188613892\n",
            "Train loss for epoch8964: 1.850656509399414\n",
            "Test loss for epoch8964: 1.9497742652893066\n",
            "Train loss for epoch8965: 2.149439573287964\n",
            "Test loss for epoch8965: 2.2214715480804443\n",
            "Train loss for epoch8966: 1.7186126708984375\n",
            "Test loss for epoch8966: 1.9843477010726929\n",
            "Train loss for epoch8967: 2.055248260498047\n",
            "Test loss for epoch8967: 2.141146183013916\n",
            "Train loss for epoch8968: 2.1770172119140625\n",
            "Test loss for epoch8968: 2.1455323696136475\n",
            "Train loss for epoch8969: 1.9750735759735107\n",
            "Test loss for epoch8969: 1.9025704860687256\n",
            "Train loss for epoch8970: 1.7855563163757324\n",
            "Test loss for epoch8970: 1.689579725265503\n",
            "Train loss for epoch8971: 2.0989227294921875\n",
            "Test loss for epoch8971: 1.9850292205810547\n",
            "Train loss for epoch8972: 1.9839818477630615\n",
            "Test loss for epoch8972: 1.981908917427063\n",
            "Train loss for epoch8973: 2.026433229446411\n",
            "Test loss for epoch8973: 1.869695782661438\n",
            "Train loss for epoch8974: 1.9015021324157715\n",
            "Test loss for epoch8974: 2.0219671726226807\n",
            "Train loss for epoch8975: 2.0308210849761963\n",
            "Test loss for epoch8975: 1.8737493753433228\n",
            "Train loss for epoch8976: 1.9038984775543213\n",
            "Test loss for epoch8976: 1.8891937732696533\n",
            "Train loss for epoch8977: 1.8066816329956055\n",
            "Test loss for epoch8977: 2.0837137699127197\n",
            "Train loss for epoch8978: 1.9884486198425293\n",
            "Test loss for epoch8978: 1.8539701700210571\n",
            "Train loss for epoch8979: 2.0325093269348145\n",
            "Test loss for epoch8979: 1.9891068935394287\n",
            "Train loss for epoch8980: 1.8876999616622925\n",
            "Test loss for epoch8980: 2.1028268337249756\n",
            "Train loss for epoch8981: 1.9430571794509888\n",
            "Test loss for epoch8981: 2.0446009635925293\n",
            "Train loss for epoch8982: 1.8842791318893433\n",
            "Test loss for epoch8982: 2.027066469192505\n",
            "Train loss for epoch8983: 2.007070779800415\n",
            "Test loss for epoch8983: 2.0010592937469482\n",
            "Train loss for epoch8984: 1.9579147100448608\n",
            "Test loss for epoch8984: 1.8940438032150269\n",
            "Train loss for epoch8985: 2.093670606613159\n",
            "Test loss for epoch8985: 1.9063767194747925\n",
            "Train loss for epoch8986: 1.840951919555664\n",
            "Test loss for epoch8986: 2.1671340465545654\n",
            "Train loss for epoch8987: 2.095038652420044\n",
            "Test loss for epoch8987: 2.0424797534942627\n",
            "Train loss for epoch8988: 2.044466495513916\n",
            "Test loss for epoch8988: 2.1220061779022217\n",
            "Train loss for epoch8989: 2.2956676483154297\n",
            "Test loss for epoch8989: 2.0212643146514893\n",
            "Train loss for epoch8990: 1.9695566892623901\n",
            "Test loss for epoch8990: 1.9454537630081177\n",
            "Train loss for epoch8991: 1.9929804801940918\n",
            "Test loss for epoch8991: 2.36049747467041\n",
            "Train loss for epoch8992: 2.055464744567871\n",
            "Test loss for epoch8992: 2.188839912414551\n",
            "Train loss for epoch8993: 1.9964145421981812\n",
            "Test loss for epoch8993: 1.8329540491104126\n",
            "Train loss for epoch8994: 1.946895718574524\n",
            "Test loss for epoch8994: 1.9328463077545166\n",
            "Train loss for epoch8995: 1.8423839807510376\n",
            "Test loss for epoch8995: 1.997140645980835\n",
            "Train loss for epoch8996: 2.046816825866699\n",
            "Test loss for epoch8996: 2.187164306640625\n",
            "Train loss for epoch8997: 1.9312398433685303\n",
            "Test loss for epoch8997: 2.0419511795043945\n",
            "Train loss for epoch8998: 1.928892970085144\n",
            "Test loss for epoch8998: 1.8972554206848145\n",
            "Train loss for epoch8999: 2.0031163692474365\n",
            "Test loss for epoch8999: 2.0285353660583496\n",
            "Train loss for epoch9000: 1.7772045135498047\n",
            "Test loss for epoch9000: 2.0674819946289062\n",
            "Train loss for epoch9001: 1.9496355056762695\n",
            "Test loss for epoch9001: 2.0701775550842285\n",
            "Train loss for epoch9002: 1.937241554260254\n",
            "Test loss for epoch9002: 1.9434014558792114\n",
            "Train loss for epoch9003: 1.8179069757461548\n",
            "Test loss for epoch9003: 2.1183149814605713\n",
            "Train loss for epoch9004: 1.8889551162719727\n",
            "Test loss for epoch9004: 2.123433828353882\n",
            "Train loss for epoch9005: 1.9838135242462158\n",
            "Test loss for epoch9005: 1.9802950620651245\n",
            "Train loss for epoch9006: 1.8930577039718628\n",
            "Test loss for epoch9006: 1.9441065788269043\n",
            "Train loss for epoch9007: 1.9522449970245361\n",
            "Test loss for epoch9007: 2.091301679611206\n",
            "Train loss for epoch9008: 2.0381269454956055\n",
            "Test loss for epoch9008: 2.028745651245117\n",
            "Train loss for epoch9009: 2.0963826179504395\n",
            "Test loss for epoch9009: 1.942858338356018\n",
            "Train loss for epoch9010: 1.8358304500579834\n",
            "Test loss for epoch9010: 2.1559014320373535\n",
            "Train loss for epoch9011: 1.872875690460205\n",
            "Test loss for epoch9011: 2.0966665744781494\n",
            "Train loss for epoch9012: 2.019317388534546\n",
            "Test loss for epoch9012: 2.007009983062744\n",
            "Train loss for epoch9013: 2.0002362728118896\n",
            "Test loss for epoch9013: 2.2353501319885254\n",
            "Train loss for epoch9014: 1.9972552061080933\n",
            "Test loss for epoch9014: 2.122415542602539\n",
            "Train loss for epoch9015: 1.764416217803955\n",
            "Test loss for epoch9015: 2.0952248573303223\n",
            "Train loss for epoch9016: 1.8850764036178589\n",
            "Test loss for epoch9016: 2.0513360500335693\n",
            "Train loss for epoch9017: 1.9420808553695679\n",
            "Test loss for epoch9017: 2.1327285766601562\n",
            "Train loss for epoch9018: 2.205688953399658\n",
            "Test loss for epoch9018: 1.9732462167739868\n",
            "Train loss for epoch9019: 1.85300612449646\n",
            "Test loss for epoch9019: 2.0327084064483643\n",
            "Train loss for epoch9020: 1.868321180343628\n",
            "Test loss for epoch9020: 2.196467161178589\n",
            "Train loss for epoch9021: 1.9439681768417358\n",
            "Test loss for epoch9021: 1.9218854904174805\n",
            "Train loss for epoch9022: 2.070164442062378\n",
            "Test loss for epoch9022: 1.9797589778900146\n",
            "Train loss for epoch9023: 1.8651072978973389\n",
            "Test loss for epoch9023: 2.1542789936065674\n",
            "Train loss for epoch9024: 1.7242062091827393\n",
            "Test loss for epoch9024: 1.8983840942382812\n",
            "Train loss for epoch9025: 1.9578773975372314\n",
            "Test loss for epoch9025: 2.110506057739258\n",
            "Train loss for epoch9026: 2.0466325283050537\n",
            "Test loss for epoch9026: 2.1743597984313965\n",
            "Train loss for epoch9027: 1.8075348138809204\n",
            "Test loss for epoch9027: 2.0059547424316406\n",
            "Train loss for epoch9028: 1.9706242084503174\n",
            "Test loss for epoch9028: 2.022721767425537\n",
            "Train loss for epoch9029: 1.774664282798767\n",
            "Test loss for epoch9029: 2.0171759128570557\n",
            "Train loss for epoch9030: 1.9274077415466309\n",
            "Test loss for epoch9030: 1.9397153854370117\n",
            "Train loss for epoch9031: 2.016324758529663\n",
            "Test loss for epoch9031: 2.087813377380371\n",
            "Train loss for epoch9032: 1.9535537958145142\n",
            "Test loss for epoch9032: 2.006850004196167\n",
            "Train loss for epoch9033: 2.051598072052002\n",
            "Test loss for epoch9033: 2.227891683578491\n",
            "Train loss for epoch9034: 1.752028226852417\n",
            "Test loss for epoch9034: 2.2793939113616943\n",
            "Train loss for epoch9035: 1.9798580408096313\n",
            "Test loss for epoch9035: 2.098357677459717\n",
            "Train loss for epoch9036: 2.0336732864379883\n",
            "Test loss for epoch9036: 2.1048333644866943\n",
            "Train loss for epoch9037: 2.0287158489227295\n",
            "Test loss for epoch9037: 2.168516159057617\n",
            "Train loss for epoch9038: 1.8867267370224\n",
            "Test loss for epoch9038: 2.2112233638763428\n",
            "Train loss for epoch9039: 2.0744853019714355\n",
            "Test loss for epoch9039: 1.9656537771224976\n",
            "Train loss for epoch9040: 2.222522258758545\n",
            "Test loss for epoch9040: 2.0371782779693604\n",
            "Train loss for epoch9041: 1.7898728847503662\n",
            "Test loss for epoch9041: 1.9329601526260376\n",
            "Train loss for epoch9042: 1.9517614841461182\n",
            "Test loss for epoch9042: 1.9285378456115723\n",
            "Train loss for epoch9043: 1.892531394958496\n",
            "Test loss for epoch9043: 1.901536464691162\n",
            "Train loss for epoch9044: 2.0127735137939453\n",
            "Test loss for epoch9044: 2.2800610065460205\n",
            "Train loss for epoch9045: 1.8819125890731812\n",
            "Test loss for epoch9045: 1.9412150382995605\n",
            "Train loss for epoch9046: 1.9885333776474\n",
            "Test loss for epoch9046: 2.0387163162231445\n",
            "Train loss for epoch9047: 1.9546279907226562\n",
            "Test loss for epoch9047: 2.139195680618286\n",
            "Train loss for epoch9048: 1.82821524143219\n",
            "Test loss for epoch9048: 1.9242125749588013\n",
            "Train loss for epoch9049: 2.10496187210083\n",
            "Test loss for epoch9049: 2.2576498985290527\n",
            "Train loss for epoch9050: 1.9110227823257446\n",
            "Test loss for epoch9050: 2.103076696395874\n",
            "Train loss for epoch9051: 1.9935911893844604\n",
            "Test loss for epoch9051: 2.1776039600372314\n",
            "Train loss for epoch9052: 2.098055601119995\n",
            "Test loss for epoch9052: 2.1527841091156006\n",
            "Train loss for epoch9053: 1.9791978597640991\n",
            "Test loss for epoch9053: 2.0219171047210693\n",
            "Train loss for epoch9054: 2.090740203857422\n",
            "Test loss for epoch9054: 2.16772198677063\n",
            "Train loss for epoch9055: 1.8512914180755615\n",
            "Test loss for epoch9055: 2.27911114692688\n",
            "Train loss for epoch9056: 2.1893858909606934\n",
            "Test loss for epoch9056: 1.9840748310089111\n",
            "Train loss for epoch9057: 2.033703327178955\n",
            "Test loss for epoch9057: 1.9604363441467285\n",
            "Train loss for epoch9058: 1.8771238327026367\n",
            "Test loss for epoch9058: 2.057382583618164\n",
            "Train loss for epoch9059: 1.9004746675491333\n",
            "Test loss for epoch9059: 2.03757381439209\n",
            "Train loss for epoch9060: 2.0244200229644775\n",
            "Test loss for epoch9060: 2.094438076019287\n",
            "Train loss for epoch9061: 2.075502872467041\n",
            "Test loss for epoch9061: 2.1123123168945312\n",
            "Train loss for epoch9062: 1.838486909866333\n",
            "Test loss for epoch9062: 1.869762897491455\n",
            "Train loss for epoch9063: 1.9641788005828857\n",
            "Test loss for epoch9063: 2.164356231689453\n",
            "Train loss for epoch9064: 2.0781686305999756\n",
            "Test loss for epoch9064: 1.9022817611694336\n",
            "Train loss for epoch9065: 1.9017119407653809\n",
            "Test loss for epoch9065: 1.9053934812545776\n",
            "Train loss for epoch9066: 1.972479224205017\n",
            "Test loss for epoch9066: 2.1256186962127686\n",
            "Train loss for epoch9067: 2.0000131130218506\n",
            "Test loss for epoch9067: 2.082533597946167\n",
            "Train loss for epoch9068: 1.757006287574768\n",
            "Test loss for epoch9068: 1.9936332702636719\n",
            "Train loss for epoch9069: 1.8571312427520752\n",
            "Test loss for epoch9069: 1.9300144910812378\n",
            "Train loss for epoch9070: 2.026019811630249\n",
            "Test loss for epoch9070: 2.2689146995544434\n",
            "Train loss for epoch9071: 1.9412211179733276\n",
            "Test loss for epoch9071: 1.7858740091323853\n",
            "Train loss for epoch9072: 2.0723979473114014\n",
            "Test loss for epoch9072: 2.118818521499634\n",
            "Train loss for epoch9073: 1.8786355257034302\n",
            "Test loss for epoch9073: 2.021775007247925\n",
            "Train loss for epoch9074: 2.257662057876587\n",
            "Test loss for epoch9074: 1.9021003246307373\n",
            "Train loss for epoch9075: 1.9735873937606812\n",
            "Test loss for epoch9075: 1.920209288597107\n",
            "Train loss for epoch9076: 1.966300368309021\n",
            "Test loss for epoch9076: 2.0568110942840576\n",
            "Train loss for epoch9077: 1.9639087915420532\n",
            "Test loss for epoch9077: 1.992245078086853\n",
            "Train loss for epoch9078: 1.8468315601348877\n",
            "Test loss for epoch9078: 1.8712776899337769\n",
            "Train loss for epoch9079: 1.910637617111206\n",
            "Test loss for epoch9079: 2.107913017272949\n",
            "Train loss for epoch9080: 1.9956469535827637\n",
            "Test loss for epoch9080: 2.0773138999938965\n",
            "Train loss for epoch9081: 2.2213099002838135\n",
            "Test loss for epoch9081: 1.9655084609985352\n",
            "Train loss for epoch9082: 2.110811233520508\n",
            "Test loss for epoch9082: 2.209425687789917\n",
            "Train loss for epoch9083: 1.9715677499771118\n",
            "Test loss for epoch9083: 2.12764835357666\n",
            "Train loss for epoch9084: 1.8368743658065796\n",
            "Test loss for epoch9084: 2.030435800552368\n",
            "Train loss for epoch9085: 2.116408586502075\n",
            "Test loss for epoch9085: 2.1714730262756348\n",
            "Train loss for epoch9086: 2.0779342651367188\n",
            "Test loss for epoch9086: 2.106398344039917\n",
            "Train loss for epoch9087: 1.9116544723510742\n",
            "Test loss for epoch9087: 1.907007098197937\n",
            "Train loss for epoch9088: 2.0333008766174316\n",
            "Test loss for epoch9088: 2.187807321548462\n",
            "Train loss for epoch9089: 1.8851149082183838\n",
            "Test loss for epoch9089: 2.0016844272613525\n",
            "Train loss for epoch9090: 2.019843339920044\n",
            "Test loss for epoch9090: 2.0970582962036133\n",
            "Train loss for epoch9091: 1.904057264328003\n",
            "Test loss for epoch9091: 2.20906925201416\n",
            "Train loss for epoch9092: 1.6544897556304932\n",
            "Test loss for epoch9092: 2.054474115371704\n",
            "Train loss for epoch9093: 1.8368093967437744\n",
            "Test loss for epoch9093: 2.0680387020111084\n",
            "Train loss for epoch9094: 1.9461737871170044\n",
            "Test loss for epoch9094: 2.170069932937622\n",
            "Train loss for epoch9095: 2.0481979846954346\n",
            "Test loss for epoch9095: 2.1890511512756348\n",
            "Train loss for epoch9096: 1.9635881185531616\n",
            "Test loss for epoch9096: 2.101654291152954\n",
            "Train loss for epoch9097: 1.893066644668579\n",
            "Test loss for epoch9097: 2.175534963607788\n",
            "Train loss for epoch9098: 1.931241750717163\n",
            "Test loss for epoch9098: 1.9217660427093506\n",
            "Train loss for epoch9099: 1.9023489952087402\n",
            "Test loss for epoch9099: 1.9357692003250122\n",
            "Train loss for epoch9100: 1.8460549116134644\n",
            "Test loss for epoch9100: 2.1289830207824707\n",
            "Train loss for epoch9101: 1.902167558670044\n",
            "Test loss for epoch9101: 2.0052616596221924\n",
            "Train loss for epoch9102: 1.9409173727035522\n",
            "Test loss for epoch9102: 2.0887556076049805\n",
            "Train loss for epoch9103: 1.8397449254989624\n",
            "Test loss for epoch9103: 1.9859910011291504\n",
            "Train loss for epoch9104: 1.8126046657562256\n",
            "Test loss for epoch9104: 1.955450415611267\n",
            "Train loss for epoch9105: 2.0195534229278564\n",
            "Test loss for epoch9105: 2.067803144454956\n",
            "Train loss for epoch9106: 1.9254961013793945\n",
            "Test loss for epoch9106: 1.990359902381897\n",
            "Train loss for epoch9107: 2.1696105003356934\n",
            "Test loss for epoch9107: 1.9381130933761597\n",
            "Train loss for epoch9108: 1.9809343814849854\n",
            "Test loss for epoch9108: 2.075226068496704\n",
            "Train loss for epoch9109: 1.850740671157837\n",
            "Test loss for epoch9109: 1.944442629814148\n",
            "Train loss for epoch9110: 1.931397557258606\n",
            "Test loss for epoch9110: 2.029092788696289\n",
            "Train loss for epoch9111: 2.0717663764953613\n",
            "Test loss for epoch9111: 1.964444637298584\n",
            "Train loss for epoch9112: 1.699450135231018\n",
            "Test loss for epoch9112: 2.031358242034912\n",
            "Train loss for epoch9113: 1.9077383279800415\n",
            "Test loss for epoch9113: 2.0893828868865967\n",
            "Train loss for epoch9114: 1.9500292539596558\n",
            "Test loss for epoch9114: 1.9946211576461792\n",
            "Train loss for epoch9115: 1.9153441190719604\n",
            "Test loss for epoch9115: 1.934108853340149\n",
            "Train loss for epoch9116: 1.9268543720245361\n",
            "Test loss for epoch9116: 2.016808032989502\n",
            "Train loss for epoch9117: 1.8756130933761597\n",
            "Test loss for epoch9117: 2.036818265914917\n",
            "Train loss for epoch9118: 2.003375768661499\n",
            "Test loss for epoch9118: 2.0623624324798584\n",
            "Train loss for epoch9119: 2.112880229949951\n",
            "Test loss for epoch9119: 2.09714412689209\n",
            "Train loss for epoch9120: 2.0642197132110596\n",
            "Test loss for epoch9120: 2.0293426513671875\n",
            "Train loss for epoch9121: 1.9164230823516846\n",
            "Test loss for epoch9121: 2.0140397548675537\n",
            "Train loss for epoch9122: 1.931922197341919\n",
            "Test loss for epoch9122: 2.1450912952423096\n",
            "Train loss for epoch9123: 1.888644814491272\n",
            "Test loss for epoch9123: 2.0016109943389893\n",
            "Train loss for epoch9124: 2.010934591293335\n",
            "Test loss for epoch9124: 2.120668411254883\n",
            "Train loss for epoch9125: 2.0296223163604736\n",
            "Test loss for epoch9125: 2.0964925289154053\n",
            "Train loss for epoch9126: 1.7895731925964355\n",
            "Test loss for epoch9126: 2.1219661235809326\n",
            "Train loss for epoch9127: 1.9549957513809204\n",
            "Test loss for epoch9127: 2.024904489517212\n",
            "Train loss for epoch9128: 1.775194525718689\n",
            "Test loss for epoch9128: 2.3388819694519043\n",
            "Train loss for epoch9129: 1.8594717979431152\n",
            "Test loss for epoch9129: 2.14339280128479\n",
            "Train loss for epoch9130: 1.9583170413970947\n",
            "Test loss for epoch9130: 1.9070026874542236\n",
            "Train loss for epoch9131: 2.078648567199707\n",
            "Test loss for epoch9131: 2.1524300575256348\n",
            "Train loss for epoch9132: 1.8069758415222168\n",
            "Test loss for epoch9132: 2.1143248081207275\n",
            "Train loss for epoch9133: 1.9616974592208862\n",
            "Test loss for epoch9133: 2.0535457134246826\n",
            "Train loss for epoch9134: 2.2145204544067383\n",
            "Test loss for epoch9134: 2.2423555850982666\n",
            "Train loss for epoch9135: 2.013542652130127\n",
            "Test loss for epoch9135: 2.038815498352051\n",
            "Train loss for epoch9136: 2.007021903991699\n",
            "Test loss for epoch9136: 2.165513038635254\n",
            "Train loss for epoch9137: 1.884507417678833\n",
            "Test loss for epoch9137: 2.1553761959075928\n",
            "Train loss for epoch9138: 1.8852570056915283\n",
            "Test loss for epoch9138: 1.9779322147369385\n",
            "Train loss for epoch9139: 1.8604098558425903\n",
            "Test loss for epoch9139: 2.027855396270752\n",
            "Train loss for epoch9140: 1.9486480951309204\n",
            "Test loss for epoch9140: 1.864292860031128\n",
            "Train loss for epoch9141: 1.9268029928207397\n",
            "Test loss for epoch9141: 2.098341941833496\n",
            "Train loss for epoch9142: 1.7600326538085938\n",
            "Test loss for epoch9142: 1.9325095415115356\n",
            "Train loss for epoch9143: 1.791439414024353\n",
            "Test loss for epoch9143: 2.2348835468292236\n",
            "Train loss for epoch9144: 1.7742193937301636\n",
            "Test loss for epoch9144: 1.9879169464111328\n",
            "Train loss for epoch9145: 2.0632145404815674\n",
            "Test loss for epoch9145: 1.903221607208252\n",
            "Train loss for epoch9146: 1.8053314685821533\n",
            "Test loss for epoch9146: 1.9964537620544434\n",
            "Train loss for epoch9147: 2.090322494506836\n",
            "Test loss for epoch9147: 2.1485440731048584\n",
            "Train loss for epoch9148: 1.838172197341919\n",
            "Test loss for epoch9148: 2.194915294647217\n",
            "Train loss for epoch9149: 2.0330092906951904\n",
            "Test loss for epoch9149: 2.0437653064727783\n",
            "Train loss for epoch9150: 2.0187315940856934\n",
            "Test loss for epoch9150: 2.1972923278808594\n",
            "Train loss for epoch9151: 1.9210304021835327\n",
            "Test loss for epoch9151: 1.9178977012634277\n",
            "Train loss for epoch9152: 2.0337517261505127\n",
            "Test loss for epoch9152: 1.944673776626587\n",
            "Train loss for epoch9153: 2.1479060649871826\n",
            "Test loss for epoch9153: 1.926979899406433\n",
            "Train loss for epoch9154: 1.9346446990966797\n",
            "Test loss for epoch9154: 2.0599207878112793\n",
            "Train loss for epoch9155: 1.7651724815368652\n",
            "Test loss for epoch9155: 2.2031142711639404\n",
            "Train loss for epoch9156: 1.8155124187469482\n",
            "Test loss for epoch9156: 2.2051310539245605\n",
            "Train loss for epoch9157: 2.0017597675323486\n",
            "Test loss for epoch9157: 1.9453586339950562\n",
            "Train loss for epoch9158: 1.980490803718567\n",
            "Test loss for epoch9158: 2.182434320449829\n",
            "Train loss for epoch9159: 1.836052656173706\n",
            "Test loss for epoch9159: 2.1267781257629395\n",
            "Train loss for epoch9160: 1.874843955039978\n",
            "Test loss for epoch9160: 2.0696027278900146\n",
            "Train loss for epoch9161: 1.953017234802246\n",
            "Test loss for epoch9161: 1.948035717010498\n",
            "Train loss for epoch9162: 2.101253032684326\n",
            "Test loss for epoch9162: 2.0679128170013428\n",
            "Train loss for epoch9163: 1.980096459388733\n",
            "Test loss for epoch9163: 2.250823736190796\n",
            "Train loss for epoch9164: 1.7184793949127197\n",
            "Test loss for epoch9164: 2.0089874267578125\n",
            "Train loss for epoch9165: 1.9257762432098389\n",
            "Test loss for epoch9165: 1.905006766319275\n",
            "Train loss for epoch9166: 1.759028434753418\n",
            "Test loss for epoch9166: 2.1777307987213135\n",
            "Train loss for epoch9167: 2.0040862560272217\n",
            "Test loss for epoch9167: 2.242553949356079\n",
            "Train loss for epoch9168: 2.065783977508545\n",
            "Test loss for epoch9168: 1.8836358785629272\n",
            "Train loss for epoch9169: 2.106078863143921\n",
            "Test loss for epoch9169: 1.9667432308197021\n",
            "Train loss for epoch9170: 1.891435146331787\n",
            "Test loss for epoch9170: 1.8968210220336914\n",
            "Train loss for epoch9171: 1.8575259447097778\n",
            "Test loss for epoch9171: 2.043513298034668\n",
            "Train loss for epoch9172: 2.0988333225250244\n",
            "Test loss for epoch9172: 2.0371487140655518\n",
            "Train loss for epoch9173: 1.9526118040084839\n",
            "Test loss for epoch9173: 1.9650415182113647\n",
            "Train loss for epoch9174: 1.8128572702407837\n",
            "Test loss for epoch9174: 2.142296552658081\n",
            "Train loss for epoch9175: 1.9678443670272827\n",
            "Test loss for epoch9175: 2.1636202335357666\n",
            "Train loss for epoch9176: 2.112694501876831\n",
            "Test loss for epoch9176: 1.9842324256896973\n",
            "Train loss for epoch9177: 1.9306453466415405\n",
            "Test loss for epoch9177: 2.1376993656158447\n",
            "Train loss for epoch9178: 1.8205128908157349\n",
            "Test loss for epoch9178: 2.2422096729278564\n",
            "Train loss for epoch9179: 1.9828081130981445\n",
            "Test loss for epoch9179: 2.1219654083251953\n",
            "Train loss for epoch9180: 1.8335332870483398\n",
            "Test loss for epoch9180: 2.0220370292663574\n",
            "Train loss for epoch9181: 1.9029940366744995\n",
            "Test loss for epoch9181: 1.7219878435134888\n",
            "Train loss for epoch9182: 1.8718997240066528\n",
            "Test loss for epoch9182: 2.16706919670105\n",
            "Train loss for epoch9183: 2.0891482830047607\n",
            "Test loss for epoch9183: 2.214038372039795\n",
            "Train loss for epoch9184: 1.8505263328552246\n",
            "Test loss for epoch9184: 1.91592276096344\n",
            "Train loss for epoch9185: 1.9504244327545166\n",
            "Test loss for epoch9185: 2.073782205581665\n",
            "Train loss for epoch9186: 1.967286467552185\n",
            "Test loss for epoch9186: 2.115450620651245\n",
            "Train loss for epoch9187: 2.0194942951202393\n",
            "Test loss for epoch9187: 2.141862154006958\n",
            "Train loss for epoch9188: 1.9368863105773926\n",
            "Test loss for epoch9188: 1.9866288900375366\n",
            "Train loss for epoch9189: 2.0819711685180664\n",
            "Test loss for epoch9189: 2.2036068439483643\n",
            "Train loss for epoch9190: 2.1602978706359863\n",
            "Test loss for epoch9190: 2.3190715312957764\n",
            "Train loss for epoch9191: 2.1450185775756836\n",
            "Test loss for epoch9191: 2.12217378616333\n",
            "Train loss for epoch9192: 2.042048931121826\n",
            "Test loss for epoch9192: 2.255506992340088\n",
            "Train loss for epoch9193: 2.0223491191864014\n",
            "Test loss for epoch9193: 1.9461593627929688\n",
            "Train loss for epoch9194: 1.9747811555862427\n",
            "Test loss for epoch9194: 2.1414828300476074\n",
            "Train loss for epoch9195: 1.970821738243103\n",
            "Test loss for epoch9195: 2.00797963142395\n",
            "Train loss for epoch9196: 1.937956690788269\n",
            "Test loss for epoch9196: 1.798269510269165\n",
            "Train loss for epoch9197: 1.9827219247817993\n",
            "Test loss for epoch9197: 2.0915026664733887\n",
            "Train loss for epoch9198: 2.0905237197875977\n",
            "Test loss for epoch9198: 2.1417877674102783\n",
            "Train loss for epoch9199: 2.1139609813690186\n",
            "Test loss for epoch9199: 2.0332765579223633\n",
            "Train loss for epoch9200: 1.9172922372817993\n",
            "Test loss for epoch9200: 1.8390105962753296\n",
            "Train loss for epoch9201: 1.8691195249557495\n",
            "Test loss for epoch9201: 1.8312108516693115\n",
            "Train loss for epoch9202: 1.9199864864349365\n",
            "Test loss for epoch9202: 2.02811598777771\n",
            "Train loss for epoch9203: 2.182762861251831\n",
            "Test loss for epoch9203: 2.00384521484375\n",
            "Train loss for epoch9204: 1.9534034729003906\n",
            "Test loss for epoch9204: 1.867795467376709\n",
            "Train loss for epoch9205: 1.9569731950759888\n",
            "Test loss for epoch9205: 2.1326100826263428\n",
            "Train loss for epoch9206: 1.8627725839614868\n",
            "Test loss for epoch9206: 2.0545897483825684\n",
            "Train loss for epoch9207: 1.9320790767669678\n",
            "Test loss for epoch9207: 2.066861391067505\n",
            "Train loss for epoch9208: 1.9216970205307007\n",
            "Test loss for epoch9208: 1.876769781112671\n",
            "Train loss for epoch9209: 1.898421049118042\n",
            "Test loss for epoch9209: 2.1323421001434326\n",
            "Train loss for epoch9210: 1.9161721467971802\n",
            "Test loss for epoch9210: 2.1413156986236572\n",
            "Train loss for epoch9211: 2.018465518951416\n",
            "Test loss for epoch9211: 2.000974416732788\n",
            "Train loss for epoch9212: 1.8649623394012451\n",
            "Test loss for epoch9212: 2.0558159351348877\n",
            "Train loss for epoch9213: 2.1106204986572266\n",
            "Test loss for epoch9213: 2.112964391708374\n",
            "Train loss for epoch9214: 2.037175178527832\n",
            "Test loss for epoch9214: 2.193729877471924\n",
            "Train loss for epoch9215: 1.8475933074951172\n",
            "Test loss for epoch9215: 2.0347445011138916\n",
            "Train loss for epoch9216: 1.9371548891067505\n",
            "Test loss for epoch9216: 1.9448814392089844\n",
            "Train loss for epoch9217: 2.0503201484680176\n",
            "Test loss for epoch9217: 2.0204434394836426\n",
            "Train loss for epoch9218: 1.9816781282424927\n",
            "Test loss for epoch9218: 2.209721326828003\n",
            "Train loss for epoch9219: 1.729485034942627\n",
            "Test loss for epoch9219: 2.0727994441986084\n",
            "Train loss for epoch9220: 1.8949315547943115\n",
            "Test loss for epoch9220: 1.9455349445343018\n",
            "Train loss for epoch9221: 2.195728302001953\n",
            "Test loss for epoch9221: 2.1264429092407227\n",
            "Train loss for epoch9222: 1.8559257984161377\n",
            "Test loss for epoch9222: 2.189305543899536\n",
            "Train loss for epoch9223: 1.7591320276260376\n",
            "Test loss for epoch9223: 2.0824320316314697\n",
            "Train loss for epoch9224: 1.940479040145874\n",
            "Test loss for epoch9224: 2.0846798419952393\n",
            "Train loss for epoch9225: 1.9195432662963867\n",
            "Test loss for epoch9225: 2.1160123348236084\n",
            "Train loss for epoch9226: 1.8225756883621216\n",
            "Test loss for epoch9226: 2.077726364135742\n",
            "Train loss for epoch9227: 2.0129542350769043\n",
            "Test loss for epoch9227: 1.7679238319396973\n",
            "Train loss for epoch9228: 1.8384214639663696\n",
            "Test loss for epoch9228: 2.039477586746216\n",
            "Train loss for epoch9229: 1.9572193622589111\n",
            "Test loss for epoch9229: 1.908619999885559\n",
            "Train loss for epoch9230: 2.182826519012451\n",
            "Test loss for epoch9230: 1.9975029230117798\n",
            "Train loss for epoch9231: 2.024641513824463\n",
            "Test loss for epoch9231: 1.8632882833480835\n",
            "Train loss for epoch9232: 1.8018522262573242\n",
            "Test loss for epoch9232: 2.3758177757263184\n",
            "Train loss for epoch9233: 2.0457210540771484\n",
            "Test loss for epoch9233: 1.8548765182495117\n",
            "Train loss for epoch9234: 2.079986333847046\n",
            "Test loss for epoch9234: 2.104815721511841\n",
            "Train loss for epoch9235: 2.1361873149871826\n",
            "Test loss for epoch9235: 2.035155773162842\n",
            "Train loss for epoch9236: 2.0305323600769043\n",
            "Test loss for epoch9236: 2.1120190620422363\n",
            "Train loss for epoch9237: 1.6974307298660278\n",
            "Test loss for epoch9237: 2.0710155963897705\n",
            "Train loss for epoch9238: 1.9145257472991943\n",
            "Test loss for epoch9238: 2.0951430797576904\n",
            "Train loss for epoch9239: 2.2634878158569336\n",
            "Test loss for epoch9239: 1.933313250541687\n",
            "Train loss for epoch9240: 1.7740873098373413\n",
            "Test loss for epoch9240: 2.148810625076294\n",
            "Train loss for epoch9241: 1.9640796184539795\n",
            "Test loss for epoch9241: 1.9696025848388672\n",
            "Train loss for epoch9242: 1.838757038116455\n",
            "Test loss for epoch9242: 1.9060055017471313\n",
            "Train loss for epoch9243: 1.841632604598999\n",
            "Test loss for epoch9243: 1.96958327293396\n",
            "Train loss for epoch9244: 2.0836007595062256\n",
            "Test loss for epoch9244: 2.124295473098755\n",
            "Train loss for epoch9245: 2.1364669799804688\n",
            "Test loss for epoch9245: 2.007798671722412\n",
            "Train loss for epoch9246: 2.1896419525146484\n",
            "Test loss for epoch9246: 1.7566972970962524\n",
            "Train loss for epoch9247: 2.1737349033355713\n",
            "Test loss for epoch9247: 1.9740504026412964\n",
            "Train loss for epoch9248: 2.06523060798645\n",
            "Test loss for epoch9248: 2.2239153385162354\n",
            "Train loss for epoch9249: 1.850930094718933\n",
            "Test loss for epoch9249: 1.8360507488250732\n",
            "Train loss for epoch9250: 1.7397247552871704\n",
            "Test loss for epoch9250: 2.17073130607605\n",
            "Train loss for epoch9251: 1.7701411247253418\n",
            "Test loss for epoch9251: 2.016972303390503\n",
            "Train loss for epoch9252: 2.023197889328003\n",
            "Test loss for epoch9252: 2.2260732650756836\n",
            "Train loss for epoch9253: 2.0641016960144043\n",
            "Test loss for epoch9253: 1.9761773347854614\n",
            "Train loss for epoch9254: 1.9464194774627686\n",
            "Test loss for epoch9254: 2.12939453125\n",
            "Train loss for epoch9255: 1.994559407234192\n",
            "Test loss for epoch9255: 1.9923841953277588\n",
            "Train loss for epoch9256: 2.040498733520508\n",
            "Test loss for epoch9256: 1.9847463369369507\n",
            "Train loss for epoch9257: 2.0266547203063965\n",
            "Test loss for epoch9257: 2.1814205646514893\n",
            "Train loss for epoch9258: 2.0377089977264404\n",
            "Test loss for epoch9258: 2.051736831665039\n",
            "Train loss for epoch9259: 2.100400686264038\n",
            "Test loss for epoch9259: 2.009223699569702\n",
            "Train loss for epoch9260: 1.943875789642334\n",
            "Test loss for epoch9260: 2.0778138637542725\n",
            "Train loss for epoch9261: 2.0689756870269775\n",
            "Test loss for epoch9261: 1.8773308992385864\n",
            "Train loss for epoch9262: 2.0974204540252686\n",
            "Test loss for epoch9262: 2.229787826538086\n",
            "Train loss for epoch9263: 1.9677904844284058\n",
            "Test loss for epoch9263: 1.9604991674423218\n",
            "Train loss for epoch9264: 1.9031873941421509\n",
            "Test loss for epoch9264: 2.064002513885498\n",
            "Train loss for epoch9265: 2.101011276245117\n",
            "Test loss for epoch9265: 2.0626637935638428\n",
            "Train loss for epoch9266: 2.3362104892730713\n",
            "Test loss for epoch9266: 2.122833490371704\n",
            "Train loss for epoch9267: 1.9706848859786987\n",
            "Test loss for epoch9267: 1.9032886028289795\n",
            "Train loss for epoch9268: 2.0677735805511475\n",
            "Test loss for epoch9268: 1.8609250783920288\n",
            "Train loss for epoch9269: 1.7323185205459595\n",
            "Test loss for epoch9269: 1.9581191539764404\n",
            "Train loss for epoch9270: 2.114668130874634\n",
            "Test loss for epoch9270: 1.8266915082931519\n",
            "Train loss for epoch9271: 1.8733965158462524\n",
            "Test loss for epoch9271: 2.1205124855041504\n",
            "Train loss for epoch9272: 1.849536418914795\n",
            "Test loss for epoch9272: 2.253019332885742\n",
            "Train loss for epoch9273: 2.0070724487304688\n",
            "Test loss for epoch9273: 2.2468090057373047\n",
            "Train loss for epoch9274: 1.7932199239730835\n",
            "Test loss for epoch9274: 2.130612373352051\n",
            "Train loss for epoch9275: 2.0717718601226807\n",
            "Test loss for epoch9275: 2.144740581512451\n",
            "Train loss for epoch9276: 1.874788761138916\n",
            "Test loss for epoch9276: 1.9569886922836304\n",
            "Train loss for epoch9277: 1.7815343141555786\n",
            "Test loss for epoch9277: 1.9547224044799805\n",
            "Train loss for epoch9278: 1.9747151136398315\n",
            "Test loss for epoch9278: 1.8762308359146118\n",
            "Train loss for epoch9279: 2.102440595626831\n",
            "Test loss for epoch9279: 1.9475232362747192\n",
            "Train loss for epoch9280: 1.8154489994049072\n",
            "Test loss for epoch9280: 2.1359972953796387\n",
            "Train loss for epoch9281: 1.9669455289840698\n",
            "Test loss for epoch9281: 2.056793212890625\n",
            "Train loss for epoch9282: 2.0416436195373535\n",
            "Test loss for epoch9282: 2.0994296073913574\n",
            "Train loss for epoch9283: 2.0333170890808105\n",
            "Test loss for epoch9283: 2.2158663272857666\n",
            "Train loss for epoch9284: 1.7840389013290405\n",
            "Test loss for epoch9284: 2.0402839183807373\n",
            "Train loss for epoch9285: 2.068887710571289\n",
            "Test loss for epoch9285: 2.0289928913116455\n",
            "Train loss for epoch9286: 1.9641605615615845\n",
            "Test loss for epoch9286: 1.836300253868103\n",
            "Train loss for epoch9287: 1.9551293849945068\n",
            "Test loss for epoch9287: 2.196225166320801\n",
            "Train loss for epoch9288: 1.9793192148208618\n",
            "Test loss for epoch9288: 2.058547258377075\n",
            "Train loss for epoch9289: 1.770309567451477\n",
            "Test loss for epoch9289: 1.8827235698699951\n",
            "Train loss for epoch9290: 1.915860891342163\n",
            "Test loss for epoch9290: 2.095275402069092\n",
            "Train loss for epoch9291: 2.087632894515991\n",
            "Test loss for epoch9291: 2.218939781188965\n",
            "Train loss for epoch9292: 1.9249367713928223\n",
            "Test loss for epoch9292: 2.1606485843658447\n",
            "Train loss for epoch9293: 1.8919938802719116\n",
            "Test loss for epoch9293: 1.9538127183914185\n",
            "Train loss for epoch9294: 1.957203984260559\n",
            "Test loss for epoch9294: 2.1700408458709717\n",
            "Train loss for epoch9295: 1.8792364597320557\n",
            "Test loss for epoch9295: 2.1339645385742188\n",
            "Train loss for epoch9296: 1.9364557266235352\n",
            "Test loss for epoch9296: 1.7157551050186157\n",
            "Train loss for epoch9297: 1.9066587686538696\n",
            "Test loss for epoch9297: 1.97268545627594\n",
            "Train loss for epoch9298: 1.9304437637329102\n",
            "Test loss for epoch9298: 2.0478053092956543\n",
            "Train loss for epoch9299: 1.9365392923355103\n",
            "Test loss for epoch9299: 2.0494534969329834\n",
            "Train loss for epoch9300: 2.0927772521972656\n",
            "Test loss for epoch9300: 1.994693398475647\n",
            "Train loss for epoch9301: 2.0299346446990967\n",
            "Test loss for epoch9301: 2.132493257522583\n",
            "Train loss for epoch9302: 1.9512001276016235\n",
            "Test loss for epoch9302: 2.025317430496216\n",
            "Train loss for epoch9303: 1.8462589979171753\n",
            "Test loss for epoch9303: 2.0076301097869873\n",
            "Train loss for epoch9304: 2.074657440185547\n",
            "Test loss for epoch9304: 2.065643072128296\n",
            "Train loss for epoch9305: 2.0610103607177734\n",
            "Test loss for epoch9305: 2.062678575515747\n",
            "Train loss for epoch9306: 1.9836965799331665\n",
            "Test loss for epoch9306: 2.116509437561035\n",
            "Train loss for epoch9307: 2.031510353088379\n",
            "Test loss for epoch9307: 1.9253625869750977\n",
            "Train loss for epoch9308: 1.8244801759719849\n",
            "Test loss for epoch9308: 2.0455784797668457\n",
            "Train loss for epoch9309: 1.9402379989624023\n",
            "Test loss for epoch9309: 1.92843759059906\n",
            "Train loss for epoch9310: 1.9335789680480957\n",
            "Test loss for epoch9310: 2.263021230697632\n",
            "Train loss for epoch9311: 1.756899356842041\n",
            "Test loss for epoch9311: 2.2345151901245117\n",
            "Train loss for epoch9312: 1.8536027669906616\n",
            "Test loss for epoch9312: 2.015890121459961\n",
            "Train loss for epoch9313: 1.9217499494552612\n",
            "Test loss for epoch9313: 2.124542474746704\n",
            "Train loss for epoch9314: 2.0691559314727783\n",
            "Test loss for epoch9314: 2.0892436504364014\n",
            "Train loss for epoch9315: 1.9582542181015015\n",
            "Test loss for epoch9315: 2.001605272293091\n",
            "Train loss for epoch9316: 1.8606455326080322\n",
            "Test loss for epoch9316: 1.9725415706634521\n",
            "Train loss for epoch9317: 1.9403761625289917\n",
            "Test loss for epoch9317: 1.9706391096115112\n",
            "Train loss for epoch9318: 1.8317660093307495\n",
            "Test loss for epoch9318: 2.0031938552856445\n",
            "Train loss for epoch9319: 1.9407446384429932\n",
            "Test loss for epoch9319: 2.03994083404541\n",
            "Train loss for epoch9320: 2.05964994430542\n",
            "Test loss for epoch9320: 1.9410940408706665\n",
            "Train loss for epoch9321: 1.7947273254394531\n",
            "Test loss for epoch9321: 1.7901155948638916\n",
            "Train loss for epoch9322: 1.8767049312591553\n",
            "Test loss for epoch9322: 2.263925313949585\n",
            "Train loss for epoch9323: 1.7095122337341309\n",
            "Test loss for epoch9323: 2.149275541305542\n",
            "Train loss for epoch9324: 1.775459885597229\n",
            "Test loss for epoch9324: 1.915549874305725\n",
            "Train loss for epoch9325: 1.8775758743286133\n",
            "Test loss for epoch9325: 2.05668568611145\n",
            "Train loss for epoch9326: 1.9387129545211792\n",
            "Test loss for epoch9326: 2.1030995845794678\n",
            "Train loss for epoch9327: 2.0984041690826416\n",
            "Test loss for epoch9327: 2.0051496028900146\n",
            "Train loss for epoch9328: 1.849236249923706\n",
            "Test loss for epoch9328: 1.8976978063583374\n",
            "Train loss for epoch9329: 2.1117544174194336\n",
            "Test loss for epoch9329: 2.1994118690490723\n",
            "Train loss for epoch9330: 1.9802404642105103\n",
            "Test loss for epoch9330: 2.076322555541992\n",
            "Train loss for epoch9331: 2.0687196254730225\n",
            "Test loss for epoch9331: 1.9711109399795532\n",
            "Train loss for epoch9332: 1.8461297750473022\n",
            "Test loss for epoch9332: 2.140099048614502\n",
            "Train loss for epoch9333: 1.8632334470748901\n",
            "Test loss for epoch9333: 2.042231321334839\n",
            "Train loss for epoch9334: 1.837803840637207\n",
            "Test loss for epoch9334: 1.9629712104797363\n",
            "Train loss for epoch9335: 2.0195186138153076\n",
            "Test loss for epoch9335: 1.8851814270019531\n",
            "Train loss for epoch9336: 1.800713300704956\n",
            "Test loss for epoch9336: 2.171626091003418\n",
            "Train loss for epoch9337: 2.207089900970459\n",
            "Test loss for epoch9337: 2.08787202835083\n",
            "Train loss for epoch9338: 2.042181968688965\n",
            "Test loss for epoch9338: 2.0410995483398438\n",
            "Train loss for epoch9339: 2.200089693069458\n",
            "Test loss for epoch9339: 2.0328361988067627\n",
            "Train loss for epoch9340: 2.202878952026367\n",
            "Test loss for epoch9340: 2.2437844276428223\n",
            "Train loss for epoch9341: 1.8054020404815674\n",
            "Test loss for epoch9341: 2.22713303565979\n",
            "Train loss for epoch9342: 1.8714423179626465\n",
            "Test loss for epoch9342: 2.032846450805664\n",
            "Train loss for epoch9343: 1.9419859647750854\n",
            "Test loss for epoch9343: 1.993653416633606\n",
            "Train loss for epoch9344: 2.0190749168395996\n",
            "Test loss for epoch9344: 2.0988903045654297\n",
            "Train loss for epoch9345: 1.8595037460327148\n",
            "Test loss for epoch9345: 2.094403028488159\n",
            "Train loss for epoch9346: 1.9556355476379395\n",
            "Test loss for epoch9346: 1.8794841766357422\n",
            "Train loss for epoch9347: 1.7156277894973755\n",
            "Test loss for epoch9347: 2.0267739295959473\n",
            "Train loss for epoch9348: 1.773941159248352\n",
            "Test loss for epoch9348: 2.1646876335144043\n",
            "Train loss for epoch9349: 1.9330549240112305\n",
            "Test loss for epoch9349: 2.108100652694702\n",
            "Train loss for epoch9350: 2.098250150680542\n",
            "Test loss for epoch9350: 2.005678653717041\n",
            "Train loss for epoch9351: 2.091228485107422\n",
            "Test loss for epoch9351: 1.9944225549697876\n",
            "Train loss for epoch9352: 1.9911739826202393\n",
            "Test loss for epoch9352: 2.1080455780029297\n",
            "Train loss for epoch9353: 1.970249891281128\n",
            "Test loss for epoch9353: 1.981803297996521\n",
            "Train loss for epoch9354: 1.9925390481948853\n",
            "Test loss for epoch9354: 2.0074071884155273\n",
            "Train loss for epoch9355: 2.085162401199341\n",
            "Test loss for epoch9355: 2.078986406326294\n",
            "Train loss for epoch9356: 1.9582040309906006\n",
            "Test loss for epoch9356: 2.0990569591522217\n",
            "Train loss for epoch9357: 1.858858346939087\n",
            "Test loss for epoch9357: 1.8967907428741455\n",
            "Train loss for epoch9358: 1.9406763315200806\n",
            "Test loss for epoch9358: 2.0230016708374023\n",
            "Train loss for epoch9359: 1.9858973026275635\n",
            "Test loss for epoch9359: 2.053006887435913\n",
            "Train loss for epoch9360: 1.907761812210083\n",
            "Test loss for epoch9360: 2.118844509124756\n",
            "Train loss for epoch9361: 1.998693585395813\n",
            "Test loss for epoch9361: 1.9310706853866577\n",
            "Train loss for epoch9362: 2.1292543411254883\n",
            "Test loss for epoch9362: 2.080986499786377\n",
            "Train loss for epoch9363: 2.0360164642333984\n",
            "Test loss for epoch9363: 1.9630205631256104\n",
            "Train loss for epoch9364: 1.858916163444519\n",
            "Test loss for epoch9364: 2.130018711090088\n",
            "Train loss for epoch9365: 1.8814427852630615\n",
            "Test loss for epoch9365: 2.2580363750457764\n",
            "Train loss for epoch9366: 2.046691656112671\n",
            "Test loss for epoch9366: 1.9082612991333008\n",
            "Train loss for epoch9367: 1.8858643770217896\n",
            "Test loss for epoch9367: 1.9456400871276855\n",
            "Train loss for epoch9368: 2.105151653289795\n",
            "Test loss for epoch9368: 1.8768953084945679\n",
            "Train loss for epoch9369: 1.891249179840088\n",
            "Test loss for epoch9369: 2.216015100479126\n",
            "Train loss for epoch9370: 1.7500817775726318\n",
            "Test loss for epoch9370: 2.0308194160461426\n",
            "Train loss for epoch9371: 1.8700138330459595\n",
            "Test loss for epoch9371: 2.324305534362793\n",
            "Train loss for epoch9372: 2.0250861644744873\n",
            "Test loss for epoch9372: 1.9464643001556396\n",
            "Train loss for epoch9373: 2.0832159519195557\n",
            "Test loss for epoch9373: 2.121084451675415\n",
            "Train loss for epoch9374: 1.8892006874084473\n",
            "Test loss for epoch9374: 2.079313278198242\n",
            "Train loss for epoch9375: 1.8143316507339478\n",
            "Test loss for epoch9375: 2.0687620639801025\n",
            "Train loss for epoch9376: 1.9214978218078613\n",
            "Test loss for epoch9376: 2.056236505508423\n",
            "Train loss for epoch9377: 1.8442316055297852\n",
            "Test loss for epoch9377: 2.0688631534576416\n",
            "Train loss for epoch9378: 2.0445446968078613\n",
            "Test loss for epoch9378: 2.2560794353485107\n",
            "Train loss for epoch9379: 1.9948818683624268\n",
            "Test loss for epoch9379: 2.0189192295074463\n",
            "Train loss for epoch9380: 2.0751471519470215\n",
            "Test loss for epoch9380: 2.0499520301818848\n",
            "Train loss for epoch9381: 2.0480687618255615\n",
            "Test loss for epoch9381: 1.9009770154953003\n",
            "Train loss for epoch9382: 1.8690465688705444\n",
            "Test loss for epoch9382: 2.061967134475708\n",
            "Train loss for epoch9383: 1.792081594467163\n",
            "Test loss for epoch9383: 1.9462724924087524\n",
            "Train loss for epoch9384: 2.013002634048462\n",
            "Test loss for epoch9384: 1.9998183250427246\n",
            "Train loss for epoch9385: 1.7090955972671509\n",
            "Test loss for epoch9385: 2.101898670196533\n",
            "Train loss for epoch9386: 1.8835517168045044\n",
            "Test loss for epoch9386: 2.2493832111358643\n",
            "Train loss for epoch9387: 2.078761577606201\n",
            "Test loss for epoch9387: 2.155776023864746\n",
            "Train loss for epoch9388: 1.8868591785430908\n",
            "Test loss for epoch9388: 2.0294902324676514\n",
            "Train loss for epoch9389: 2.0170419216156006\n",
            "Test loss for epoch9389: 2.0072734355926514\n",
            "Train loss for epoch9390: 2.0386111736297607\n",
            "Test loss for epoch9390: 2.1535022258758545\n",
            "Train loss for epoch9391: 1.8881219625473022\n",
            "Test loss for epoch9391: 1.8732967376708984\n",
            "Train loss for epoch9392: 1.9240931272506714\n",
            "Test loss for epoch9392: 1.9432986974716187\n",
            "Train loss for epoch9393: 1.9436335563659668\n",
            "Test loss for epoch9393: 2.0514791011810303\n",
            "Train loss for epoch9394: 2.126258134841919\n",
            "Test loss for epoch9394: 2.333811044692993\n",
            "Train loss for epoch9395: 2.0695865154266357\n",
            "Test loss for epoch9395: 1.9610023498535156\n",
            "Train loss for epoch9396: 1.9196386337280273\n",
            "Test loss for epoch9396: 2.0228943824768066\n",
            "Train loss for epoch9397: 2.1287176609039307\n",
            "Test loss for epoch9397: 1.8229613304138184\n",
            "Train loss for epoch9398: 1.897840142250061\n",
            "Test loss for epoch9398: 2.2235679626464844\n",
            "Train loss for epoch9399: 2.0783002376556396\n",
            "Test loss for epoch9399: 1.9753516912460327\n",
            "Train loss for epoch9400: 2.0621697902679443\n",
            "Test loss for epoch9400: 2.0713789463043213\n",
            "Train loss for epoch9401: 2.1281306743621826\n",
            "Test loss for epoch9401: 1.9012548923492432\n",
            "Train loss for epoch9402: 1.8793407678604126\n",
            "Test loss for epoch9402: 2.1572189331054688\n",
            "Train loss for epoch9403: 1.9856610298156738\n",
            "Test loss for epoch9403: 2.1531853675842285\n",
            "Train loss for epoch9404: 2.132689952850342\n",
            "Test loss for epoch9404: 2.0297038555145264\n",
            "Train loss for epoch9405: 2.0049078464508057\n",
            "Test loss for epoch9405: 1.9541741609573364\n",
            "Train loss for epoch9406: 2.066913366317749\n",
            "Test loss for epoch9406: 2.1736395359039307\n",
            "Train loss for epoch9407: 2.189958095550537\n",
            "Test loss for epoch9407: 2.1422266960144043\n",
            "Train loss for epoch9408: 1.8448060750961304\n",
            "Test loss for epoch9408: 2.063983201980591\n",
            "Train loss for epoch9409: 1.8789699077606201\n",
            "Test loss for epoch9409: 1.892266035079956\n",
            "Train loss for epoch9410: 1.7194985151290894\n",
            "Test loss for epoch9410: 2.076603889465332\n",
            "Train loss for epoch9411: 1.9638259410858154\n",
            "Test loss for epoch9411: 2.1726388931274414\n",
            "Train loss for epoch9412: 1.8601980209350586\n",
            "Test loss for epoch9412: 2.085097074508667\n",
            "Train loss for epoch9413: 1.8242218494415283\n",
            "Test loss for epoch9413: 2.107063055038452\n",
            "Train loss for epoch9414: 1.890100359916687\n",
            "Test loss for epoch9414: 2.0528616905212402\n",
            "Train loss for epoch9415: 1.8954317569732666\n",
            "Test loss for epoch9415: 2.1468796730041504\n",
            "Train loss for epoch9416: 2.115072011947632\n",
            "Test loss for epoch9416: 1.9545197486877441\n",
            "Train loss for epoch9417: 2.3063151836395264\n",
            "Test loss for epoch9417: 1.9049943685531616\n",
            "Train loss for epoch9418: 1.924843668937683\n",
            "Test loss for epoch9418: 2.1408112049102783\n",
            "Train loss for epoch9419: 2.1321139335632324\n",
            "Test loss for epoch9419: 1.91182541847229\n",
            "Train loss for epoch9420: 2.0193541049957275\n",
            "Test loss for epoch9420: 2.0605480670928955\n",
            "Train loss for epoch9421: 1.854508638381958\n",
            "Test loss for epoch9421: 2.118518114089966\n",
            "Train loss for epoch9422: 1.8141824007034302\n",
            "Test loss for epoch9422: 1.9401649236679077\n",
            "Train loss for epoch9423: 1.9119518995285034\n",
            "Test loss for epoch9423: 1.8995318412780762\n",
            "Train loss for epoch9424: 1.9536360502243042\n",
            "Test loss for epoch9424: 1.816146731376648\n",
            "Train loss for epoch9425: 1.9936498403549194\n",
            "Test loss for epoch9425: 1.9007753133773804\n",
            "Train loss for epoch9426: 1.8277324438095093\n",
            "Test loss for epoch9426: 2.0407183170318604\n",
            "Train loss for epoch9427: 1.892439842224121\n",
            "Test loss for epoch9427: 1.9769809246063232\n",
            "Train loss for epoch9428: 1.9672836065292358\n",
            "Test loss for epoch9428: 1.9503110647201538\n",
            "Train loss for epoch9429: 2.002162218093872\n",
            "Test loss for epoch9429: 2.084918737411499\n",
            "Train loss for epoch9430: 1.823376178741455\n",
            "Test loss for epoch9430: 1.9888838529586792\n",
            "Train loss for epoch9431: 1.9656516313552856\n",
            "Test loss for epoch9431: 1.9796322584152222\n",
            "Train loss for epoch9432: 1.9378299713134766\n",
            "Test loss for epoch9432: 2.161825180053711\n",
            "Train loss for epoch9433: 1.908813714981079\n",
            "Test loss for epoch9433: 2.037101984024048\n",
            "Train loss for epoch9434: 2.21002459526062\n",
            "Test loss for epoch9434: 2.1415345668792725\n",
            "Train loss for epoch9435: 1.9525166749954224\n",
            "Test loss for epoch9435: 2.2266576290130615\n",
            "Train loss for epoch9436: 2.2601053714752197\n",
            "Test loss for epoch9436: 1.9786086082458496\n",
            "Train loss for epoch9437: 1.9162200689315796\n",
            "Test loss for epoch9437: 2.205324411392212\n",
            "Train loss for epoch9438: 1.9594908952713013\n",
            "Test loss for epoch9438: 2.1113693714141846\n",
            "Train loss for epoch9439: 1.9250050783157349\n",
            "Test loss for epoch9439: 1.9702874422073364\n",
            "Train loss for epoch9440: 2.039482593536377\n",
            "Test loss for epoch9440: 2.156961679458618\n",
            "Train loss for epoch9441: 1.9260787963867188\n",
            "Test loss for epoch9441: 2.1215884685516357\n",
            "Train loss for epoch9442: 1.8971930742263794\n",
            "Test loss for epoch9442: 2.133173704147339\n",
            "Train loss for epoch9443: 1.6500571966171265\n",
            "Test loss for epoch9443: 2.1427106857299805\n",
            "Train loss for epoch9444: 2.0034632682800293\n",
            "Test loss for epoch9444: 2.0911874771118164\n",
            "Train loss for epoch9445: 2.0845398902893066\n",
            "Test loss for epoch9445: 2.2498888969421387\n",
            "Train loss for epoch9446: 1.99226975440979\n",
            "Test loss for epoch9446: 1.975081443786621\n",
            "Train loss for epoch9447: 2.04548978805542\n",
            "Test loss for epoch9447: 2.0162625312805176\n",
            "Train loss for epoch9448: 1.904964804649353\n",
            "Test loss for epoch9448: 2.1205453872680664\n",
            "Train loss for epoch9449: 1.7923158407211304\n",
            "Test loss for epoch9449: 2.116173267364502\n",
            "Train loss for epoch9450: 1.83305025100708\n",
            "Test loss for epoch9450: 2.0255813598632812\n",
            "Train loss for epoch9451: 1.9469059705734253\n",
            "Test loss for epoch9451: 2.266772985458374\n",
            "Train loss for epoch9452: 1.9968339204788208\n",
            "Test loss for epoch9452: 2.2042367458343506\n",
            "Train loss for epoch9453: 1.8865368366241455\n",
            "Test loss for epoch9453: 1.9491913318634033\n",
            "Train loss for epoch9454: 1.8794437646865845\n",
            "Test loss for epoch9454: 1.954591155052185\n",
            "Train loss for epoch9455: 1.9788627624511719\n",
            "Test loss for epoch9455: 2.0655605792999268\n",
            "Train loss for epoch9456: 1.8721671104431152\n",
            "Test loss for epoch9456: 1.9288511276245117\n",
            "Train loss for epoch9457: 1.9514625072479248\n",
            "Test loss for epoch9457: 2.0779500007629395\n",
            "Train loss for epoch9458: 1.8932487964630127\n",
            "Test loss for epoch9458: 2.0711617469787598\n",
            "Train loss for epoch9459: 1.7921115159988403\n",
            "Test loss for epoch9459: 2.0956404209136963\n",
            "Train loss for epoch9460: 1.6989988088607788\n",
            "Test loss for epoch9460: 1.995221734046936\n",
            "Train loss for epoch9461: 1.7320719957351685\n",
            "Test loss for epoch9461: 2.194495916366577\n",
            "Train loss for epoch9462: 1.9123759269714355\n",
            "Test loss for epoch9462: 1.9788837432861328\n",
            "Train loss for epoch9463: 1.857094645500183\n",
            "Test loss for epoch9463: 2.3331527709960938\n",
            "Train loss for epoch9464: 2.0509145259857178\n",
            "Test loss for epoch9464: 2.1183440685272217\n",
            "Train loss for epoch9465: 2.006378173828125\n",
            "Test loss for epoch9465: 2.0863611698150635\n",
            "Train loss for epoch9466: 1.7723530530929565\n",
            "Test loss for epoch9466: 2.1172385215759277\n",
            "Train loss for epoch9467: 1.87474524974823\n",
            "Test loss for epoch9467: 2.015904188156128\n",
            "Train loss for epoch9468: 1.8885271549224854\n",
            "Test loss for epoch9468: 2.0361788272857666\n",
            "Train loss for epoch9469: 1.7861475944519043\n",
            "Test loss for epoch9469: 1.8984358310699463\n",
            "Train loss for epoch9470: 1.9548455476760864\n",
            "Test loss for epoch9470: 1.9875613451004028\n",
            "Train loss for epoch9471: 2.019498109817505\n",
            "Test loss for epoch9471: 2.0786945819854736\n",
            "Train loss for epoch9472: 1.9931426048278809\n",
            "Test loss for epoch9472: 1.9631415605545044\n",
            "Train loss for epoch9473: 2.014780282974243\n",
            "Test loss for epoch9473: 2.0396416187286377\n",
            "Train loss for epoch9474: 2.0141749382019043\n",
            "Test loss for epoch9474: 1.903272271156311\n",
            "Train loss for epoch9475: 1.9945653676986694\n",
            "Test loss for epoch9475: 1.9160370826721191\n",
            "Train loss for epoch9476: 1.9910560846328735\n",
            "Test loss for epoch9476: 1.9872666597366333\n",
            "Train loss for epoch9477: 1.7280434370040894\n",
            "Test loss for epoch9477: 2.1085383892059326\n",
            "Train loss for epoch9478: 2.030341863632202\n",
            "Test loss for epoch9478: 1.8092225790023804\n",
            "Train loss for epoch9479: 1.9023462533950806\n",
            "Test loss for epoch9479: 1.9577189683914185\n",
            "Train loss for epoch9480: 1.921235203742981\n",
            "Test loss for epoch9480: 2.07147216796875\n",
            "Train loss for epoch9481: 1.9570444822311401\n",
            "Test loss for epoch9481: 1.943022608757019\n",
            "Train loss for epoch9482: 1.9374183416366577\n",
            "Test loss for epoch9482: 1.8114535808563232\n",
            "Train loss for epoch9483: 1.9971601963043213\n",
            "Test loss for epoch9483: 1.936686396598816\n",
            "Train loss for epoch9484: 2.0192220211029053\n",
            "Test loss for epoch9484: 1.93665611743927\n",
            "Train loss for epoch9485: 2.0774457454681396\n",
            "Test loss for epoch9485: 1.9595822095870972\n",
            "Train loss for epoch9486: 1.778151035308838\n",
            "Test loss for epoch9486: 2.2153196334838867\n",
            "Train loss for epoch9487: 2.0116074085235596\n",
            "Test loss for epoch9487: 1.9201513528823853\n",
            "Train loss for epoch9488: 1.6686415672302246\n",
            "Test loss for epoch9488: 1.9960342645645142\n",
            "Train loss for epoch9489: 1.9635580778121948\n",
            "Test loss for epoch9489: 1.994842767715454\n",
            "Train loss for epoch9490: 1.9116312265396118\n",
            "Test loss for epoch9490: 1.981592059135437\n",
            "Train loss for epoch9491: 1.9778039455413818\n",
            "Test loss for epoch9491: 2.2647511959075928\n",
            "Train loss for epoch9492: 1.8870975971221924\n",
            "Test loss for epoch9492: 1.9873697757720947\n",
            "Train loss for epoch9493: 1.901071310043335\n",
            "Test loss for epoch9493: 1.9583903551101685\n",
            "Train loss for epoch9494: 1.7733405828475952\n",
            "Test loss for epoch9494: 1.9243346452713013\n",
            "Train loss for epoch9495: 1.9175196886062622\n",
            "Test loss for epoch9495: 2.021960735321045\n",
            "Train loss for epoch9496: 1.7262893915176392\n",
            "Test loss for epoch9496: 2.123842477798462\n",
            "Train loss for epoch9497: 1.8816038370132446\n",
            "Test loss for epoch9497: 2.082594633102417\n",
            "Train loss for epoch9498: 1.8693536520004272\n",
            "Test loss for epoch9498: 1.823982834815979\n",
            "Train loss for epoch9499: 1.9359525442123413\n",
            "Test loss for epoch9499: 2.101001024246216\n",
            "Train loss for epoch9500: 1.9808567762374878\n",
            "Test loss for epoch9500: 2.076964855194092\n",
            "Train loss for epoch9501: 2.1036033630371094\n",
            "Test loss for epoch9501: 1.871106743812561\n",
            "Train loss for epoch9502: 2.0446083545684814\n",
            "Test loss for epoch9502: 2.055755615234375\n",
            "Train loss for epoch9503: 1.8234351873397827\n",
            "Test loss for epoch9503: 1.8470534086227417\n",
            "Train loss for epoch9504: 2.06526255607605\n",
            "Test loss for epoch9504: 2.0526845455169678\n",
            "Train loss for epoch9505: 2.0205490589141846\n",
            "Test loss for epoch9505: 1.924745798110962\n",
            "Train loss for epoch9506: 1.925377368927002\n",
            "Test loss for epoch9506: 2.1159493923187256\n",
            "Train loss for epoch9507: 1.810771107673645\n",
            "Test loss for epoch9507: 2.095734119415283\n",
            "Train loss for epoch9508: 1.840617060661316\n",
            "Test loss for epoch9508: 2.3375747203826904\n",
            "Train loss for epoch9509: 1.8827683925628662\n",
            "Test loss for epoch9509: 1.9369477033615112\n",
            "Train loss for epoch9510: 2.1142232418060303\n",
            "Test loss for epoch9510: 2.0793442726135254\n",
            "Train loss for epoch9511: 1.8246694803237915\n",
            "Test loss for epoch9511: 2.217360734939575\n",
            "Train loss for epoch9512: 1.9198938608169556\n",
            "Test loss for epoch9512: 2.03298020362854\n",
            "Train loss for epoch9513: 1.6654672622680664\n",
            "Test loss for epoch9513: 2.0568320751190186\n",
            "Train loss for epoch9514: 1.818227767944336\n",
            "Test loss for epoch9514: 2.0427889823913574\n",
            "Train loss for epoch9515: 2.131564140319824\n",
            "Test loss for epoch9515: 2.075305700302124\n",
            "Train loss for epoch9516: 1.9410862922668457\n",
            "Test loss for epoch9516: 1.8348947763442993\n",
            "Train loss for epoch9517: 1.9978951215744019\n",
            "Test loss for epoch9517: 1.9528918266296387\n",
            "Train loss for epoch9518: 2.0824732780456543\n",
            "Test loss for epoch9518: 2.274683713912964\n",
            "Train loss for epoch9519: 1.972436785697937\n",
            "Test loss for epoch9519: 2.0190932750701904\n",
            "Train loss for epoch9520: 1.7458057403564453\n",
            "Test loss for epoch9520: 2.1408474445343018\n",
            "Train loss for epoch9521: 1.8789968490600586\n",
            "Test loss for epoch9521: 2.0190412998199463\n",
            "Train loss for epoch9522: 1.854285717010498\n",
            "Test loss for epoch9522: 2.051032543182373\n",
            "Train loss for epoch9523: 2.0558278560638428\n",
            "Test loss for epoch9523: 2.076622247695923\n",
            "Train loss for epoch9524: 1.9648935794830322\n",
            "Test loss for epoch9524: 1.9998019933700562\n",
            "Train loss for epoch9525: 1.8666826486587524\n",
            "Test loss for epoch9525: 2.048921585083008\n",
            "Train loss for epoch9526: 1.8675470352172852\n",
            "Test loss for epoch9526: 2.2167630195617676\n",
            "Train loss for epoch9527: 1.8394802808761597\n",
            "Test loss for epoch9527: 2.0704779624938965\n",
            "Train loss for epoch9528: 1.9050624370574951\n",
            "Test loss for epoch9528: 1.980296015739441\n",
            "Train loss for epoch9529: 2.033691883087158\n",
            "Test loss for epoch9529: 2.066769599914551\n",
            "Train loss for epoch9530: 1.8506343364715576\n",
            "Test loss for epoch9530: 2.274542808532715\n",
            "Train loss for epoch9531: 1.906130313873291\n",
            "Test loss for epoch9531: 1.962046504020691\n",
            "Train loss for epoch9532: 2.019265651702881\n",
            "Test loss for epoch9532: 2.310535430908203\n",
            "Train loss for epoch9533: 1.835323691368103\n",
            "Test loss for epoch9533: 1.8922897577285767\n",
            "Train loss for epoch9534: 1.998218059539795\n",
            "Test loss for epoch9534: 2.177968978881836\n",
            "Train loss for epoch9535: 1.9627870321273804\n",
            "Test loss for epoch9535: 2.1262412071228027\n",
            "Train loss for epoch9536: 1.8933595418930054\n",
            "Test loss for epoch9536: 2.28883957862854\n",
            "Train loss for epoch9537: 2.018277883529663\n",
            "Test loss for epoch9537: 2.11979341506958\n",
            "Train loss for epoch9538: 2.0365889072418213\n",
            "Test loss for epoch9538: 1.8842378854751587\n",
            "Train loss for epoch9539: 2.2005231380462646\n",
            "Test loss for epoch9539: 1.9811185598373413\n",
            "Train loss for epoch9540: 2.0543718338012695\n",
            "Test loss for epoch9540: 2.1172544956207275\n",
            "Train loss for epoch9541: 1.9646538496017456\n",
            "Test loss for epoch9541: 2.136889934539795\n",
            "Train loss for epoch9542: 1.9531681537628174\n",
            "Test loss for epoch9542: 1.9988000392913818\n",
            "Train loss for epoch9543: 1.8314632177352905\n",
            "Test loss for epoch9543: 1.907548189163208\n",
            "Train loss for epoch9544: 1.9775367975234985\n",
            "Test loss for epoch9544: 2.037001371383667\n",
            "Train loss for epoch9545: 1.9684925079345703\n",
            "Test loss for epoch9545: 2.051081895828247\n",
            "Train loss for epoch9546: 1.738120675086975\n",
            "Test loss for epoch9546: 2.0307655334472656\n",
            "Train loss for epoch9547: 1.9103738069534302\n",
            "Test loss for epoch9547: 2.162792921066284\n",
            "Train loss for epoch9548: 2.098698854446411\n",
            "Test loss for epoch9548: 2.093395233154297\n",
            "Train loss for epoch9549: 2.051013708114624\n",
            "Test loss for epoch9549: 2.099987268447876\n",
            "Train loss for epoch9550: 1.962270975112915\n",
            "Test loss for epoch9550: 2.0070366859436035\n",
            "Train loss for epoch9551: 1.935062050819397\n",
            "Test loss for epoch9551: 1.9032061100006104\n",
            "Train loss for epoch9552: 1.955084204673767\n",
            "Test loss for epoch9552: 2.2447493076324463\n",
            "Train loss for epoch9553: 1.753130555152893\n",
            "Test loss for epoch9553: 2.2344002723693848\n",
            "Train loss for epoch9554: 2.129518508911133\n",
            "Test loss for epoch9554: 2.0475690364837646\n",
            "Train loss for epoch9555: 2.032834768295288\n",
            "Test loss for epoch9555: 2.179511308670044\n",
            "Train loss for epoch9556: 1.8498682975769043\n",
            "Test loss for epoch9556: 1.988275408744812\n",
            "Train loss for epoch9557: 1.9262492656707764\n",
            "Test loss for epoch9557: 2.0035619735717773\n",
            "Train loss for epoch9558: 1.994540810585022\n",
            "Test loss for epoch9558: 1.9826209545135498\n",
            "Train loss for epoch9559: 1.8815484046936035\n",
            "Test loss for epoch9559: 2.2232065200805664\n",
            "Train loss for epoch9560: 2.043430805206299\n",
            "Test loss for epoch9560: 2.127189874649048\n",
            "Train loss for epoch9561: 2.011955499649048\n",
            "Test loss for epoch9561: 2.0151877403259277\n",
            "Train loss for epoch9562: 1.9609413146972656\n",
            "Test loss for epoch9562: 2.010643482208252\n",
            "Train loss for epoch9563: 1.859665870666504\n",
            "Test loss for epoch9563: 2.1989493370056152\n",
            "Train loss for epoch9564: 1.8536779880523682\n",
            "Test loss for epoch9564: 1.9153988361358643\n",
            "Train loss for epoch9565: 1.9910705089569092\n",
            "Test loss for epoch9565: 1.982614517211914\n",
            "Train loss for epoch9566: 1.9310754537582397\n",
            "Test loss for epoch9566: 1.8514907360076904\n",
            "Train loss for epoch9567: 1.7587783336639404\n",
            "Test loss for epoch9567: 2.209602117538452\n",
            "Train loss for epoch9568: 1.9578607082366943\n",
            "Test loss for epoch9568: 1.9461454153060913\n",
            "Train loss for epoch9569: 1.8297573328018188\n",
            "Test loss for epoch9569: 1.937828779220581\n",
            "Train loss for epoch9570: 1.8145862817764282\n",
            "Test loss for epoch9570: 2.111027717590332\n",
            "Train loss for epoch9571: 1.9642324447631836\n",
            "Test loss for epoch9571: 1.894626498222351\n",
            "Train loss for epoch9572: 1.9113959074020386\n",
            "Test loss for epoch9572: 2.0778822898864746\n",
            "Train loss for epoch9573: 1.9463179111480713\n",
            "Test loss for epoch9573: 2.179502248764038\n",
            "Train loss for epoch9574: 2.0703980922698975\n",
            "Test loss for epoch9574: 2.0951404571533203\n",
            "Train loss for epoch9575: 1.7659578323364258\n",
            "Test loss for epoch9575: 1.9431202411651611\n",
            "Train loss for epoch9576: 1.9241853952407837\n",
            "Test loss for epoch9576: 1.9286595582962036\n",
            "Train loss for epoch9577: 1.7142517566680908\n",
            "Test loss for epoch9577: 2.132089853286743\n",
            "Train loss for epoch9578: 1.8623722791671753\n",
            "Test loss for epoch9578: 2.098579168319702\n",
            "Train loss for epoch9579: 1.9516856670379639\n",
            "Test loss for epoch9579: 2.0095598697662354\n",
            "Train loss for epoch9580: 1.9833061695098877\n",
            "Test loss for epoch9580: 2.0044069290161133\n",
            "Train loss for epoch9581: 1.9704163074493408\n",
            "Test loss for epoch9581: 2.2206525802612305\n",
            "Train loss for epoch9582: 1.9124813079833984\n",
            "Test loss for epoch9582: 2.19500994682312\n",
            "Train loss for epoch9583: 1.673042893409729\n",
            "Test loss for epoch9583: 2.0732059478759766\n",
            "Train loss for epoch9584: 2.013201951980591\n",
            "Test loss for epoch9584: 2.172865152359009\n",
            "Train loss for epoch9585: 1.8951084613800049\n",
            "Test loss for epoch9585: 2.0849759578704834\n",
            "Train loss for epoch9586: 1.8847678899765015\n",
            "Test loss for epoch9586: 2.0248234272003174\n",
            "Train loss for epoch9587: 1.9480576515197754\n",
            "Test loss for epoch9587: 2.2238149642944336\n",
            "Train loss for epoch9588: 1.8154902458190918\n",
            "Test loss for epoch9588: 2.0539960861206055\n",
            "Train loss for epoch9589: 1.9006670713424683\n",
            "Test loss for epoch9589: 1.8375095129013062\n",
            "Train loss for epoch9590: 2.1195127964019775\n",
            "Test loss for epoch9590: 2.078839063644409\n",
            "Train loss for epoch9591: 1.85866379737854\n",
            "Test loss for epoch9591: 2.0438239574432373\n",
            "Train loss for epoch9592: 1.7160844802856445\n",
            "Test loss for epoch9592: 2.2068932056427\n",
            "Train loss for epoch9593: 1.8617769479751587\n",
            "Test loss for epoch9593: 2.01464581489563\n",
            "Train loss for epoch9594: 1.8867300748825073\n",
            "Test loss for epoch9594: 2.0360755920410156\n",
            "Train loss for epoch9595: 1.890689730644226\n",
            "Test loss for epoch9595: 1.8219683170318604\n",
            "Train loss for epoch9596: 2.0435245037078857\n",
            "Test loss for epoch9596: 1.952148675918579\n",
            "Train loss for epoch9597: 2.0521042346954346\n",
            "Test loss for epoch9597: 2.2844338417053223\n",
            "Train loss for epoch9598: 1.9185614585876465\n",
            "Test loss for epoch9598: 2.0664708614349365\n",
            "Train loss for epoch9599: 2.116159439086914\n",
            "Test loss for epoch9599: 2.0249452590942383\n",
            "Train loss for epoch9600: 1.9955317974090576\n",
            "Test loss for epoch9600: 1.9375362396240234\n",
            "Train loss for epoch9601: 2.035038948059082\n",
            "Test loss for epoch9601: 2.095512628555298\n",
            "Train loss for epoch9602: 1.969867467880249\n",
            "Test loss for epoch9602: 2.071688175201416\n",
            "Train loss for epoch9603: 1.8999338150024414\n",
            "Test loss for epoch9603: 1.9619606733322144\n",
            "Train loss for epoch9604: 1.9277698993682861\n",
            "Test loss for epoch9604: 1.9554213285446167\n",
            "Train loss for epoch9605: 2.047095775604248\n",
            "Test loss for epoch9605: 2.0002927780151367\n",
            "Train loss for epoch9606: 1.964387059211731\n",
            "Test loss for epoch9606: 1.7360669374465942\n",
            "Train loss for epoch9607: 2.052150249481201\n",
            "Test loss for epoch9607: 2.0220892429351807\n",
            "Train loss for epoch9608: 2.0460686683654785\n",
            "Test loss for epoch9608: 2.1684763431549072\n",
            "Train loss for epoch9609: 1.9357407093048096\n",
            "Test loss for epoch9609: 2.0846755504608154\n",
            "Train loss for epoch9610: 1.9163305759429932\n",
            "Test loss for epoch9610: 2.1376497745513916\n",
            "Train loss for epoch9611: 1.8851900100708008\n",
            "Test loss for epoch9611: 1.9837818145751953\n",
            "Train loss for epoch9612: 1.8360660076141357\n",
            "Test loss for epoch9612: 1.8917632102966309\n",
            "Train loss for epoch9613: 2.1846134662628174\n",
            "Test loss for epoch9613: 2.0396785736083984\n",
            "Train loss for epoch9614: 2.0796661376953125\n",
            "Test loss for epoch9614: 1.9033626317977905\n",
            "Train loss for epoch9615: 2.04441499710083\n",
            "Test loss for epoch9615: 1.9400771856307983\n",
            "Train loss for epoch9616: 1.8187432289123535\n",
            "Test loss for epoch9616: 2.012319326400757\n",
            "Train loss for epoch9617: 1.953558325767517\n",
            "Test loss for epoch9617: 2.067967176437378\n",
            "Train loss for epoch9618: 1.9833956956863403\n",
            "Test loss for epoch9618: 2.0773050785064697\n",
            "Train loss for epoch9619: 2.005941867828369\n",
            "Test loss for epoch9619: 1.995300531387329\n",
            "Train loss for epoch9620: 1.9802772998809814\n",
            "Test loss for epoch9620: 1.9087532758712769\n",
            "Train loss for epoch9621: 1.9154951572418213\n",
            "Test loss for epoch9621: 1.9243266582489014\n",
            "Train loss for epoch9622: 1.9846948385238647\n",
            "Test loss for epoch9622: 2.0841453075408936\n",
            "Train loss for epoch9623: 1.9275975227355957\n",
            "Test loss for epoch9623: 2.1204800605773926\n",
            "Train loss for epoch9624: 1.923731803894043\n",
            "Test loss for epoch9624: 1.9312987327575684\n",
            "Train loss for epoch9625: 1.7854697704315186\n",
            "Test loss for epoch9625: 1.8993744850158691\n",
            "Train loss for epoch9626: 1.842964768409729\n",
            "Test loss for epoch9626: 1.9565799236297607\n",
            "Train loss for epoch9627: 1.9757046699523926\n",
            "Test loss for epoch9627: 2.0668928623199463\n",
            "Train loss for epoch9628: 1.904578685760498\n",
            "Test loss for epoch9628: 2.1775121688842773\n",
            "Train loss for epoch9629: 1.9415394067764282\n",
            "Test loss for epoch9629: 1.9909619092941284\n",
            "Train loss for epoch9630: 1.6589406728744507\n",
            "Test loss for epoch9630: 2.121674060821533\n",
            "Train loss for epoch9631: 1.825369954109192\n",
            "Test loss for epoch9631: 1.8905541896820068\n",
            "Train loss for epoch9632: 2.0589005947113037\n",
            "Test loss for epoch9632: 2.0089616775512695\n",
            "Train loss for epoch9633: 2.1109490394592285\n",
            "Test loss for epoch9633: 2.039001941680908\n",
            "Train loss for epoch9634: 2.036785364151001\n",
            "Test loss for epoch9634: 2.1491570472717285\n",
            "Train loss for epoch9635: 1.9556154012680054\n",
            "Test loss for epoch9635: 1.9422273635864258\n",
            "Train loss for epoch9636: 1.902543306350708\n",
            "Test loss for epoch9636: 2.0278875827789307\n",
            "Train loss for epoch9637: 1.9244472980499268\n",
            "Test loss for epoch9637: 2.0331687927246094\n",
            "Train loss for epoch9638: 1.754282832145691\n",
            "Test loss for epoch9638: 2.104376792907715\n",
            "Train loss for epoch9639: 1.845463752746582\n",
            "Test loss for epoch9639: 2.03105092048645\n",
            "Train loss for epoch9640: 2.2123353481292725\n",
            "Test loss for epoch9640: 2.0629987716674805\n",
            "Train loss for epoch9641: 2.0219569206237793\n",
            "Test loss for epoch9641: 2.066129207611084\n",
            "Train loss for epoch9642: 2.1570019721984863\n",
            "Test loss for epoch9642: 1.9972409009933472\n",
            "Train loss for epoch9643: 1.822204828262329\n",
            "Test loss for epoch9643: 1.958186149597168\n",
            "Train loss for epoch9644: 1.9655286073684692\n",
            "Test loss for epoch9644: 2.1175167560577393\n",
            "Train loss for epoch9645: 1.9572312831878662\n",
            "Test loss for epoch9645: 1.9629393815994263\n",
            "Train loss for epoch9646: 1.8282594680786133\n",
            "Test loss for epoch9646: 2.008319854736328\n",
            "Train loss for epoch9647: 1.776943325996399\n",
            "Test loss for epoch9647: 2.0986337661743164\n",
            "Train loss for epoch9648: 1.9771119356155396\n",
            "Test loss for epoch9648: 1.9200059175491333\n",
            "Train loss for epoch9649: 1.809516191482544\n",
            "Test loss for epoch9649: 2.0621185302734375\n",
            "Train loss for epoch9650: 1.89976167678833\n",
            "Test loss for epoch9650: 2.015592336654663\n",
            "Train loss for epoch9651: 2.1296374797821045\n",
            "Test loss for epoch9651: 2.267657995223999\n",
            "Train loss for epoch9652: 1.9164162874221802\n",
            "Test loss for epoch9652: 1.9919092655181885\n",
            "Train loss for epoch9653: 2.116323471069336\n",
            "Test loss for epoch9653: 2.204681396484375\n",
            "Train loss for epoch9654: 1.9314748048782349\n",
            "Test loss for epoch9654: 2.1321635246276855\n",
            "Train loss for epoch9655: 1.8591840267181396\n",
            "Test loss for epoch9655: 2.0334794521331787\n",
            "Train loss for epoch9656: 2.127206325531006\n",
            "Test loss for epoch9656: 1.9532653093338013\n",
            "Train loss for epoch9657: 1.8967198133468628\n",
            "Test loss for epoch9657: 2.04436993598938\n",
            "Train loss for epoch9658: 1.7886995077133179\n",
            "Test loss for epoch9658: 2.316793203353882\n",
            "Train loss for epoch9659: 2.0010993480682373\n",
            "Test loss for epoch9659: 2.024981737136841\n",
            "Train loss for epoch9660: 1.9448446035385132\n",
            "Test loss for epoch9660: 1.9042994976043701\n",
            "Train loss for epoch9661: 2.022329807281494\n",
            "Test loss for epoch9661: 2.4120192527770996\n",
            "Train loss for epoch9662: 2.116103410720825\n",
            "Test loss for epoch9662: 2.0831665992736816\n",
            "Train loss for epoch9663: 1.9916999340057373\n",
            "Test loss for epoch9663: 1.9223687648773193\n",
            "Train loss for epoch9664: 1.8520764112472534\n",
            "Test loss for epoch9664: 2.252689838409424\n",
            "Train loss for epoch9665: 2.0239758491516113\n",
            "Test loss for epoch9665: 1.7557483911514282\n",
            "Train loss for epoch9666: 2.0487658977508545\n",
            "Test loss for epoch9666: 1.8363738059997559\n",
            "Train loss for epoch9667: 2.2241034507751465\n",
            "Test loss for epoch9667: 2.1873300075531006\n",
            "Train loss for epoch9668: 1.9974251985549927\n",
            "Test loss for epoch9668: 1.9182713031768799\n",
            "Train loss for epoch9669: 2.0139594078063965\n",
            "Test loss for epoch9669: 1.93658447265625\n",
            "Train loss for epoch9670: 2.030792713165283\n",
            "Test loss for epoch9670: 2.220655679702759\n",
            "Train loss for epoch9671: 1.6987935304641724\n",
            "Test loss for epoch9671: 2.021310567855835\n",
            "Train loss for epoch9672: 1.8705670833587646\n",
            "Test loss for epoch9672: 2.119410276412964\n",
            "Train loss for epoch9673: 2.1625893115997314\n",
            "Test loss for epoch9673: 1.9841252565383911\n",
            "Train loss for epoch9674: 2.0115199089050293\n",
            "Test loss for epoch9674: 1.932997703552246\n",
            "Train loss for epoch9675: 1.8011152744293213\n",
            "Test loss for epoch9675: 2.1547608375549316\n",
            "Train loss for epoch9676: 1.8824061155319214\n",
            "Test loss for epoch9676: 1.9951390027999878\n",
            "Train loss for epoch9677: 1.9644172191619873\n",
            "Test loss for epoch9677: 1.906045913696289\n",
            "Train loss for epoch9678: 1.828980565071106\n",
            "Test loss for epoch9678: 2.2688677310943604\n",
            "Train loss for epoch9679: 1.9437618255615234\n",
            "Test loss for epoch9679: 2.032705545425415\n",
            "Train loss for epoch9680: 1.8017618656158447\n",
            "Test loss for epoch9680: 2.203866720199585\n",
            "Train loss for epoch9681: 1.9073086977005005\n",
            "Test loss for epoch9681: 2.1505398750305176\n",
            "Train loss for epoch9682: 1.978635549545288\n",
            "Test loss for epoch9682: 1.9644407033920288\n",
            "Train loss for epoch9683: 1.9889785051345825\n",
            "Test loss for epoch9683: 2.019127130508423\n",
            "Train loss for epoch9684: 1.8597580194473267\n",
            "Test loss for epoch9684: 2.0026021003723145\n",
            "Train loss for epoch9685: 2.0183968544006348\n",
            "Test loss for epoch9685: 1.862218976020813\n",
            "Train loss for epoch9686: 2.191959857940674\n",
            "Test loss for epoch9686: 2.0785892009735107\n",
            "Train loss for epoch9687: 2.018254518508911\n",
            "Test loss for epoch9687: 2.094647169113159\n",
            "Train loss for epoch9688: 1.9750027656555176\n",
            "Test loss for epoch9688: 1.9965269565582275\n",
            "Train loss for epoch9689: 2.2973337173461914\n",
            "Test loss for epoch9689: 2.0185017585754395\n",
            "Train loss for epoch9690: 1.9127000570297241\n",
            "Test loss for epoch9690: 1.9617036581039429\n",
            "Train loss for epoch9691: 1.983370304107666\n",
            "Test loss for epoch9691: 2.1078040599823\n",
            "Train loss for epoch9692: 1.9902936220169067\n",
            "Test loss for epoch9692: 1.9489128589630127\n",
            "Train loss for epoch9693: 2.0121426582336426\n",
            "Test loss for epoch9693: 2.178603172302246\n",
            "Train loss for epoch9694: 1.9146745204925537\n",
            "Test loss for epoch9694: 2.1365089416503906\n",
            "Train loss for epoch9695: 1.8832261562347412\n",
            "Test loss for epoch9695: 1.9976623058319092\n",
            "Train loss for epoch9696: 1.9461381435394287\n",
            "Test loss for epoch9696: 2.158167839050293\n",
            "Train loss for epoch9697: 1.815860390663147\n",
            "Test loss for epoch9697: 2.024369955062866\n",
            "Train loss for epoch9698: 1.907431721687317\n",
            "Test loss for epoch9698: 2.0443594455718994\n",
            "Train loss for epoch9699: 1.871638298034668\n",
            "Test loss for epoch9699: 1.9695830345153809\n",
            "Train loss for epoch9700: 2.0916330814361572\n",
            "Test loss for epoch9700: 2.246751308441162\n",
            "Train loss for epoch9701: 1.9836474657058716\n",
            "Test loss for epoch9701: 2.024179697036743\n",
            "Train loss for epoch9702: 1.9402973651885986\n",
            "Test loss for epoch9702: 2.072270393371582\n",
            "Train loss for epoch9703: 2.024949789047241\n",
            "Test loss for epoch9703: 2.2162013053894043\n",
            "Train loss for epoch9704: 1.8088977336883545\n",
            "Test loss for epoch9704: 2.003235101699829\n",
            "Train loss for epoch9705: 2.1668622493743896\n",
            "Test loss for epoch9705: 2.1476099491119385\n",
            "Train loss for epoch9706: 2.035874366760254\n",
            "Test loss for epoch9706: 1.857625961303711\n",
            "Train loss for epoch9707: 2.0562469959259033\n",
            "Test loss for epoch9707: 2.1394238471984863\n",
            "Train loss for epoch9708: 2.0337748527526855\n",
            "Test loss for epoch9708: 2.1742641925811768\n",
            "Train loss for epoch9709: 1.8843363523483276\n",
            "Test loss for epoch9709: 2.0676374435424805\n",
            "Train loss for epoch9710: 2.2749977111816406\n",
            "Test loss for epoch9710: 2.1238551139831543\n",
            "Train loss for epoch9711: 1.9849270582199097\n",
            "Test loss for epoch9711: 2.130890369415283\n",
            "Train loss for epoch9712: 2.014789581298828\n",
            "Test loss for epoch9712: 1.9998509883880615\n",
            "Train loss for epoch9713: 1.9911904335021973\n",
            "Test loss for epoch9713: 1.859542727470398\n",
            "Train loss for epoch9714: 1.9554378986358643\n",
            "Test loss for epoch9714: 1.920785665512085\n",
            "Train loss for epoch9715: 2.094332456588745\n",
            "Test loss for epoch9715: 2.0829756259918213\n",
            "Train loss for epoch9716: 1.7638155221939087\n",
            "Test loss for epoch9716: 2.254371404647827\n",
            "Train loss for epoch9717: 2.02397084236145\n",
            "Test loss for epoch9717: 2.179751396179199\n",
            "Train loss for epoch9718: 1.9821628332138062\n",
            "Test loss for epoch9718: 1.914540410041809\n",
            "Train loss for epoch9719: 1.8941659927368164\n",
            "Test loss for epoch9719: 1.9947874546051025\n",
            "Train loss for epoch9720: 2.0991930961608887\n",
            "Test loss for epoch9720: 1.9071235656738281\n",
            "Train loss for epoch9721: 1.9337435960769653\n",
            "Test loss for epoch9721: 2.1691391468048096\n",
            "Train loss for epoch9722: 2.0248382091522217\n",
            "Test loss for epoch9722: 1.9238369464874268\n",
            "Train loss for epoch9723: 1.7612910270690918\n",
            "Test loss for epoch9723: 2.0536208152770996\n",
            "Train loss for epoch9724: 2.007526159286499\n",
            "Test loss for epoch9724: 1.964526653289795\n",
            "Train loss for epoch9725: 1.867082953453064\n",
            "Test loss for epoch9725: 2.0890448093414307\n",
            "Train loss for epoch9726: 1.985619068145752\n",
            "Test loss for epoch9726: 1.8085153102874756\n",
            "Train loss for epoch9727: 1.9964652061462402\n",
            "Test loss for epoch9727: 2.2245612144470215\n",
            "Train loss for epoch9728: 1.9164788722991943\n",
            "Test loss for epoch9728: 1.9138565063476562\n",
            "Train loss for epoch9729: 2.050535202026367\n",
            "Test loss for epoch9729: 2.0164411067962646\n",
            "Train loss for epoch9730: 1.9602500200271606\n",
            "Test loss for epoch9730: 2.110091209411621\n",
            "Train loss for epoch9731: 2.002077579498291\n",
            "Test loss for epoch9731: 1.977055549621582\n",
            "Train loss for epoch9732: 1.8002803325653076\n",
            "Test loss for epoch9732: 2.168684482574463\n",
            "Train loss for epoch9733: 1.9253126382827759\n",
            "Test loss for epoch9733: 2.139615774154663\n",
            "Train loss for epoch9734: 1.7848231792449951\n",
            "Test loss for epoch9734: 2.0036630630493164\n",
            "Train loss for epoch9735: 1.9955394268035889\n",
            "Test loss for epoch9735: 2.119368076324463\n",
            "Train loss for epoch9736: 1.8675987720489502\n",
            "Test loss for epoch9736: 2.1057045459747314\n",
            "Train loss for epoch9737: 1.9244463443756104\n",
            "Test loss for epoch9737: 2.297273635864258\n",
            "Train loss for epoch9738: 1.9847337007522583\n",
            "Test loss for epoch9738: 1.9226124286651611\n",
            "Train loss for epoch9739: 2.0638620853424072\n",
            "Test loss for epoch9739: 2.1375105381011963\n",
            "Train loss for epoch9740: 1.9191752672195435\n",
            "Test loss for epoch9740: 2.288853406906128\n",
            "Train loss for epoch9741: 1.9868038892745972\n",
            "Test loss for epoch9741: 2.100834846496582\n",
            "Train loss for epoch9742: 1.9386320114135742\n",
            "Test loss for epoch9742: 2.2371644973754883\n",
            "Train loss for epoch9743: 2.203216552734375\n",
            "Test loss for epoch9743: 2.191096305847168\n",
            "Train loss for epoch9744: 1.8030421733856201\n",
            "Test loss for epoch9744: 1.94904625415802\n",
            "Train loss for epoch9745: 1.7990528345108032\n",
            "Test loss for epoch9745: 2.0203683376312256\n",
            "Train loss for epoch9746: 1.8843138217926025\n",
            "Test loss for epoch9746: 2.1084632873535156\n",
            "Train loss for epoch9747: 1.8884097337722778\n",
            "Test loss for epoch9747: 1.9761296510696411\n",
            "Train loss for epoch9748: 1.7842581272125244\n",
            "Test loss for epoch9748: 2.080989360809326\n",
            "Train loss for epoch9749: 2.024080753326416\n",
            "Test loss for epoch9749: 2.056800603866577\n",
            "Train loss for epoch9750: 2.0144994258880615\n",
            "Test loss for epoch9750: 2.100863456726074\n",
            "Train loss for epoch9751: 1.873430848121643\n",
            "Test loss for epoch9751: 2.294023275375366\n",
            "Train loss for epoch9752: 1.8532533645629883\n",
            "Test loss for epoch9752: 2.0801584720611572\n",
            "Train loss for epoch9753: 1.7136986255645752\n",
            "Test loss for epoch9753: 1.9542688131332397\n",
            "Train loss for epoch9754: 2.2116527557373047\n",
            "Test loss for epoch9754: 2.0160751342773438\n",
            "Train loss for epoch9755: 1.79830002784729\n",
            "Test loss for epoch9755: 2.045713424682617\n",
            "Train loss for epoch9756: 2.0582058429718018\n",
            "Test loss for epoch9756: 2.042156934738159\n",
            "Train loss for epoch9757: 1.7691718339920044\n",
            "Test loss for epoch9757: 1.9778332710266113\n",
            "Train loss for epoch9758: 1.9358861446380615\n",
            "Test loss for epoch9758: 2.4368367195129395\n",
            "Train loss for epoch9759: 1.7938288450241089\n",
            "Test loss for epoch9759: 2.0186359882354736\n",
            "Train loss for epoch9760: 1.9027198553085327\n",
            "Test loss for epoch9760: 2.1436870098114014\n",
            "Train loss for epoch9761: 2.1220972537994385\n",
            "Test loss for epoch9761: 2.047487497329712\n",
            "Train loss for epoch9762: 1.8045121431350708\n",
            "Test loss for epoch9762: 1.9525701999664307\n",
            "Train loss for epoch9763: 2.048529863357544\n",
            "Test loss for epoch9763: 1.9994583129882812\n",
            "Train loss for epoch9764: 1.9034701585769653\n",
            "Test loss for epoch9764: 2.0781123638153076\n",
            "Train loss for epoch9765: 1.9338675737380981\n",
            "Test loss for epoch9765: 2.0940945148468018\n",
            "Train loss for epoch9766: 1.824711561203003\n",
            "Test loss for epoch9766: 2.037311553955078\n",
            "Train loss for epoch9767: 1.8456480503082275\n",
            "Test loss for epoch9767: 1.943880319595337\n",
            "Train loss for epoch9768: 1.880829095840454\n",
            "Test loss for epoch9768: 1.9542052745819092\n",
            "Train loss for epoch9769: 2.1111626625061035\n",
            "Test loss for epoch9769: 2.1339704990386963\n",
            "Train loss for epoch9770: 2.001049518585205\n",
            "Test loss for epoch9770: 2.087648391723633\n",
            "Train loss for epoch9771: 1.7670344114303589\n",
            "Test loss for epoch9771: 1.8833706378936768\n",
            "Train loss for epoch9772: 1.993282437324524\n",
            "Test loss for epoch9772: 2.0652620792388916\n",
            "Train loss for epoch9773: 2.0173215866088867\n",
            "Test loss for epoch9773: 1.951462984085083\n",
            "Train loss for epoch9774: 1.9818804264068604\n",
            "Test loss for epoch9774: 2.1933817863464355\n",
            "Train loss for epoch9775: 2.0130367279052734\n",
            "Test loss for epoch9775: 2.043572425842285\n",
            "Train loss for epoch9776: 1.8228827714920044\n",
            "Test loss for epoch9776: 1.9807541370391846\n",
            "Train loss for epoch9777: 1.9775758981704712\n",
            "Test loss for epoch9777: 1.8166476488113403\n",
            "Train loss for epoch9778: 2.079397201538086\n",
            "Test loss for epoch9778: 1.8989845514297485\n",
            "Train loss for epoch9779: 1.8208996057510376\n",
            "Test loss for epoch9779: 2.1146724224090576\n",
            "Train loss for epoch9780: 1.8405311107635498\n",
            "Test loss for epoch9780: 2.1178016662597656\n",
            "Train loss for epoch9781: 2.1469225883483887\n",
            "Test loss for epoch9781: 2.055999279022217\n",
            "Train loss for epoch9782: 1.6191798448562622\n",
            "Test loss for epoch9782: 2.21071195602417\n",
            "Train loss for epoch9783: 1.9547319412231445\n",
            "Test loss for epoch9783: 2.022115468978882\n",
            "Train loss for epoch9784: 1.7674837112426758\n",
            "Test loss for epoch9784: 2.1770598888397217\n",
            "Train loss for epoch9785: 1.9975306987762451\n",
            "Test loss for epoch9785: 2.1087582111358643\n",
            "Train loss for epoch9786: 1.8924684524536133\n",
            "Test loss for epoch9786: 2.070889949798584\n",
            "Train loss for epoch9787: 1.8820689916610718\n",
            "Test loss for epoch9787: 2.0323846340179443\n",
            "Train loss for epoch9788: 1.9222363233566284\n",
            "Test loss for epoch9788: 2.0477564334869385\n",
            "Train loss for epoch9789: 1.8272629976272583\n",
            "Test loss for epoch9789: 2.1148178577423096\n",
            "Train loss for epoch9790: 1.8396239280700684\n",
            "Test loss for epoch9790: 2.2235074043273926\n",
            "Train loss for epoch9791: 1.8689435720443726\n",
            "Test loss for epoch9791: 2.2236194610595703\n",
            "Train loss for epoch9792: 1.9453964233398438\n",
            "Test loss for epoch9792: 1.9385437965393066\n",
            "Train loss for epoch9793: 2.0835249423980713\n",
            "Test loss for epoch9793: 1.9612281322479248\n",
            "Train loss for epoch9794: 1.9522026777267456\n",
            "Test loss for epoch9794: 2.0919618606567383\n",
            "Train loss for epoch9795: 1.8256314992904663\n",
            "Test loss for epoch9795: 2.1017181873321533\n",
            "Train loss for epoch9796: 2.0175716876983643\n",
            "Test loss for epoch9796: 1.9451355934143066\n",
            "Train loss for epoch9797: 1.8445504903793335\n",
            "Test loss for epoch9797: 2.1934025287628174\n",
            "Train loss for epoch9798: 1.8573548793792725\n",
            "Test loss for epoch9798: 2.0278897285461426\n",
            "Train loss for epoch9799: 2.019447088241577\n",
            "Test loss for epoch9799: 2.0068981647491455\n",
            "Train loss for epoch9800: 2.2323760986328125\n",
            "Test loss for epoch9800: 1.8914567232131958\n",
            "Train loss for epoch9801: 2.0306894779205322\n",
            "Test loss for epoch9801: 2.009640693664551\n",
            "Train loss for epoch9802: 1.9371006488800049\n",
            "Test loss for epoch9802: 2.3068222999572754\n",
            "Train loss for epoch9803: 1.9235889911651611\n",
            "Test loss for epoch9803: 2.394382953643799\n",
            "Train loss for epoch9804: 2.0014760494232178\n",
            "Test loss for epoch9804: 1.9921672344207764\n",
            "Train loss for epoch9805: 1.8263977766036987\n",
            "Test loss for epoch9805: 1.9193366765975952\n",
            "Train loss for epoch9806: 1.9100819826126099\n",
            "Test loss for epoch9806: 2.15666127204895\n",
            "Train loss for epoch9807: 1.923028826713562\n",
            "Test loss for epoch9807: 2.017947196960449\n",
            "Train loss for epoch9808: 1.9042398929595947\n",
            "Test loss for epoch9808: 2.191537857055664\n",
            "Train loss for epoch9809: 2.0066370964050293\n",
            "Test loss for epoch9809: 1.9710934162139893\n",
            "Train loss for epoch9810: 2.1860339641571045\n",
            "Test loss for epoch9810: 2.026827812194824\n",
            "Train loss for epoch9811: 1.8238003253936768\n",
            "Test loss for epoch9811: 1.9153810739517212\n",
            "Train loss for epoch9812: 1.9931645393371582\n",
            "Test loss for epoch9812: 1.8678297996520996\n",
            "Train loss for epoch9813: 1.9087096452713013\n",
            "Test loss for epoch9813: 1.8417222499847412\n",
            "Train loss for epoch9814: 1.7736045122146606\n",
            "Test loss for epoch9814: 2.0505523681640625\n",
            "Train loss for epoch9815: 1.83302903175354\n",
            "Test loss for epoch9815: 2.020775318145752\n",
            "Train loss for epoch9816: 2.062354803085327\n",
            "Test loss for epoch9816: 2.050266742706299\n",
            "Train loss for epoch9817: 1.935412883758545\n",
            "Test loss for epoch9817: 1.877795696258545\n",
            "Train loss for epoch9818: 1.8482203483581543\n",
            "Test loss for epoch9818: 2.164360523223877\n",
            "Train loss for epoch9819: 1.9785234928131104\n",
            "Test loss for epoch9819: 2.1205501556396484\n",
            "Train loss for epoch9820: 1.8730355501174927\n",
            "Test loss for epoch9820: 2.0483083724975586\n",
            "Train loss for epoch9821: 2.125343084335327\n",
            "Test loss for epoch9821: 2.114053964614868\n",
            "Train loss for epoch9822: 1.88077712059021\n",
            "Test loss for epoch9822: 1.9827977418899536\n",
            "Train loss for epoch9823: 1.9122319221496582\n",
            "Test loss for epoch9823: 2.2094388008117676\n",
            "Train loss for epoch9824: 1.811496376991272\n",
            "Test loss for epoch9824: 2.139294147491455\n",
            "Train loss for epoch9825: 2.007197618484497\n",
            "Test loss for epoch9825: 2.083470106124878\n",
            "Train loss for epoch9826: 1.9651005268096924\n",
            "Test loss for epoch9826: 1.9389311075210571\n",
            "Train loss for epoch9827: 1.8913885354995728\n",
            "Test loss for epoch9827: 2.1072161197662354\n",
            "Train loss for epoch9828: 2.0167856216430664\n",
            "Test loss for epoch9828: 2.1887059211730957\n",
            "Train loss for epoch9829: 2.0364110469818115\n",
            "Test loss for epoch9829: 2.1562376022338867\n",
            "Train loss for epoch9830: 2.0712594985961914\n",
            "Test loss for epoch9830: 2.126847743988037\n",
            "Train loss for epoch9831: 1.9913502931594849\n",
            "Test loss for epoch9831: 1.858188271522522\n",
            "Train loss for epoch9832: 1.9186381101608276\n",
            "Test loss for epoch9832: 2.20467472076416\n",
            "Train loss for epoch9833: 2.1789298057556152\n",
            "Test loss for epoch9833: 2.0577399730682373\n",
            "Train loss for epoch9834: 1.9590442180633545\n",
            "Test loss for epoch9834: 2.106720209121704\n",
            "Train loss for epoch9835: 2.2276463508605957\n",
            "Test loss for epoch9835: 2.184873342514038\n",
            "Train loss for epoch9836: 1.656141996383667\n",
            "Test loss for epoch9836: 2.14152193069458\n",
            "Train loss for epoch9837: 2.215329170227051\n",
            "Test loss for epoch9837: 2.0991039276123047\n",
            "Train loss for epoch9838: 2.088305711746216\n",
            "Test loss for epoch9838: 1.980578899383545\n",
            "Train loss for epoch9839: 2.0433406829833984\n",
            "Test loss for epoch9839: 2.0464487075805664\n",
            "Train loss for epoch9840: 1.8408229351043701\n",
            "Test loss for epoch9840: 1.9402918815612793\n",
            "Train loss for epoch9841: 1.7109425067901611\n",
            "Test loss for epoch9841: 2.0506560802459717\n",
            "Train loss for epoch9842: 2.2174952030181885\n",
            "Test loss for epoch9842: 2.258086919784546\n",
            "Train loss for epoch9843: 2.0078301429748535\n",
            "Test loss for epoch9843: 2.060403347015381\n",
            "Train loss for epoch9844: 2.073291301727295\n",
            "Test loss for epoch9844: 2.0942606925964355\n",
            "Train loss for epoch9845: 2.2868285179138184\n",
            "Test loss for epoch9845: 1.9960432052612305\n",
            "Train loss for epoch9846: 1.9220991134643555\n",
            "Test loss for epoch9846: 2.194051504135132\n",
            "Train loss for epoch9847: 1.8117952346801758\n",
            "Test loss for epoch9847: 2.049626350402832\n",
            "Train loss for epoch9848: 2.029219150543213\n",
            "Test loss for epoch9848: 2.136207103729248\n",
            "Train loss for epoch9849: 1.7763200998306274\n",
            "Test loss for epoch9849: 2.0078063011169434\n",
            "Train loss for epoch9850: 1.9358049631118774\n",
            "Test loss for epoch9850: 2.0799038410186768\n",
            "Train loss for epoch9851: 1.9374366998672485\n",
            "Test loss for epoch9851: 1.9263180494308472\n",
            "Train loss for epoch9852: 1.8123873472213745\n",
            "Test loss for epoch9852: 1.7830448150634766\n",
            "Train loss for epoch9853: 2.06951904296875\n",
            "Test loss for epoch9853: 2.0366878509521484\n",
            "Train loss for epoch9854: 2.0603697299957275\n",
            "Test loss for epoch9854: 2.2783336639404297\n",
            "Train loss for epoch9855: 1.8845267295837402\n",
            "Test loss for epoch9855: 1.9597128629684448\n",
            "Train loss for epoch9856: 1.9135841131210327\n",
            "Test loss for epoch9856: 2.024958610534668\n",
            "Train loss for epoch9857: 1.9241691827774048\n",
            "Test loss for epoch9857: 2.065703868865967\n",
            "Train loss for epoch9858: 1.7708933353424072\n",
            "Test loss for epoch9858: 2.1070730686187744\n",
            "Train loss for epoch9859: 1.8871817588806152\n",
            "Test loss for epoch9859: 2.2600739002227783\n",
            "Train loss for epoch9860: 1.7984540462493896\n",
            "Test loss for epoch9860: 1.9279814958572388\n",
            "Train loss for epoch9861: 1.8235511779785156\n",
            "Test loss for epoch9861: 2.058990478515625\n",
            "Train loss for epoch9862: 1.8005590438842773\n",
            "Test loss for epoch9862: 2.07222318649292\n",
            "Train loss for epoch9863: 2.075965404510498\n",
            "Test loss for epoch9863: 2.1534383296966553\n",
            "Train loss for epoch9864: 1.9839308261871338\n",
            "Test loss for epoch9864: 2.1498312950134277\n",
            "Train loss for epoch9865: 2.0276999473571777\n",
            "Test loss for epoch9865: 1.8235163688659668\n",
            "Train loss for epoch9866: 1.9017839431762695\n",
            "Test loss for epoch9866: 1.9358452558517456\n",
            "Train loss for epoch9867: 2.2433180809020996\n",
            "Test loss for epoch9867: 2.2468578815460205\n",
            "Train loss for epoch9868: 1.933565616607666\n",
            "Test loss for epoch9868: 2.0401759147644043\n",
            "Train loss for epoch9869: 2.006889820098877\n",
            "Test loss for epoch9869: 2.216217517852783\n",
            "Train loss for epoch9870: 1.8763821125030518\n",
            "Test loss for epoch9870: 1.9671210050582886\n",
            "Train loss for epoch9871: 2.0575690269470215\n",
            "Test loss for epoch9871: 2.3153185844421387\n",
            "Train loss for epoch9872: 2.009671211242676\n",
            "Test loss for epoch9872: 1.8826144933700562\n",
            "Train loss for epoch9873: 1.986106038093567\n",
            "Test loss for epoch9873: 2.1849451065063477\n",
            "Train loss for epoch9874: 1.8773635625839233\n",
            "Test loss for epoch9874: 2.03389310836792\n",
            "Train loss for epoch9875: 1.5920952558517456\n",
            "Test loss for epoch9875: 1.926106572151184\n",
            "Train loss for epoch9876: 1.9574614763259888\n",
            "Test loss for epoch9876: 2.0329718589782715\n",
            "Train loss for epoch9877: 1.7929972410202026\n",
            "Test loss for epoch9877: 1.9691122770309448\n",
            "Train loss for epoch9878: 2.1898210048675537\n",
            "Test loss for epoch9878: 1.9111740589141846\n",
            "Train loss for epoch9879: 2.0707619190216064\n",
            "Test loss for epoch9879: 2.078681468963623\n",
            "Train loss for epoch9880: 1.9844632148742676\n",
            "Test loss for epoch9880: 2.222208023071289\n",
            "Train loss for epoch9881: 1.763203501701355\n",
            "Test loss for epoch9881: 1.8356997966766357\n",
            "Train loss for epoch9882: 2.030320882797241\n",
            "Test loss for epoch9882: 2.033076286315918\n",
            "Train loss for epoch9883: 2.174056053161621\n",
            "Test loss for epoch9883: 2.077399730682373\n",
            "Train loss for epoch9884: 2.0410263538360596\n",
            "Test loss for epoch9884: 2.0128302574157715\n",
            "Train loss for epoch9885: 1.8403098583221436\n",
            "Test loss for epoch9885: 2.1368346214294434\n",
            "Train loss for epoch9886: 1.9849759340286255\n",
            "Test loss for epoch9886: 2.0756657123565674\n",
            "Train loss for epoch9887: 2.0231106281280518\n",
            "Test loss for epoch9887: 2.043325424194336\n",
            "Train loss for epoch9888: 1.9706823825836182\n",
            "Test loss for epoch9888: 2.0262973308563232\n",
            "Train loss for epoch9889: 2.2981178760528564\n",
            "Test loss for epoch9889: 2.0691583156585693\n",
            "Train loss for epoch9890: 1.8651834726333618\n",
            "Test loss for epoch9890: 2.0120770931243896\n",
            "Train loss for epoch9891: 2.092473268508911\n",
            "Test loss for epoch9891: 1.8703123331069946\n",
            "Train loss for epoch9892: 2.0380067825317383\n",
            "Test loss for epoch9892: 2.045745611190796\n",
            "Train loss for epoch9893: 1.8958182334899902\n",
            "Test loss for epoch9893: 1.9311108589172363\n",
            "Train loss for epoch9894: 2.0034570693969727\n",
            "Test loss for epoch9894: 2.107882022857666\n",
            "Train loss for epoch9895: 1.9459244012832642\n",
            "Test loss for epoch9895: 1.9922966957092285\n",
            "Train loss for epoch9896: 2.061140537261963\n",
            "Test loss for epoch9896: 2.2420711517333984\n",
            "Train loss for epoch9897: 1.906308889389038\n",
            "Test loss for epoch9897: 2.2041656970977783\n",
            "Train loss for epoch9898: 1.8257008790969849\n",
            "Test loss for epoch9898: 2.2777512073516846\n",
            "Train loss for epoch9899: 1.7579655647277832\n",
            "Test loss for epoch9899: 2.1664347648620605\n",
            "Train loss for epoch9900: 2.0956685543060303\n",
            "Test loss for epoch9900: 2.113072156906128\n",
            "Train loss for epoch9901: 1.7938225269317627\n",
            "Test loss for epoch9901: 2.0276710987091064\n",
            "Train loss for epoch9902: 1.9301635026931763\n",
            "Test loss for epoch9902: 2.1883327960968018\n",
            "Train loss for epoch9903: 1.660329818725586\n",
            "Test loss for epoch9903: 1.954904317855835\n",
            "Train loss for epoch9904: 1.9772366285324097\n",
            "Test loss for epoch9904: 2.1182587146759033\n",
            "Train loss for epoch9905: 1.9435752630233765\n",
            "Test loss for epoch9905: 2.0372331142425537\n",
            "Train loss for epoch9906: 1.9672282934188843\n",
            "Test loss for epoch9906: 2.102609872817993\n",
            "Train loss for epoch9907: 1.9637318849563599\n",
            "Test loss for epoch9907: 1.8999717235565186\n",
            "Train loss for epoch9908: 1.8394207954406738\n",
            "Test loss for epoch9908: 1.940380334854126\n",
            "Train loss for epoch9909: 2.0870273113250732\n",
            "Test loss for epoch9909: 2.07871150970459\n",
            "Train loss for epoch9910: 1.9629660844802856\n",
            "Test loss for epoch9910: 2.1786563396453857\n",
            "Train loss for epoch9911: 2.0601844787597656\n",
            "Test loss for epoch9911: 1.994186282157898\n",
            "Train loss for epoch9912: 2.008415460586548\n",
            "Test loss for epoch9912: 1.9642627239227295\n",
            "Train loss for epoch9913: 1.9514659643173218\n",
            "Test loss for epoch9913: 2.1028873920440674\n",
            "Train loss for epoch9914: 1.9560242891311646\n",
            "Test loss for epoch9914: 2.0844385623931885\n",
            "Train loss for epoch9915: 1.962883472442627\n",
            "Test loss for epoch9915: 2.0487987995147705\n",
            "Train loss for epoch9916: 1.9656751155853271\n",
            "Test loss for epoch9916: 2.0833332538604736\n",
            "Train loss for epoch9917: 1.874610185623169\n",
            "Test loss for epoch9917: 2.1215672492980957\n",
            "Train loss for epoch9918: 1.9802675247192383\n",
            "Test loss for epoch9918: 1.9985324144363403\n",
            "Train loss for epoch9919: 2.088491916656494\n",
            "Test loss for epoch9919: 2.1515631675720215\n",
            "Train loss for epoch9920: 1.9215879440307617\n",
            "Test loss for epoch9920: 1.8974268436431885\n",
            "Train loss for epoch9921: 1.952427625656128\n",
            "Test loss for epoch9921: 2.0023252964019775\n",
            "Train loss for epoch9922: 1.703202724456787\n",
            "Test loss for epoch9922: 2.1931331157684326\n",
            "Train loss for epoch9923: 1.9002773761749268\n",
            "Test loss for epoch9923: 2.1774520874023438\n",
            "Train loss for epoch9924: 1.944999098777771\n",
            "Test loss for epoch9924: 2.0582916736602783\n",
            "Train loss for epoch9925: 1.802800178527832\n",
            "Test loss for epoch9925: 2.032320261001587\n",
            "Train loss for epoch9926: 1.828031301498413\n",
            "Test loss for epoch9926: 1.9798001050949097\n",
            "Train loss for epoch9927: 2.2988429069519043\n",
            "Test loss for epoch9927: 2.101083993911743\n",
            "Train loss for epoch9928: 1.8124685287475586\n",
            "Test loss for epoch9928: 1.97816801071167\n",
            "Train loss for epoch9929: 1.9805362224578857\n",
            "Test loss for epoch9929: 2.0404958724975586\n",
            "Train loss for epoch9930: 1.911921739578247\n",
            "Test loss for epoch9930: 1.7665475606918335\n",
            "Train loss for epoch9931: 2.0317530632019043\n",
            "Test loss for epoch9931: 1.9995218515396118\n",
            "Train loss for epoch9932: 2.080307722091675\n",
            "Test loss for epoch9932: 1.951249361038208\n",
            "Train loss for epoch9933: 1.9875644445419312\n",
            "Test loss for epoch9933: 1.9095805883407593\n",
            "Train loss for epoch9934: 1.8388557434082031\n",
            "Test loss for epoch9934: 2.1615071296691895\n",
            "Train loss for epoch9935: 1.848610520362854\n",
            "Test loss for epoch9935: 1.965434193611145\n",
            "Train loss for epoch9936: 1.910231113433838\n",
            "Test loss for epoch9936: 2.0004546642303467\n",
            "Train loss for epoch9937: 2.031597137451172\n",
            "Test loss for epoch9937: 1.9634642601013184\n",
            "Train loss for epoch9938: 1.811485767364502\n",
            "Test loss for epoch9938: 2.0107312202453613\n",
            "Train loss for epoch9939: 1.7325228452682495\n",
            "Test loss for epoch9939: 2.229712963104248\n",
            "Train loss for epoch9940: 1.779658555984497\n",
            "Test loss for epoch9940: 2.1374309062957764\n",
            "Train loss for epoch9941: 2.054025411605835\n",
            "Test loss for epoch9941: 2.2454464435577393\n",
            "Train loss for epoch9942: 2.0400874614715576\n",
            "Test loss for epoch9942: 2.1636533737182617\n",
            "Train loss for epoch9943: 1.902787685394287\n",
            "Test loss for epoch9943: 2.017770528793335\n",
            "Train loss for epoch9944: 1.7376357316970825\n",
            "Test loss for epoch9944: 1.8736882209777832\n",
            "Train loss for epoch9945: 1.8294445276260376\n",
            "Test loss for epoch9945: 2.166388750076294\n",
            "Train loss for epoch9946: 1.863724708557129\n",
            "Test loss for epoch9946: 2.1563291549682617\n",
            "Train loss for epoch9947: 2.1226980686187744\n",
            "Test loss for epoch9947: 2.060986042022705\n",
            "Train loss for epoch9948: 1.929237723350525\n",
            "Test loss for epoch9948: 1.9653810262680054\n",
            "Train loss for epoch9949: 2.113429546356201\n",
            "Test loss for epoch9949: 2.067328929901123\n",
            "Train loss for epoch9950: 1.9734686613082886\n",
            "Test loss for epoch9950: 2.1027090549468994\n",
            "Train loss for epoch9951: 2.072422981262207\n",
            "Test loss for epoch9951: 2.0709140300750732\n",
            "Train loss for epoch9952: 1.9427229166030884\n",
            "Test loss for epoch9952: 2.1459712982177734\n",
            "Train loss for epoch9953: 2.0274691581726074\n",
            "Test loss for epoch9953: 1.9705252647399902\n",
            "Train loss for epoch9954: 2.1205198764801025\n",
            "Test loss for epoch9954: 2.0490753650665283\n",
            "Train loss for epoch9955: 1.9707456827163696\n",
            "Test loss for epoch9955: 2.0093166828155518\n",
            "Train loss for epoch9956: 1.9384803771972656\n",
            "Test loss for epoch9956: 1.9076281785964966\n",
            "Train loss for epoch9957: 1.959572434425354\n",
            "Test loss for epoch9957: 2.2533628940582275\n",
            "Train loss for epoch9958: 1.9578020572662354\n",
            "Test loss for epoch9958: 1.9992963075637817\n",
            "Train loss for epoch9959: 1.90281081199646\n",
            "Test loss for epoch9959: 2.155482292175293\n",
            "Train loss for epoch9960: 1.8410050868988037\n",
            "Test loss for epoch9960: 1.8253618478775024\n",
            "Train loss for epoch9961: 2.0258853435516357\n",
            "Test loss for epoch9961: 2.1426870822906494\n",
            "Train loss for epoch9962: 1.7668612003326416\n",
            "Test loss for epoch9962: 1.8852790594100952\n",
            "Train loss for epoch9963: 2.1616921424865723\n",
            "Test loss for epoch9963: 2.0428876876831055\n",
            "Train loss for epoch9964: 1.7922698259353638\n",
            "Test loss for epoch9964: 1.994251012802124\n",
            "Train loss for epoch9965: 2.1227283477783203\n",
            "Test loss for epoch9965: 1.8635581731796265\n",
            "Train loss for epoch9966: 2.094740390777588\n",
            "Test loss for epoch9966: 2.011903762817383\n",
            "Train loss for epoch9967: 2.0093069076538086\n",
            "Test loss for epoch9967: 2.1580986976623535\n",
            "Train loss for epoch9968: 1.9421393871307373\n",
            "Test loss for epoch9968: 1.9914135932922363\n",
            "Train loss for epoch9969: 1.9401357173919678\n",
            "Test loss for epoch9969: 2.2503890991210938\n",
            "Train loss for epoch9970: 1.9895293712615967\n",
            "Test loss for epoch9970: 2.0217819213867188\n",
            "Train loss for epoch9971: 2.0179507732391357\n",
            "Test loss for epoch9971: 2.0002527236938477\n",
            "Train loss for epoch9972: 1.9761759042739868\n",
            "Test loss for epoch9972: 1.9470454454421997\n",
            "Train loss for epoch9973: 2.026174783706665\n",
            "Test loss for epoch9973: 2.077077865600586\n",
            "Train loss for epoch9974: 2.04872465133667\n",
            "Test loss for epoch9974: 2.0367846488952637\n",
            "Train loss for epoch9975: 1.8775566816329956\n",
            "Test loss for epoch9975: 2.150606393814087\n",
            "Train loss for epoch9976: 2.0872445106506348\n",
            "Test loss for epoch9976: 2.31489896774292\n",
            "Train loss for epoch9977: 1.9355992078781128\n",
            "Test loss for epoch9977: 2.061485767364502\n",
            "Train loss for epoch9978: 1.973317265510559\n",
            "Test loss for epoch9978: 2.290400505065918\n",
            "Train loss for epoch9979: 1.9452203512191772\n",
            "Test loss for epoch9979: 2.209934711456299\n",
            "Train loss for epoch9980: 1.811825156211853\n",
            "Test loss for epoch9980: 2.083906650543213\n",
            "Train loss for epoch9981: 1.7839710712432861\n",
            "Test loss for epoch9981: 2.0206754207611084\n",
            "Train loss for epoch9982: 2.012218475341797\n",
            "Test loss for epoch9982: 1.910650610923767\n",
            "Train loss for epoch9983: 2.168973684310913\n",
            "Test loss for epoch9983: 2.08663272857666\n",
            "Train loss for epoch9984: 1.9074920415878296\n",
            "Test loss for epoch9984: 2.3205573558807373\n",
            "Train loss for epoch9985: 1.9467819929122925\n",
            "Test loss for epoch9985: 2.0320191383361816\n",
            "Train loss for epoch9986: 1.8885947465896606\n",
            "Test loss for epoch9986: 2.1065831184387207\n",
            "Train loss for epoch9987: 2.19484281539917\n",
            "Test loss for epoch9987: 2.214930534362793\n",
            "Train loss for epoch9988: 1.913170576095581\n",
            "Test loss for epoch9988: 2.109379768371582\n",
            "Train loss for epoch9989: 1.9476292133331299\n",
            "Test loss for epoch9989: 1.983667254447937\n",
            "Train loss for epoch9990: 2.1302897930145264\n",
            "Test loss for epoch9990: 2.226266860961914\n",
            "Train loss for epoch9991: 1.8628817796707153\n",
            "Test loss for epoch9991: 2.115001916885376\n",
            "Train loss for epoch9992: 1.9939275979995728\n",
            "Test loss for epoch9992: 2.0399982929229736\n",
            "Train loss for epoch9993: 1.9799389839172363\n",
            "Test loss for epoch9993: 2.132946252822876\n",
            "Train loss for epoch9994: 2.0194897651672363\n",
            "Test loss for epoch9994: 2.111931085586548\n",
            "Train loss for epoch9995: 1.9992551803588867\n",
            "Test loss for epoch9995: 1.9371676445007324\n",
            "Train loss for epoch9996: 2.067131519317627\n",
            "Test loss for epoch9996: 2.063793420791626\n",
            "Train loss for epoch9997: 1.8787095546722412\n",
            "Test loss for epoch9997: 2.148714780807495\n",
            "Train loss for epoch9998: 1.8521926403045654\n",
            "Test loss for epoch9998: 2.050560235977173\n",
            "Train loss for epoch9999: 1.9062654972076416\n",
            "Test loss for epoch9999: 2.039147138595581\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for _ in range(20):\n",
        "\n",
        "    out = []\n",
        "    context = [0] * block_size # initialize with all ...\n",
        "    while True:\n",
        "      # forward pass the neural net\n",
        "      logits,_ = model(torch.tensor([context]))\n",
        "      probs = F.softmax(logits, dim=1)\n",
        "      # sample from the distribution\n",
        "      ix = torch.multinomial(probs, num_samples=1).item()\n",
        "      # shift the context window and track the samples\n",
        "      context = context[1:] + [ix]\n",
        "      out.append(ix)\n",
        "      # if we sample the special '.' token, break\n",
        "      if ix == 0:\n",
        "        break\n",
        "\n",
        "    print(''.join(itos[i] for i in out)) # decode and print the generated word"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeL-L4WQertP",
        "outputId": "8be382a7-9c25-4a30-ec09-35443faea41c"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "riston.\n",
            "breon.\n",
            "liddeyn.\n",
            "janob.\n",
            "hryn.\n",
            "sedrigk.\n",
            "ivan.\n",
            "jaelko.\n",
            "juressiel.\n",
            "brsami.\n",
            "ryeden.\n",
            "aurielynna.\n",
            "nosau.\n",
            "maylana.\n",
            "zohanobi.\n",
            "emmik.\n",
            "ramcarddias.\n",
            "avyan.\n",
            "rayder.\n",
            "hazsa.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qGHZ9s4Viyb_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}